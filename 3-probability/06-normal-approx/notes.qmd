---
title: "Continuous Distributions and Normal Approximations"
subtitle: "Connections to boxes, continuous distributions, and a fundamental result"
image: images/front-image.png
format:
  html: default
  pdf: default
editor_options: 
  chunk_output_type: console
---

:::{.lo .content-hidden unless-profile="staff-site"}

#### Concept Acquisition

1. Connection with random variable and draws from a box
2. IID random variables
3. Sums and averages of IID random variables
4. Continuous random variables and their pdf
5. The unif(a,b) & normal distribution
6. The Central limit theorem
6. geom_density

#### Tool Acquisition

1. The empirical rule for a normal distribution
2. pnorm, qnorm, rnorm, punif, qunif, runif

#### Concept Application

1. computing the EV and SE of a sum of iid random variables
2. EV and SE for various distributions.
3. Relationship to the box for discrete rvs
4. Simulating box distributions and distributions of sums or averages of draws

----------------------------------------------------

:::


```{r}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(stat20data)
library(infer)
library(patchwork)
```

[I]{.dropcap}magine that a regular patron of a bar has hit the bottle rather hard one evening. When the bar closes for the night, they come out to weave their way home. Home is very near, in fact, just straight down the road. If our inebriate walks straight in the direction of their home, they can be there very soon. The only problem is - they can't walk straight. Every minute, they move in a random direction: backwards or forwards with equal probability. Where will they be after $n$ minutes? This is the famous "drunkard's walk" problem[^dw].

![](images/drunkard-walk-drawing.png){fig-aligned="center" width="500"}

Each time step (say, each minute), they go backwards or forwards with equal probability, so it is as if they are walking on the real line, and each minute they go either forward $(+1)$ or backwards $(+1)$, each with probability $\displaystyle \frac{1}{2}$. Where will our tipsy traveler be after $n$ steps? 

Here is a plot of a simulation of our itinerant inebriate's path for $n = 30$:

```{r}
#| fig-width: 8
#| fig-height: 5
#| echo: false
#| warning: false
#| message: false

# define Bernoulli on -1, 1
x <-  c(-1, 1)
px <-  c(1/2, 1/2)
n = 30
set.seed(2)
pos_x <- c(0,cumsum(sample(x, size = n, prob = px, replace = TRUE)))
t_x <- 0:n

data.frame(t_x, pos_x) |>
  ggplot(mapping = aes(x = t_x, y = pos_x)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "grey30") +
  geom_vline(xintercept = 0, color = "grey30") +
  geom_path() + theme_minimal() +
  labs(title = paste0("Where will the wasted walker be after ", n," minutes?"),
       x = "TIME (number of minutes elapsed =  number of steps taken)",
       y = "POSITION (along the road)") +
       #annotate("text", x = 0.9, y = 2, label = "X home", color = "red") +
       annotate("text", x = 0.9, y = 5, label = "X home", color = "red") +
       annotate(geom = "segment", x =-0.6, 
           y = 0.5, xend = 0, yend = 0, 
            arrow = arrow(length = unit(2, "mm")), color = "blue", lwd = 0.7) +
       annotate("text", x = -0.9, y = 0.63, label = "bar", color = "blue") +
  scale_y_continuous(breaks = seq(from = -5, to = 5, by = 1))
```

Note that the graph is spread out to show the number of steps taken, but the walker is just walking up and down the $y$-axis, since they can only go backwards and forwards. This kind of walk is called a *simple random walk*. Random walks have applications in many fields including physics and finance.They are used to model photons escaping from the center of the sun (though photons can scatter in *any* direction), a molecule in a liquid, the price of a stock etc. 

What we have shown here is one possible path our random walker might take and where they might land up after $n = 30$ minutes. We have plotted this by defining a modified Bernoulli random variable $X$ that takes the values $-1$ and $1$ with an equal probability of $\displaystyle \frac{1}{2}$. Then we sample $30$ times from this vector with replacement. Notice here that we use an argument in `sample()` that we haven't used before, `prob`. This defines the *weights* used for sampling. In this case they are the same, but they might be different. We will discuss this further in the "Ideas in Code" section. Note that the expected value of our Bernoulli random variable $X$ is $0$. What is its variance and SD?

<details><summary> Check your answer </summary>

$$
X = \begin{cases}
    -1, \; \text{ with prob } 0.5\\
    1, \; \text{ with prob } 0.5\\
 \end{cases}
$$
$$
 E(X) = (-1) \times 0.5 + 1 \times 0.5 = 0.
$$
$$
 Var(X) = E(X^2) - E(X)^2 = 1 - 0 = 1 = SD(X).
$$
 </details>
 
In the code below we are defining a vector with the values taken by $X$, a vector of probabilities for these values, and the distance traveled by the walker in $n$ steps:

```r
x <- c(-1,1) # defining the values taken by X
px <- c(1/2,1/2) # defining the probabilities of these values
sum(sample(x, size = 30, prob = px, replace = TRUE))
```

For example,in the path shown in the figure above, the first $5$ steps taken ($-1$ represents backwards and $+1$ forward): $1, -1, -1, 1, -1, -1$. The position at each step is the sum of all the steps thus far, so this sequence becomes $1, 0, -1, 0, -1, -2$. Notice that the end position after $30$ steps is $4$. One might ask - what is the *probability* that the position is $4$ after  $30$ steps? How would we compute this probability? This seems very difficult, so let's look at the empirical distribution of the position after $n = 30$ steps (we simulate many many such paths and look at the value of sum of the $+1$ and $-1$ steps at the end).


```{r}
#| echo: false
#| warning: false
#| message: false
#| fig-width: 6
#| fig-height: 4

set.seed(2)
n <- 30

# creating a function to give the value of S_n = sample sum
sn <- function(n, px){
  sum(sample(x, size = n, prob = px, replace = TRUE))
}

#defining how many replicates we want in the empirical distribution of S_n
m = 1000
sn_dist <- replicate(m, expr = sn(n, px))


data.frame(sn_dist) |> 
  ggplot(aes(x=sn_dist))+
  geom_histogram(aes(y = ..density..), breaks = seq(-25, 25, 2), color = "grey") +
  labs(x = paste0("Position after ", n, " steps"),
       y = "y = f(x), x is the position",
       title = "The probability distribution of the position
       of a random walker is bell-shaped!") + theme_minimal()


```


Now, before we can compute the expected value of the position or the associated probabilities, we need to learn about very special sets of random variables.


[^dw]: <https://en.wikipedia.org/wiki/Random_walk>



## Independent and identically distributed random variables

:::{.def}
**IID random variables**
 ~ If we have $n$ independent random variables $X_1, X_2, \ldots, X_n$ such that they all have the same pmf $f(x)$ and the same cdf $F(x)$, we call the random variables $X_1, X_2, \ldots, X_n$ independent and identically distributed random variables. We usually use the abbreviation i.i.d or iid for ``independent and identically distributed''.
::: 
 
This is a very important concept that we have already used to compute the expected value and variance of a binomial random variable by writing it as a sum of iid Bernoulli random variables.

A common example is when we toss a coin $n$ times and count the number of heads - each coin toss can be considered a Bernoulli random variable, and the total number of heads is a sum of $n$ iid Bernoulli random variables. 


### Example: Drawing tickets with replacement

Consider the box shown below:

![](images/box-1.png){fig-aligned="center" width="400"}

Say I draw $25$ tickets with replacement from this box, and let $X_k$ be the value of the $k$th ticket. Then each of the $X_k$ has the same distribution, and they are independent since we draw the tickets with replacement. Therefore $X_1, X_2, \ldots, X_{25}$ are iid random variables, and they *each* have a distribution defined by the following pmf:
$$
f(x) = \begin{cases}
      0.2, \; x = 0, 3, 4 \\
      0.1, \; x = 2 \\
      0.3, \; x = 1
      \end{cases}
$$      

\

## Sums and averages of random variables

### Sums

Suppose we make $n$ draws at random with replacement from the box above, and sum the drawn tickets. The sum $S_n = X_1, X_2, \ldots, X_{n}$ is also a random variable.
Let's simulate this by letting $n=25$ and we will sample $25$ tickets with replacement, sum them, and then repeat this process. Note that the smallest sum we can get is $S_n = 0$ and the largest is $100$. (Why?)

:::{.content-visible when-format="html"}
```{r}
#| echo: false
#| message: false
#| warning: false

set.seed(12345)
box <-  c(0,0,1,1,1,2,3,3,4,4)
s1 <- sum(sample(x = box, size = 25, replace = TRUE))
paste("The sum of 25 draws is", sep = " ", s1)
```
:::

:::{.content-visible when-format="pdf"}
```{r}
#| echo: false
#| message: false
#| warning: false

set.seed(12345)
box <-  c(0,0,1,1,1,2,3,3,4,4)
s1 <- sum(sample(box, size = 25, replace = TRUE))
paste("The sum of 25 draws is", sep = " ", s1)
```
:::

Now we will repeat this process 10 times:

:::{.content-visible when-format="html"}
```{r}
set.seed(12345)
replicate(10, sum(sample(box, size = 25, replace = TRUE)))

```
:::

:::{.content-visible when-format="pdf"}
```{r}
#| echo: false
#| message: false
#| warning: false


set.seed(12345)
replicate(10, sum(sample(box, size = 25, replace = TRUE)))

```
:::
It is clear that the sum  $S_n$ is random (because the $X_k$ are random), and we can see that the sum of the draws changes with each iteration of the process.

Since we know the distribution of the $X_k$, we can compute $E(X_k)$ and $Var(X_k)$. Note that since the $X_1, X_2, \ldots, X_{n}$ are iid, all the $X_k$ have the same mean and variance. What about their sum $S_n$?
What are $E(S_n)$ and $Var(S_n)$, when $n = 25$?

$E(X_k) = 0.2\times 0 + 0.3 \times 1 + 0.1 \times 2 + 0.2 \times 3 + 0.2 \times 4 = 1.9$ (Note that you could also have just computed the average of the tickets in the box.)

$Var(X_k) = \sum_x (x-1.9)^2 \times P(X=x) = 2.09$

$E(S_{25}) = E(X_1 + X_2 + \ldots +X_{25}) = 25 \times E(X_1) = 25 \times 1.9$.

(We just use $X_1$ since all the $X_k$ have the same distribution.)

Since the $X_k$ are independent, we can write that 

$$
\begin{aligned} 
Var(S_{25}) &= Var(X_1 + X_2 + \ldots +X_{25})\\
&= Var(X_1) + Var(X_2) + \ldots +Var(X_{25})\\
&= 25 \times 2.09
\end{aligned}
$$

We can see that the expectation and variance of the sum *scale* with $n$, so that if $S_n$ is the sum of $n$ iid random variables $X_1, X_2, \ldots, X_n$, then:

$$
\begin{aligned}
E(S_n) &= n \times E(X_1) \\
Var(S_n) &= n \times Var(X_1)\\
\end{aligned}
$$
This does not hold for $SD(S_n)$, though. For the SD, we have the following ``law'' for the standard deviation of the sum.

Square root law for sums of iid random variables
: The standard deviation of the sum of $n$ iid random variables is given by:
$$
SD(S_n) = \sqrt{n} \times SD(X_1)
$$

Since all the $X_k$ have the same distribution, we can use $X_1$ to compute the mean and SD of the sum. This law says that if the sample size increases as $n$, the expected value scales as the *number* of random variables, but the standard deviation of the sum increases more slowly, scaling as $\sqrt{n}$. In other words, if you increase the number of random variables you are summing, the spread of your sum about its expected value increases, but not as fast as the expectation of the sum.


### Averages

We denote the average of the random variables $X_1, X_2, \ldots, X_n$ by $\displaystyle \bar{X} =\frac{S_n}{n}$.

$\displaystyle \bar{X}$ is called the *sample mean* (where the ``sample'' consists of $X_1, X_2, \ldots, X_n$).

$$ E(\bar{X}) = E(\frac{S_n}{n}) = \frac{1}{n} E(S_n) = E(X_1) $$

This means that the expected value of an average does *not* scale as $n$, but $E(\bar{X})$ is the *same* as the expected value of a single random variable. Let's check the variance now:

$$ 
Var(\bar{X}) = Var(\frac{S_n}{n}) = \frac{1}{n^2} Var(S_n) =  \frac{n}{n^2} Var(X_1)
$$

Therefore $Var(\bar{X}) =  \displaystyle \frac{1}{n} Var(X_1)$

Note that, just like the sample sum $S_n$, the sample mean $\displaystyle \bar{X}$ is a random variable, and its variance scales as $\displaystyle \frac{1}{n}$, which implies that $SD(\bar{X})$ will scale as $\displaystyle \frac{1}{\sqrt{n}}$.

Square root law for averages of iid random variables
: The standard deviation of the average of $n$ iid random variables is given by:
$$
SD(\bar{X}) = \frac{1}{\sqrt{n}}SD(X_1)
$$


:::{.def}
**Standard error**
 ~ Since $S_n$ and $\bar{X}$ are numbers computed from the sample $X_1, X_2, \ldots, X_n$, they are called *statistics*. We use the term *standard error* to denote the standard deviation of a *statistic*: $SE(S_n)$ and $SE(\bar{X})$ to distinguish it from the standard deviations of random variables that do not arise as statistics that are computed from $X_1, X_2, \ldots, X_n$.
::: 

#### Example: Probability distributions for sums and averages

Let's go back to the box of colored tickets, draw from this box $n$ times, and then compute the sum and average of the draws. We will simulate the distribution of the sum and the average of 25 draws to see what the distribution of the statistics looks like. Note that when $n=25$, $E(S_n) = 25\times 1.9 = 47.5$ and $SE(S_n) = \sqrt{n} \times SD(X_1) = 5 \times 1.45 = 7.25$

:::{.content-visible when-format="html"}
```{r}
#| message: false
#| fig-width: 10
#| fig-height: 8

set.seed(12345)
box = c(0,0,1,1,1,2,3,3,4,4)
s1 <- sum(sample(box, size = 25, replace = TRUE))

sum_draws_25 = replicate(1000, sum(sample(box, size = 25, replace = TRUE)))

p1 <- data.frame(sum_draws_25) %>%
  ggplot(aes(x = sum_draws_25, y=..density..)) + 
  geom_histogram(fill = "darkolivegreen2", color = "white") + 
  xlab("sample sum") +
  ylab("density") +
  ggtitle("Empirical distribution of the sample sum, n = 25") + 
  geom_vline(xintercept = 47.5, color = "black", lwd = 1.1)


sum_draws_100 = replicate(1000, sum(sample(box, size = 100, replace = TRUE)))

p2 <- data.frame(sum_draws_100) %>%
  ggplot(aes(x = sum_draws_100, y=..density..)) + 
  geom_histogram(fill = "darkolivegreen3", color = "white") + 
  xlab("sample sum") +
  ylab("density") +
  ggtitle("Empirical distribution of the sample sum, n = 100") + 
  geom_vline(xintercept = 190, color = "black", lwd = 1.1)

mean_draws_25 = replicate(1000, mean(sample(box, size = 25, replace = TRUE)))

p3 <- data.frame(mean_draws_25) %>%
  ggplot(aes(x = mean_draws_25, y=..density..)) + 
  geom_histogram(fill = "cadetblue2", color = "white") + 
  xlab("sample mean") +
  ylab("density") +
  ggtitle("Empirical distribution of the sample mean, n = 25")  + 
  geom_vline(xintercept = 1.9, color = "black", lwd = 1.1)


mean_draws_100 = replicate(1000, mean(sample(box, size = 100, replace = TRUE)))

p4 <- data.frame(mean_draws_100) %>%
  ggplot(aes(x = mean_draws_100, y=..density..)) + 
  geom_histogram(fill = "deepskyblue", color = "white") + 
  xlab("sample mean") +
  ylab("density") +
  ggtitle("Empirical distribution of the sample mean, n = 100") +
  geom_vline(xintercept = 1.9, color = "black", lwd = 1.1)


(p1+p2)/(p3+p4)

```
:::

:::{.content-visible when-format="pdf"}
```{r}
#| echo: false
#| message: false
#| fig-width: 10
#| fig-height: 8

set.seed(12345)
box = c(0,0,1,1,1,2,3,3,4,4)
s1 <- sum(sample(box, size = 25, replace = TRUE))

sum_draws_25 = replicate(1000, sum(sample(box, size = 25, replace = TRUE)))

p1 <- data.frame(sum_draws_25) %>%
  ggplot(aes(x = sum_draws_25, y=..density..)) + 
  geom_histogram(fill = "darkolivegreen2", color = "white") + 
  xlab("sample sum") +
  ylab("density") +
  ggtitle("Empirical distribution of the sample sum, n = 25") + 
  geom_vline(xintercept = 47.5, color = "black", lwd = 1.1)


sum_draws_100 = replicate(1000, sum(sample(box, size = 100, replace = TRUE)))

p2 <- data.frame(sum_draws_100) %>%
  ggplot(aes(x = sum_draws_100, y=..density..)) + 
  geom_histogram(fill = "darkolivegreen3", color = "white") + 
  xlab("sample sum") +
  ylab("density") +
  ggtitle("Empirical distribution of the sample sum, n = 100") + 
  geom_vline(xintercept = 190, color = "black", lwd = 1.1)

mean_draws_25 = replicate(1000, mean(sample(box, size = 25, replace = TRUE)))

p3 <- data.frame(mean_draws_25) %>%
  ggplot(aes(x = mean_draws_25, y=..density..)) + 
  geom_histogram(fill = "cadetblue2", color = "white") + 
  xlab("sample mean") +
  ylab("density") +
  ggtitle("Empirical distribution of the sample mean, n = 25")  + 
  geom_vline(xintercept = 1.9, color = "black", lwd = 1.1)


mean_draws_100 = replicate(1000, mean(sample(box, size = 100, replace = TRUE)))

p4 <- data.frame(mean_draws_100) %>%
  ggplot(aes(x = mean_draws_100, y=..density..)) + 
  geom_histogram(fill = "deepskyblue", color = "white") + 
  xlab("sample mean") +
  ylab("density") +
  ggtitle("Empirical distribution of the sample mean, n = 100") +
  geom_vline(xintercept = 1.9, color = "black", lwd = 1.1)


(p1+p2)/(p3+p4)

```
:::

What do we notice in these figures? The black line is the expected value. We see that the center of the distribution for the sample sum grows as the sample size increases (look at the x-axis), but this does not happen for the distribution of the sample mean. You can also see that the spread of the data for the sample sum is much greater when n = 100, but this does not happen for the distribution of the sample mean. We will explore the sample sum and sample mean next week. Now, the $y$ axis has neither counts nor proportion, but it has "density". This makes the histogram have a total area of one, similar to a probability histogram. Now we can think of this density histogram as an *approximation* of the probability histogram.


## Continuous distributions

So far, we have talked about discrete distributions, and the probability mass functions for such distributions. Consider a random variable that takes *any* value in a given interval. Recall that  we call such random variables *continuous*. In this situation, we cannot think about discrete bits of probability mass which are non-zero for certain numbers, but rather we imagine that our total probability mass of $1$ is *smeared* over the interval, giving us a smooth density curve, rather than a histogram. To define the probabilities associated with a continuous random variable, we define a probability *density* function (pdf) rather than a probability mass function. 

### Probability density function of a distribution

:::{.def}
**Probability density function**
 ~ This is a function $f(x)$ that satisfies two conditions:

- (1) it is non-negative ($f(x) \ge 0$) and 
- (2) the total area under the curve $y = f(x)$ is 1. That is, 
$$ \int_{-\infty}^\infty f(x) dx = 1 $$
:::

If $X$ is a continuous random variable, we don't talk about $P(X = x$), that is, the probability that $X$ takes a particular value. Rather, we ask what is the probability that $X$ lies in an interval around $x$. Since there are infinitely many outcomes in any interval on the real line, no single outcome can have positive probability, so $P(X=x) =0$ for any particular $x$ in the interval where $X$ is defined. To find the probability that $X$ lies in an interval $(a,b)$, we integrate $f(x)$ over the interval $(a,b)$. That is, we find the area under the curve $f(x)$ over the interval $(a, b)$.

:::{.def}
**The probability that a continuous random variable lies in an interval**
 ~ The probability that $X$ is in the interval $(a,b)$ is given by
$$
P(a < X < b) = P(X \text{ is in the interval }(a,b)) =  \int_{a}^b f(x) dx
$$
:::


### Example: Uniform(0,1) distribution

Let $X$ be a random variable that takes values in the interval $(0,1)$ with probability density function $f(x) = 1$ for $x$ in $(0,1)$ and $f(x) = 0$ outside of this interval. 

Because $f(x)$ is flat, all intervals of the same length will have the *same area*, so the distribution defined by $f$ is called the *Uniform*$(0,1)$ distribution. If a random variable $X$ has this distribution, we denote this by $X \sim U(0,1)$. The probability that $X$ is in any interval $(a, b)$ which is a subinterval of $(0,1)$ is given by the area of the rectangle formed by the interval and $y=f(x)$, and so is just the width of the interval.

![](images/unif-1.png){fig-aligned="center" width="600"}

### Cumulative distribution function $F(x)$

:::{.def}
**Cumulative distribution function (cdf)**
 ~ The cdf is defined the same way as for discrete random variables:
$$
F(x) = P(X \le x) = \int_{-\infty}^x f(t) dt
$$
:::

The difference is in how we compute $P(X \le x)$. There are no discrete bits of probability mass for $F(x)$ to collect. Instead we have that $F(x)$ is the area under the curve $y = f(x)$ all the way up to the point $x$. 


### Example: cdf for the Uniform (0,1) distribution

Let $X \sim U(0,1)$. What is $F(0.3)$?

<details> <summary> Check your answer </summary>

$$ 
F(0.3) = P(X \le 0.3) = \int_{-\infty}^0.3 f(t) dt =  \int_0^0.3 1 dt = 0.3
$$
In general, for the $U(0,1)$ distribution, $F(x) = x$.

</details>



## Summary

- We defined the expected value or the mean of a discrete random variable and listed the properties of expectation including linearity and additivity. 

- We defined the variance and standard deviation of a random  variable. Both expectation and variance (and therefore standard deviation) are constants associated to the distribution of the random variable. The variance is more convenient than the sd for computation because it doesn't have square roots. However, the units are squared, so you have to be careful while interpreting the variance. We discussed the properties of variance and standard deviation. 

- We wrote down the expected values and variance for various special random variables.


<!-- We defined the expectation and variance of sums and averages of an iid sample of random variables, and introduced the term *standard error*. We recognized how the mean and variance scale with $n$ and defined the square root law for the standard error of the sum or mean of an iid sample. 

We considered the probability distributions of sums and averages. 

Finally, we introduced continuous distributions. In subsequent notes, we will introduce the most celebrated continuous distribution, the normal distribution. 
-->
