---
title: "Probability Distributions"
subtitle: "Visualizing probabilities"
image: images/die-emp-prob-dsn.jpg
format:
  html: default
  pdf: default
editor_options: 
  chunk_output_type: console
---

:::{.lo .content-hidden unless-profile="staff-site"}

#### Concept Acquisition

1. Probability distributions
2. Probability histograms
3. Empirical histograms
4. Distribution tables



#### Tool Acquisition

1. How to write down the distribution of the probabilities of outcomes
2. What a probability histogram represents
3. Empirical histograms vs probability histograms
4. `geom_col()`, R script files, `replicate()`


#### Concept Application

1.Drawing probability histograms
2.Using R to simulate probabilities
3.Drawing empirical histograms


----------------------------------------------------

:::

```{r}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(stat20data)
library(infer)
library(patchwork)
```



[S]{.dropcap}o far we have seen examples of outcome spaces, and descriptions of how we might compute probabilities, along with tabular representations of the probabilities. In this set of notes, we are going to talk about how to visualize probabilities using histograms (and *why*) we would do this, as well as how to visualize simulations of outcomes from actions such as tossing coins or rolling dice. 

## Probability histograms

In this section, we will introduce a new kind of histogram that is a very natural way of representing probability distributions.

### Box of tickets

Recall the example in which we drew a ticket from a box with 5 tickets in it:

![](images/box.jpeg){fig-align=center width="200px"}

If we draw one ticket at random from this box, we know that the probabilities of the four distinct outcomes can be listed in a table as:

|**Outcome** | $1$ | $2$ | $3$ | $4$ |
|:----:|:----:|:----:|:----:|:----:|
|**Probability**| $\displaystyle \frac{1}{5}$ |$\displaystyle \frac{2}{5}$ |$\displaystyle \frac{1}{5}$ |$\displaystyle \frac{1}{5}$ |

What we have described in the table above is a __probability distribution__. We have shown how the total probability of one  or 100% is *distributed* among all the possible outcomes. Since the ticket $\fbox{2}$ is twice as likely as any of the other outcomes, it gets twice as much of the probability. 

A table is nice, but a visual representation would be even better. How might we visualize this distribution? One way might be to represent the outcomes along the $x$-axis, and the probabilities as vertical lines:

```{r}
#| fig.width: 4
#| fig.height: 3
#| fig.align: center
#| echo: false
#| message: false

tkts_box <- c(1, 2, 3, 4)
prob_box <- c(1/5, 2/5, 1/5, 1/5)

data.frame(tkts_box) |>
  ggplot(mapping = aes(x = tkts_box, y = prob_box)) + 
  geom_point() +
  ylim(c(0, 0.5)) + 
  geom_segment(x = tkts_box, xend = tkts_box, y = c(0, 0, 0 , 0), yend = prob_box) +
  geom_hline(yintercept = 0) + 
  labs(title = "Probability distribution of a ticket\n drawn from the box above",
       x = "ticket value",
       y = "probability")
```


This looks good, but for reasons that will become clearer as we learn more about probability, it turns out that it is better to represent the distribution in the form of a histogram, with the areas of the bars representing probabilities:

```{r}
#| fig.width: 4
#| fig.height: 3
#| fig.align: center
#| echo: false

tkts_box <- c(1, 2, 3, 4)
prob_box <- c(1/5, 2/5, 1/5, 1/5)

box <- c(1, 2, 2, 3, 4)

data.frame(tkts_box) |> 
  ggplot(aes(x=tkts_box, y=prob_box)) +
  geom_col(width = 0.98, fill = "goldenrod2") +
  ylim(c(0, 0.5)) + 
 labs(title = "Probability distribution of a ticket\n drawn from the box above",
       x = "ticket value",
       y = "probability")

```

Notice that this histogram is different from the ones we have seen before, since we didn't collect any data. We just defined the probabilities based on the outcomes, and then drew bars with the heights being the probabilities. This type of *theoretical* histogram is called a __*probability histogram*__.

We could do this for any of examples that we have seen - die rolls, coin tosses etc. What about if we *don't* know the probability distribution of the outcomes of an experiment? For example, what if we didn't know how to compute the probability distribution above? What could we do to get an idea of what the probabilities might be? Well, we could keep drawing tickets over and over again from the box, __with__ replacement, keep track of the tickets we draw, and make a bar graph. This kind of histogram, which is the kind we have seen before, is a visual representation of *data*, and is called an __*empirical*__ histogram, representing an *empirical* distribution of the values.


### Probability histograms vs empirical histograms

You see two plots below: the top (purple) figure is a bar graph showing the distribution of the tickets in the box, and the bottom (golden) is the probability histogram for the value of a randomly drawn ticket.


```{r}
#| fig.width: 6
#| fig.height: 5
#| fig.align: center
#| echo: false

tkts_box <- c(1, 2, 3, 4)
prob_box <- c(1/5, 2/5, 1/5, 1/5)

box <- c(1, 2, 2, 3, 4)

p1 <- data.frame(box) |> 
  ggplot(aes(x=box)) +
  geom_bar(width = 0.98, fill = "darkorchid") +
  xlab("ticket value")  +
  ggtitle("Ticket distribution")

p2 <- data.frame(tkts_box) |> 
  ggplot(aes(x=tkts_box, y=prob_box)) +
  geom_col(width = 0.98, fill = "goldenrod2") +
  xlab("ticket value") +
  ylab("probability") + 
  ggtitle("Probability distribution")

p1/p2
```

Notice that the only difference between these two plots is the vertical scale. When the tickets are equally likely, the box distribution (the purple plot) completely determines the probability distribution (the golden plot).

Now, how would we figure out the probabilities (shown in the golden plot) using simulation?

<!--
Since the tickets are drawn according to their chance, we could compute what a draw would be, on average, by computing a weighted (by their chance) average of the possible values. We would get, for a single draw: $1\times \frac{1}{5} + 2 \times \frac{2}{5} + 3 \times \frac{1}{5} + 4 \times \frac{1}{5} = 2.4$. (Note that this exactly matches the average of the tickets in the box $= \displaystyle \frac{1 + 2 + 2 + 3 + 4}{5}$). 
-->


How about if we draw 50 times at random with replacement from this box (that is, each time we draw a ticket, we put the ticket back before we draw another ticket, so that the box we draw from always has the same number of tickets), and see what our sample looks like. That is, we will see what proportion of the draws are 1's, 2's etc.


```{r}
#| fig.width: 4
#| fig.height: 4
#| fig.align: center
#| warning: false
#| echo: false

set.seed(12345)
box <- c(1,2,2,3,4)
sample_size <- 50
sample_box <- sample(box, size = sample_size, replace = TRUE)
data.frame(sample_box) |>
  ggplot(aes(sample_box)) + 
  geom_bar(aes(y = ..prop..), width = 0.98, fill = "blue") +
  xlab("values of draws") + 
  ylab("proportion of draws")
```


We can see that the (blue) *sample proportions* look similar to the (gold) *population (box) proportions* (the probability distribution above), but are somewhat different. It turns out that the counts of the drawn tickets are:

<center>
<div style="width:300px">

|    Ticket | Number of times drawn | Proportion of times drawn |
|:---------:|:---------------------:|:---------------------:|
| $\fbox{1}$| `r table(sample_box)[[1]]` | `r table(sample_box)[[1]]/sample_size` |
| $\fbox{2}$| `r table(sample_box)[[2]]` |`r table(sample_box)[[2]]/sample_size` |
| $\fbox{3}$| `r table(sample_box)[[3]]` | `r table(sample_box)[[3]]/sample_size` |
| $\fbox{4}$|  `r table(sample_box)[[4]]` | `r table(sample_box)[[4]]/sample_size` |

</div>
</center>

What we have seen here is how when we draw at random, we get a sample that resembles the population, that is, a *representative sample*, but it isn't *exactly* the true probabilities. This is a plot of the *empirical* distribution, that is, a distribution that is obtained from the data.

We will see how to simulate plot empirical histograms for various distributions later, but for now let's look at some more examples of probability histograms. 

## Examples

### Tossing a coin

Let's think about the probabilities when we toss a coin. If we toss a *fair* coin once, we have two equally likely outcomes that are possible, "Heads" and "Tails", each of which are equally likely. In order to plot a histogram, we need to record each toss that lands heads as $1$ and each toss that lands tails as $0$. This is like drawing a ticket from a box that has two tickets marked $0$ and $1$.

What if the coin is not fair, and the chance of landing heads is $2/3$ or even $3/4$. In these cases, the histogram's bar over $1$ has to reflect this probability. Since the area of the bar is the probability, if the width was $1$, the height of the bar over $1$ would be $3/4$. Here are three probability histograms, one for a fair coin, and the other two for biased coins. 

![](images/prob-hist-coin-toss.png){fig-align=center width="600px"}

Suppose we wanted to represent tossing a __biased__ coin by drawing tickets from a box. What box would we use? For example, if the probability of the coin landing heads is $3/4$, and we wanted to represent this by drawing from a box of tickets marked $\fbox{0}$ or $\fbox{1}$, then we would need *three* tickets marked $\fbox{1}$, and one ticket marked $\fbox{0}$. What about if the probability of the coin landing heads was $2/3$? What tickets would the corresponding box contain? What about if the probability of the coin landing heads was 0.3?

<details> <summary> Check your answer </summary>
1. If the probability of heads is $2/3$, then we need the probability of drawing a $\fbox{1}$ to be $2/3$, so we would need two tickets marked $\fbox{1}$ and one marked $\fbox{0}$. 

2. If the probability of the coin landing heads is $0.3$, then we need three out of ten tickets in the box to be marked $\fbox{1}$ and the other seven to be $\fbox{0}$.

</details>

### Rolling a pair of dice and summing the spots

The outcomes are already numbers, so we don't need to represent them differently. We know that there are $36$ total possible equally likely outcomes when we roll a pair of dice, but when we add the spots, we have only 11 possible outcomes, which are __not__ equally likely (the chance of seeing a $2$ is 1/36, but $P(6)=5/36$.)

The histogram will have the possible outcomes listed on the x-axis, and bars of width $1$ over each possible outcome. The height of these bars will be the probability, so that the areas of the bars represent the probability of the value under the bar.
## The Ideas in Code

Before discussing how to simulate the distributions, we are going to introduce three useful functions and a very important file type..

### Three useful functions

#### 1. `rep()`: replicates values in a vector

Sometimes we need to create vectors with repeated values. In these cases, `rep()` is very useful. 

- **Arguments** 
    - `x`: the vector or list that is to be repeated. This *must* be specified
    - `times`: the number of times we should repeat the elements of `x`. This could be a vector the same length as `x` detailing how many times each element is to be repeated, or it could be a single number, in which case the entire `x` is repeated that many times.
    - `each`: the default is 1, and if specified, each element of `x` is repeated `each` times. 
    
**Example: Rolling a pair of dice and summing the spots**

Suppose we want to represent the rolling of a pair of dice and summing the spots using a box with appropriately marked tickets. We could create a vector that has each possible value.
```{r}
#| code-fold: true

sum_dice <- seq(from = 2, by = 1, to = 12)
sum_dice

```
 
The problem with the vector `sum_dice` is that the probabilities are not represented correctly. For example, there is only one way to get a sum of $2$, but $6$ ways to get $7$. If we want to represent this action of rolling a pair of dice and taking the sum of spots, we have to use a box in which values will be repeated to reflect their probability. How would you use `rep()` to create a vector representing a box with $36$ tickets, each representing a possible sum, and with the tickets repeated so the probabilities are correct. For example, since the sum $7$ can be obtained in $6$ ways (by rolling a $1$ and a $6$ or a $6$ and a $1$; a $2$ and a $5$ or a $5$ and a $2$ and so on), we need __six__ tickets marked $7$, so that the chance of drawing a $7$ is $6/36$.

<details> <summary> Check your answer </summary>
```{r}
#| code-fold: false

sum_dice <- seq(from = 2, by = 1, to = 12)

sum_dice_1 <- rep(sum_dice, 
                  times = c(1,2, 3, 4, 5, 6, 5, 4, 3, 2, 1))
sum_dice_1
```
</details>

#### 2. `replicate()`: repeat a specific set of tasks a large number of times.

- **Arguments** 
    - `n`: the number of times we want to repeat the task. This *must* be specified
    - `expr`: the task we want to repeat, usually an expression that is some combinations of functions, for example, maybe we take a sample from a vector, and then sum the sample values.
    
**Example: Rolling a pair of dice and summing the spots**    
  
Let's simulate rolling a pair of dice and summing the spots using `sample()`. Remember, we should use `set.seed()` to make sure we can reproduce our results. First, let's do it once. We need to sample twice, and then use `sum()` to add up the sample values. The easiest way to do this is to *nest* the functions, with the one we want executed first on the inside. So we will nest `sample()` inside of `sum()`.

```{r}
set.seed(214)
die <- seq(from = 1, to = 6)
sum(sample(die, size = 2, replace = TRUE))
```

Now I want to do this task 10 times. That is, each time I sample twice (to simulate rolling a pair of dice), and then sum the sample values. I should get 10 sums (so the numbers will be between $2$ and $12$).

```{r}
set.seed(214)
replicate(n=10, expr = sum(sample(die, size = 2, replace = TRUE)))
```

We could replicate as many times as we like
    
**Example: Rolling a pair of dice and summing the spots**

### Simulations

:::{.callout-tip}

## Code along

As you read through these notes, keep RStudio open in another window to code along at the console.
:::




Let's simulate rolling a die and counting how many times we see each face. 

If we roll it 6 times, we don't really expect to see each face exactly once, and as you can see below, in this particular instance of rolling the die six times, we didn't see the face with one spot, but saw two spots twice. 




```{r}
set.seed(12345)
die <- seq(from = 1, by = 1, to = 6)

die_rolls <- sample(die, 6, replace = TRUE)

data.frame(die_rolls) |>
  group_by(die_rolls) |> 
  summarise(n = n())
```


What about if we roll the die 60 times? We should see each face *about* ten times:

```{r}
set.seed(12345)

die_rolls <- sample(die, 60, replace = TRUE)

data.frame(die_rolls) |>
  group_by(die_rolls) |> 
  summarise(n = n())
```

Not so great, but let's try rolling the die 600 times:
```{r}
set.seed(12345)

die_rolls <- sample(die, 600, replace = TRUE)

data.frame(die_rolls) |>
  group_by(die_rolls) |> 
  summarise(n = n())
```

It might be better to visualize it. We will draw the *probability distribution* in gold, which shows the probabilities of each possible outcome (the probability distribution), and compare it to the *empirical* distributions in blue (plotting the *data* distribution, not the probability distribution) of the results of rolling the die $60, 600$, and $6000$ times. Note that the probability distribution is *theoretical*, where the area of the bars represent probabilities, and the total area of the bars is $1$.

:::{.content-visible when-format="html"}
```{r}
#| fig-align: center
#| fig-height: 6
#| fig-width: 6

prob_die <- rep(1/6, 6)
set.seed(12345)

p1 <- data.frame(die) |> 
  ggplot(aes(x = factor(die), y=prob_die)) +
  geom_col(width = 0.98, fill = "goldenrod2") +
  xlab("number of spots") +
  ylab("probability") +
  ggtitle("Probability distribution of the \n outcome of a die roll") +
  lims(y = c(0, .35))

roll_60 <- sample(die, 60, replace = TRUE)
roll_60 <- data.frame(table(roll_60)) |> 
  mutate(prop_rolls = Freq/60)


p2 <- roll_60 |>
  ggplot(aes(x = factor(die), y= prop_rolls)) +
  geom_col(width = 0.98, fill = "blue") +
  xlab("number of spots") +
  ylab("proportion of rolls") +
  ggtitle("Empirical distribution for \n 60 rolls") +
  lims(y = c(0, .35))


roll_600 <- sample(die, 600, replace = TRUE)

roll_600 <- data.frame(table(roll_600)) |>
    mutate(prop_rolls = Freq/600)

p3 <- roll_600 |>
  ggplot(aes(x = factor(die), y= prop_rolls)) +
  geom_col(width = 0.98, fill = "blue") +
  xlab("number of spots") +
  ylab("proportion of rolls") +
  ggtitle("Empirical distribution for \n 600 rolls") +
  lims(y = c(0, .35))
  

roll_6000 <- sample(die, 6000, replace = TRUE)

roll_6000 <- data.frame(table(roll_6000)) |>
    mutate(prop_rolls = Freq/6000)


p4 <- data.frame(roll_6000) |>
  ggplot(aes(x = factor(die), y=prop_rolls)) +
  geom_col(width = 0.98, fill = "blue") +
  xlab("number of spots") +
  ylab("proportion of rolls") +
  ggtitle("Empirical distribution for \n 6000 rolls") +
  lims(y = c(0, .35))

(p1 + p2)/(p3+p4)
```
:::

:::{.content-hidden when-format="html"}
```{r}
#| fig-align: center
#| fig-height: 6
#| fig-width: 6
#| code-fold: false
#| echo: false

prob_die <- rep(1/6, 6)
set.seed(12345)

p1 <- data.frame(die) |> 
  ggplot(aes(x = factor(die), y=prob_die)) +
  geom_col(width = 0.98, fill = "goldenrod2") +
  xlab("number of spots") +
  ylab("probability") +
  ggtitle("Probability distribution of the \n outcome of a die roll") +
  lims(y = c(0, .35))

roll_60 <- sample(die, 60, replace = TRUE)
roll_60 <- data.frame(table(roll_60)) |> 
  mutate(prop_rolls = Freq/60)


p2 <- roll_60 |>
  ggplot(aes(x = factor(die), y= prop_rolls)) +
  geom_col(width = 0.98, fill = "blue") +
  xlab("number of spots") +
  ylab("proportion of rolls") +
  ggtitle("Empirical distribution for \n 60 rolls") +
  lims(y = c(0, .35))


roll_600 <- sample(die, 600, replace = TRUE)

roll_600 <- data.frame(table(roll_600)) |>
    mutate(prop_rolls = Freq/600)

p3 <- roll_600 |>
  ggplot(aes(x = factor(die), y= prop_rolls)) +
  geom_col(width = 0.98, fill = "blue") +
  xlab("number of spots") +
  ylab("proportion of rolls") +
  ggtitle("Empirical distribution for \n 600 rolls") +
  lims(y = c(0, .35))
  

roll_6000 <- sample(die, 6000, replace = TRUE)

roll_6000 <- data.frame(table(roll_6000)) |>
    mutate(prop_rolls = Freq/6000)


p4 <- data.frame(roll_6000) |>
  ggplot(aes(x = factor(die), y=prop_rolls)) +
  geom_col(width = 0.98, fill = "blue") +
  xlab("number of spots") +
  ylab("proportion of rolls") +
  ggtitle("Empirical distribution for \n 6000 rolls") +
  lims(y = c(0, .35))

(p1 + p2)/(p3+p4)
```
:::

The important takeaway here is that we have a *theoretical* probability distribution of the outcomes, and we have what actually happens when we perform the experiment over and over. Eventually, the empirical distribution begins to look like the theoretical distribution. 


```{r}
#| code-fold: false
set.seed(12345)
box <- c(1, 2, 2, 3, 4)
sample(box,1)
```

We can use `sample()` to *estimate* the chance of a particular outcome  when we aren't sure of what that chance might be. We would do this by repeatedly sampling from the "box" with replacement (many times), then computing the proportion of times we drew each ticket. For example, say we consider our first example (the simple box), and want to estimate the chance of each ticket.

In the code below, another new function is introduced: `replicate()`. The function `replicate(reps, expr)` is a very useful function that takes as input an expression `expr` and evaluates it `reps` times, returning a vector. 

```{r}
#| code-fold: false
#| fig-width: 4
#| fig-height: 3
#| fig-align: center

box <- c(1, 2, 2, 3, 4)
draws <- replicate(2000, sample(box, 1, replace = TRUE))
ggplot(data.frame(draws), aes(x=draws)) + 
  geom_bar(aes(y=..prop..), fill="blue", width = 0.98) + 
  ylab("proportion of draws") + 
  xlab("ticket drawn")
```

We see that the *estimated* chance of drawing a $\fbox{2}$ is about 0.4, and this is about twice the estimated chance of drawing any other ticket. Of course, we knew this already, without needing to code it in R. Let's think of a more complicated situation: 

What if we wanted to wanted to draw _five_ tickets with replacement from this box, and sum the draws? What would be the possible values that we would get? What could their chances be? We can visualize this in R:


:::{.content-visible when-format="html"}
```{r}
#| fig-width: 4
#| fig-height: 3
#| fig-align: center

box <- c(1, 2, 2, 3, 4)
draws <- replicate(5000, sum(sample(box, size = 5, replace = TRUE)))
ggplot(data.frame(draws), aes(x=draws)) + 
  geom_bar(aes(y=..prop..), fill="blue", width = 0.98) + 
  ylab("proportion") + 
  xlab("sum of draws") + 
  scale_x_continuous(breaks = seq(min(draws), max(draws), by = 1))
```
:::

:::{.content-hidden when-format="html"}
```{r}
#| fig-width: 4
#| fig-height: 3
#| fig-align: center
#| code-fold: false
#| echo: false

box <- c(1, 2, 2, 3, 4)
draws <- replicate(5000, sum(sample(box, size = 5, replace = TRUE)))
ggplot(data.frame(draws), aes(x=draws)) + 
  geom_bar(aes(y=..prop..), fill="blue", width = 0.98) + 
  ylab("proportion") + 
  xlab("sum of draws") + 
  scale_x_continuous(breaks = seq(min(draws), max(draws), by = 1))
```
:::

We can see that there is a lot more variation in the values taken by the sum of 5 draws.


#### Tossing a fair coin

We can estimate the chances of various outcomes related to coin tossing, using sampling from a box. 

Suppose, for example, that we would like to figure out the chance of exactly 2 heads if we toss a coin 4 times. Think about how you would use the functions `sample()` and `replicate()` to model this, using the 0-1 box we defined earlier, for tossing a coin.

```{r}

coin <- c(0, 1) #1 represents the toss landing heads
two_heads <-replicate(50000, sum(sample(coin, 4, replace = TRUE)) == 2)
cat("The proportion of times we see 2 heads out of 4 tosses is", mean(two_heads))

```


#### Rolling a pair of dice and summing the spots

This is something that we could use if we wanted to play Monopoly and couldn't find the dice. Recall the box we used to simulate a die roll. Now we are going to define a vector in R to represent a die, and sample twice with replacement, from this vector, and add the spots.


```{r}
die <- seq(from = 1, by = 1, to = 6)
# to simulate rolling a die twice and summing the spots
draws <- sample(die, size = 2, replace = TRUE)
sum(draws)

```

We could also repeat it many times and estimate the chance of each of the possible outcomes.

:::{.content-visible when-format="html"}
```{r}
#| fig-width: 4
#| fig-height: 3
#| fig-align: center

many_draws <- replicate(5000, sum(sample(die, size = 2, replace = TRUE)))
ggplot(data.frame(many_draws), aes(x=many_draws)) + 
  geom_bar(aes(y=..prop..), fill="blue", width = 0.98) + 
  ylab("proportion") + 
  xlab("sum of two draws") + 
  scale_x_continuous(breaks = seq(min(many_draws), 
                                  max(many_draws), by = 1))

```
:::

:::{.content-hidden when-format="html"}
```{r}
#| fig-width: 4
#| fig-height: 3
#| fig-align: center
#| code-fold: false
#| echo: false

many_draws <- 
ggplot(data.frame(many_draws), aes(x=many_draws)) + 
  geom_bar(aes(y=..prop..), fill="blue", width = 0.98) + 
  ylab("proportion") + 
  xlab("sum of two draws") + 
  scale_x_continuous(breaks = seq(min(many_draws), 
                                  max(many_draws), by = 1))

```
:::

:::{.callout-warning icon=false}

## Ask yourself

We know all the possible outcomes of summing a pair of dice (between $2$ and $12$ spots). Why not make a box with tickets labeled $2, 3, 4, \ldots, 12$ and draw *once* from that box? If we did indeed want a box from which we would only draw once, what would the box be? (Hint: How many possible outcomes would there be if you rolled a pair of dice. )
:::



<!--## Ideas in Code

You can run the simulations yourself using the code below. Copy and paste it into RStudio and play around with it.

First, lets define two vectors called **die** and **pair_of_dice** that represent the outcome spaces of rolling a die once and rolling a pair of dice once, respectively. 

```{r}
#| code-fold: false

die <- 1:6

die

pair_dice <- c( 2, 3, 3, rep(4,3), rep(5,4), rep(6, 5), rep(7,6), 
                rep(8,5), rep(9,4), rep(10,3), rep(11,2), 12)

pair_dice

```


We used a new function here called `rep(x, n)`. This function has two arguments, the first `x` is a vector of any type, and the second `n` is an integer. It returns an object creates a vector of the same type as`x` by repeating `x`, `n` times. For example`rep(2,5)` gives `r rep(2,5)`, `rep(2:4,5)` gives `r rep(2:4,5)`.

Now let's set up the first game, and repeat it 1000 times. You can change the number of simulations below. The first line has another new function called $\texttt{set.seed()}$. We write this with an integer argument that can be any integer you like, this integer is called the *seed*. Using this function ensures that any time you are using a random number generator (which you do when you sample at random), you will be able to reproduce your results as long as you use the same seed.

```{r}
#| code-fold: false

##### de mere first game ############

set.seed(123123) 
# This ensures that each time we run this code, we will get the same results.

num_simulations <- 1000 
# specifying the number of simulations, or the number of times we will play

# now we will play the game of rolling the die 4 times num_simulations times
die_4 <- replicate(num_simulations, sample(die, 4, replace = TRUE)) 

# the results of play are saved as a numerical array (a matrix), not a data frame
# so we save it as a data frame, and transpose the rows and columns so each row is the 
# result of one game
die_4_df <- data.frame(t(die_4))

# the next two lines of code are to make our data frame look better
# and you can ignore them

colnames(die_4_df) <- paste("roll", sep = " ", 1:4) 

rownames(die_4_df) <- paste("simulation", sep = " ", 1:num_simulations)

# let's take a look at our data frame
head(die_4_df)


# we will create a new column that checks, for each play (each row), if there is at least 
# one 6. What will be the values in this column? Break the pipe and check!
# Finally, we will compute the proportion of plays in which at least one 6 was rolled.
die_4_df |> 
  mutate(at_least_one_six = if_any(everything(), ~ . == 6)) |>
  summarise(prop_wins = mean(at_least_one_six))
```

Let's repeat the simulation for the second game, in which we roll a pair of dice 24 times and record a win if at least one double six is rolled. 

Note that the number of simulations is defined above.

```{r}
#| code-fold: false

set.seed(123123)

######## de mere second game  ############

# define the outcome space from rolling a pair of dice listing all the outcomes, repeated
# the number of ways that can occur. For example we can get 3 by rolling a 2, 1 or a 1, 2
# note that we can get 12 in exactly 1 way, by rolling a double six.


pair_dice <- c( 2, 3, 3, rep(4,3), rep(5,4), rep(6, 5), rep(7,6), 
                rep(8,5), rep(9,4), rep(10,3), rep(11,2), 12)

## note, not ideal because if see 7 don't know how we got it

# play the game on repeat
dice_24 <- replicate(num_simulations, sample(pair_dice, 24, replace = TRUE) )

# make a data frame
dice_24_df <-  data.frame(t(dice_24)) 

# make the data frame easier to read
colnames(dice_24_df) <- paste("roll", sep = " ", 1:24)
rownames(dice_24_df) <- paste("simulation", sep = " ", 1:num_simulations)


#head(dice_24_df)

dice_24_df |> 
  mutate(at_least_one_boxcars = if_any(everything(), ~ . == 12)) |>
  summarise(prop_wins_game_2 = mean(at_least_one_boxcars))
```
-->

## Random variables

What we did, in fact, was define a **function** that assigned a real number to each possible outcome in $\Omega$. In our simulation above, if $\Omega$ is the set of outcomes $\{\text{``Heads'', ``Tails''}\}$,  we assigned the outcome  $\text{``Heads''}$ to the real number $1$, and the outcome  $\text{``Tails''}$ to the real number $0$. By sampling over and over again from `(0,1)`, we got a sequence of $0$'s and $1$'s that was *randomly generated* by our sampling, and then we could do arithmetic on this sequence, such as compute the proportion of times we sampled $1$. To put it in mathematical notation:

$$ X : \Omega \rightarrow \mathbb{R}$$

$X$ is called a *random variable*: variable, because it takes different values on the real line, and random, because it inherits the randomness from the generating process (in this case, the process is tossing a coin).

:::{.def}
**Random variable**
 ~ A random variable is a function that associates real numbers with outcomes from a random experiment which are in an outcome space $\Omega$.
::: 

These assigned numbers have probabilities coming from the probability distribution on $\Omega$. The range of the random variable is the set of all the possible values that $X$ can take. We usually denote random variables by $X, Y, \ldots$ or capital letters towards the end of the alphabet.  We write statements about the values $X$ takes, such as $X = 1$ or $X = 0$. Note that $X = 1$ is an *event* and it may be true or not. The probability of such events is written as $P(X = x)$, where $x$ is a real number.

:::{.def}
**Probability distribution of a random variable $X$**
 ~ The set of possible values of $X$, along with the associated probabilities, is called the *probability distribution* for the random variable $X$.
:::

![](images/rv-picture.png){fig-align="center" width="600"}


:::{.def}
**Discrete and continuous random variables**
 ~ Discrete random variables are restricted to take *particular* values in an interval, they cannot take just any value, as opposed to continuous random variables which can take any value in some specified interval. Note the similarity to discrete and continuous quantitative data types. 
:::

#### Examples of discrete random variables 
- The number of heads in $3$ tosses of a fair coin: The assignment is similar to the outcomes from a single toss, except now we have the possible outcome from tossing a coin three times. For example, the outcome $\texttt{HHH}$ is assigned the number 3, the outcomes $\texttt{HHT, HTH, THH}$ are all assigned the number 2 etc. Note that even though we should write $X(\texttt{HHH}) = 3$, it is common to write just $X = 3$.
- The number of tosses until the coin lands heads for the first time: If $X$ is the random variable representing the number of tosses until a coin lands heads, the smallest value $X$ can take is 1 (you need at least 1 toss), and there is no upper bound, since in theory, one could keep tossing the coin forever and it could land tails every single time. For this reason, the number of tosses until the coin lands heads for the first time is called a counting random variable.
- The number of people that arrive at an ATM in a day: This is also a counting random variable, as described.

#### Examples of continuous random variables 

In all of the following, we do not restrict the value taken by the random variable.

- Time between consecutive people arriving at an ATM
- Price of a stock
- Height of a randomly selected stat 20 student


#### Example: Making bets on red in Roulette

![](images/roulette-us.jpg){fig-align="center" width="400"}

The roulette wheels used in Las Vegas have 38 numbered slots, numbered from $1$ to $36$, of which 18 are colored red, and 18 black. There are two green slots numbered with zero and a double zero. As the wheel spins, a ball is sent spinning in the opposite direction. When the wheel slows the ball will land in one of the numbered slots. Players can make various bets on where the ball lands, such as betting on whether the ball will land in a red slot or a black slot. If a player bets one dollar on red, and the ball lands on red, then they win a dollar, in addition to getting their stake of one dollar back. If the ball does not land on red, then they lose their dollar to the casino. Suppose a player bets six times on six consecutive spins, betting on red each time. Their *net gain* can be defined as the amount they won minus the amount they lost. Is net gain a random variable? What are its possible values (write down how much the player can win or lose in one spin of the wheel, then two, and so on)? 

<details> <summary> Check your answer </summary>

Yes, net gain *is* a random variable, and its possible values are: $-6, -4, -2, 0, 2, 4, 6$. (Why?)

</details>


### The probability distribution of a random variable $X$

We can list the values taken by a random variable in a table, along with the probability that the variable takes a particular value. This table is called the *distribution table* of the random variable. For example, let $X$ be the number of heads in $3$ tosses of a fair coin. The probability distribution table for $X$ is shown below. The first column should have the possible values that $X$ can take, denoted by $x$, and the second column should have $P(X = x)$. We should make sure that the probabilities add up to 1: $\displaystyle \sum_x P(X = x) = 1$.

| $x$ |  $P(X = x)$  |
|:------:|:---------:|
|  $0$  |  $\displaystyle \frac{1}{8}$ |
|  $1$  | $\displaystyle \frac{3}{8}$ |
|  $2$  | $\displaystyle \frac{3}{8}$ |   
|  $3$  | $\displaystyle \frac{1}{8}$ |   


### The probability mass function or pmf of a discrete random variable
 
:::{.def}
**Probability mass function (pmf) of a discrete random variable $X$**
 ~ The **pmf** of a discrete random variable $X$ is defined to be the function $f(x) = P(X = x)$.
:::

We can write down the definition of the function $f(x)$ and it gives the same information as in the table:

$$
f(x) = \begin{cases}
          \frac{1}{8}, \; x = 0, 3 \\
          \frac{3}{8}, \; x = 1, 2 
  \end{cases}
$$

We see here that $f(x) > 0$ for only $4$ real numbers, and is $0$ otherwise. We can think of the total probability mass as $1$, and $f(x)$ describes how this mass of $1$ is distributed among the real numbers. It is often easier and more compact to define the probability distribution of $X$ using $f$ rather than the table. 


## Special distributions

### Bernoulli distribution

This is the simplest discrete distribution: $X$ be a random variable that takes the value $1$ with probability $p$ and the value $0$ with probability $1-p$, then $X$ is called a *Bernoulli* random variable. We say that $X$ is Bernoulli with parameter $p$, because if we know $p$, we can calculate the probabilities associated with $X$. We can visualize the distribution and the pmf using a probability histogram. This is similar to the plots of distributions that we have seen before. It is called a ``histogram'' for a couple of reasons. Firstly, we want to think about areas, with the total area of the bins being 1. Secondly, we will use the same type of plot for continuous random variables, and it can easily generalize. 

![](images/bernoulli.png){fig-align="center" width="500"}

:::{.def}
**Parameter of a probability distribution**
 ~ A constant(s) associated with the distribution. If you know the parameters of a probability distribution, then you can compute all the values of $f(x)$.
:::

### Discrete uniform random distribution

Let's suppose $X$ takes the values $1, 2, 3, \ldots, n$ with $P(X = k) = \displaystyle \frac{1}{n}$ for each of the $k$ from $1$ to $n$. We call $X$ a *discrete uniform* random variable, and the probability distribution is called the discrete uniform probability distribution: $P(X = k) = \displaystyle \frac{1}{n}$ for $1 \le k \le n$. Think of die rolls, where $n = 6$ for a standard die. If $X$ is the outcome when we roll a die, then $X$ is called the discrete uniform random variable with $n = 6$. In fact, the only thing we need to know, in order to compute the probability that $X$ will take a particular value is $n$. We call $n$ the *parameter* of the discrete uniform distribution.


##### Rolling a pair of dice and summing the spots
Suppose we roll a pair of dice and sum the spots, and let $X$ be the sum. Is $X$ a discrete uniform random variable?

<details> <summary> Check your answer </summary>

No. $X$ takes discrete values: $2, 3, 4, \ldots, 12$, but these are **not** equally likely. Can you compute their probabilities?

</details>

### Binomial distribution

Suppose a particular random experiment has two possible outcomes which we designate as a ''success'' or a ``failure'', where the probability of a success on any trial is $p$, regardless of what happens on any other trial. Suppose we repeat this experiment $n$ times, and let $X$ count the number of successes in $n$ **independent** trials of this random experiment (think tossing a coin $n$ times, and counting the number of heads). The independence of the trials is a consequence of the probability of success $p$ staying the same for every trial. Then 
$$P(X = k ) = \binom{n}{k} p^k (1-p)^{n-k} $$
where $k$ takes the values $0, 1, 2, \ldots, n$, and $\displaystyle \binom{n}{k} = \frac{n!}{k!(n-k)!}$ and is read as "$n$ *choose* $k$". $\displaystyle \binom{n}{k}$ is called the binomial coefficient.[^binom]

We say that $X$ has the *binomial* distribution with parameters $n$ and $p$ and write this as $X \sim Bin(n, p)$. Note that the Bernoulli distribution is a special case of the binomial, with $n = 1$.


#### Example: Proportion of Californians that have had at least one positive COVID test.
Based on the December 2022 report "State of the COVID-19 Pandemic"[^covid], about 35% of Americans reported having at least one positive COVID test. Suppose we survey 10 California residents, sampling them *with* replacement from a database of voters, what is the probability that more than $3$ of the individuals in our sample would have had at least one positive COVID test?


In this example, since we are counting the number of individuals in our sample that have had at least one positive COVID test, such an individual would count as a "success", because a success is whatever outcome we are counting. We can now set up our random variable $X$:

Let $X$ be the number of individuals in our sample who have had at least one positive COVID test.

Then $X \sim Bin(10, p = 0.35)$. (Consider why these are the parameters.s)
Using the complement rule, 

$P(X > 3) = 1 - P(X \le 3) = 1 - \left(P(X = 0) + P(X = 1) + P(X = 2) + P(X = 3)\right)$. 

Note that since these events are mutually exclusive, we can use the addition rule. This gives us:

$$ 
\begin{aligned}
P(X > 3) &= 1 - \left(\binom{10}{0}(0.35)^0 (0.65)^{10} + \binom{10}{1}(0.35)^1 (0.65)^9 + \binom{10}{2}(0.35)^2 (0.65)^8 + \binom{10}{3}(0.35)^3 (0.65)^7 \right)\\
& \approx 0.486\\
\end{aligned}
$$

In `R` we can use a special function to compute the binomial probabilities $f(k) = P(X=k)$. It is called `dbinom(x, size, prob)` and takes as input the $k$ that we want (`x`), the number of trials $n$ (`size`), and $p$ (`prob`). In this example, `x` are the values of interest, or 0, 1, 2; `size` is the parameter $n$, which is 10, and `prob` is the probability of a success.

```{r}
#| code-fold: false

1 - dbinom(0, size = 10, prob = 0.35) - 
  dbinom(1, size = 10, prob = 0.35) - 
  dbinom(2, size = 10, prob = 0.35)
```

As stated above, we can define *events* by the values of random variables.
For example, let $X = Bin(10,0.4)$. In words $X$ counts the number of successes in 10 trials.
 Given this $X$, what are the following events in words?

- $X = 5$
- $X \le 5$
- $3 \le X \le 8$

What are these events in words? What are their probabilities?

<details> <summary> Check your answer </summary>
- $X$ is the number of successes in ten trials, where the probability of success in each trial is 40%. $X = 5$ is the event that we see exactly five successes in the ten trials, while $X \le 5$ is the event of seeing *at most* five successes in ten trials. The last event, $3 \le X \le 8$ is the event of at least three successes, but not more than eight, in ten trials.
We will use `dbinom()` to compute the probabilities $P(X = x)$ or the pmf values.

```{r}
#| code-fold: false

# P(X=5)
dbinom(x = 5, size = 10, prob = 0.4)

# P(X <= 5)
dbinom(x = 0, size = 10, prob = 0.4) + dbinom(x = 1, size = 10, prob = 0.4) + 
  dbinom(x = 2, size = 10, prob = 0.4) + dbinom(x = 3, size = 10, prob = 0.4) +
  dbinom(x = 4, size = 10, prob = 0.4) + dbinom(x = 5, size = 10, prob = 0.4)

# P(3 <= X <= 8)
dbinom(x = 3, size = 10, prob = 0.4) + dbinom(x = 4, size = 10, prob = 0.4) + 
  dbinom(x = 5, size = 10, prob = 0.4) + dbinom(x = 6, size = 10, prob = 0.4) +
  dbinom(x = 7, size = 10, prob = 0.4) + dbinom(x = 8, size = 10, prob = 0.4)

```


</details>

### Hypergeometric distribution

In the example above, we sampled from the population of California *with* replacement. Usually we sample *without* replacement. Suppose our population consists of $N$ units. If, each time we draw a unit for our sample, all the units are equally likely to be drawn (just like in the box model), a sample drawn without replacement is called a *simple random sample*. Suppose our population consists of just two types of units that we call "successes" and "failures" (as usual, a "success" is whatever outcome we are interested in), and we draw a sample of size $n$ without replacement. Now, the probability of success will *not* stay the same on each draw. If we let $X$ be the number of successes in $n$ draws, then we have that 

$$ P(X = k) = \frac{\binom{G}{k} \times \binom{N-G}{n-k}}{\binom{N}{n}} $$
where $N$ is the size of the population, $G$ is the total number of successes in the population, and $n$ is the sample size (so $k$ can take the values $0, 1, \ldots, n$ or $0, 1, \ldots, G$, if the number of successes in the population is smaller than the sample size.)

#### Example: Gender discrimination at large supermarket?

A large supermarket chain in Florida (with 1,000 employees) occasionally selects employees
to receive management training. A group of women there claimed that
female employees were passed over for this training in favor of
their male colleagues. The company denied this claim. 
(A similar complaint of gender bias was made about promotions and pay
for the 1.6 million women who work or who have worked for
Wal-Mart. The Supreme Court heard the case in 2011 and ruled in favor
of Wal-Mart, in that it rejected the effort to sue Wal-Mart.)[^wm] If we set this up as a probability problem, we might ask the question of how many women have been selected for executive training in the last 10 years. Suppose no women had ever been selected in 10 years of annually selecting one employee for training. Further suppose that the number of men and women were equal, and suppose the company claims that it draws employees at random for the training, from the 1,000 eligible employees. If $X$ is the number of women that have been picked for training in the past 10 years, what is $P(X = 0)$?

Since there are 1,000 employees, and half are women, we have $G = N-G = 500$. Of the 10 picked, none are women. We are picking a sample of size 10 without replacment, therefore we have that:
$$P(X = 0) = \frac{\binom{500}{0} \times \binom{500}{10}}{\binom{1000}{10}} \approx 0.0009$$

The function that we can use in `R` to compute hypergeometric probabilities is called `dhyper(x, m, n, k)`, where `x` is the number of successes in the sample that we are counting (what we call $k$), `m` is $G$ or the number of successes in the population, and $n$ is $N-G$. $k$ is the sample size.

```{r}
#| code-fold: false
dhyper(0, 500, 500, 10)
```

Note that the function used in `R` to compute the binomial coefficient $\binom{n}{k}$ is `choose(n,k)`.

```{r}
#| code-fold: false

choose(500, 0)*choose(500, 10)/choose(1000, 10)

```


### Binomial vs Hypergeometric distributions

Both these distributions deal with the counting the number of successes in a *fixed* number of trials (where each instance of the random experiment that generates a success or a failure is called a trial, for example each toss of a coin or each card dealt from a deck). The difference is that for a binomial random variable, the probability of a success stays the *same* for each trial, and for a hypergeometric random variable, the probability *changes* with each trial. If we use a box of tickets to describe these random variables, both distributions can be modeled by sampling from boxes with each ticket marked with $0$ or $1$, but for the binomial distribution, we sample $n$ times *with* replacement and count the number of successes by summing the draws; and for the hypergeometric distribution, we sample $n$ times *without* replacement, and count the number of successes by summing the draws.

Note that when the sample size is small relative to the population size, there is not much difference between the probabilities if we use a binomial distribution vs using a hypergeometric distribution. Let's pretend in the gender discrimination example above that we are sampling *with* replacement. Now suppose we sample 10 times with replacement, let's use the binomial distribution to compute the chance of never picking a woman. 

```{r}
#| code-fold: false

# n = 10
# p = 0.5

dbinom(0, 10, 0.5)

```


You can see that the values are very close to each other. (Recall that the probability using the hypergeometric distribution was `0.0009331878`.


## The cumulative distribution function $F(x)$

:::{.def}
**Cumulative distribution function (cdf) $F(x)$**
 ~ The cumulative distribution function of a random variable $X$ is defined for *every* real number, and gives, for each $x$, the amount of probability or mass that has been *accumulated* up to (and including) the point $x$, that is, 
$F(x) = P(X \le x)$.
:::

We usually abbreviate this function to cdf. It is a very important function since it also describes the probability distribution of $X$.

For example, if $X$ is the number of heads in $3$ tosses of a fair coin, recall that:

$$ f(x) = \begin{cases}
  \displaystyle \frac{1}{8}, \; x = 0, 3 \\
  \displaystyle \frac{3}{8}, \; x = 1, 2 
  \end{cases} $$

In this case, $F(x) = P(X\le x) = 0$ for all $x < 0$ since the first positive probability is at $0$. Then, $F(0) = P(X \le 0) = 1/8$ after which it stays at $1/8$ until $x = 1$. Look at the graph below:

![](images/cdf.png){fig-align="center" width="700"}

Notice that $F(x)$ is a step function, and right continuous. The jumps are at exactly the values for which $f(x) > 0$. We can get $F(x)$ from $f(x)$ by adding the values of $f$ up to and including $x$, and we can get $f(x)$ from $F(x)$ by looking at the size of the jumps. 

#### Example: Drawing the graph of the cdf

Let $X$ be the random variable defined by the distribution table below. Find the cdf of $X$, and draw the graph, making sure to define $F(x)$ for all real numbers $x$. Before you do that, you will have to determine the value of $f(x)$ for $x = 4$.

<center>


| $x$ |  $P(X = x)$  |
|:------:|:---------:|
|  $-1$  |  $0.2$ |
|  $1$  | $0.3$ |
|  $2$  | $0.4$ |   
|  $4$  | ??  |   

</center>

<details><summary> Check your answer </summary>

Since $\displaystyle \sum_x P(X = x) = \sum_x f(x) = 1$, $f(4) = 1-(0.2+0.3+0.4) = 0.1.$ Therefore $F(x)$ is as shown below.

![](images/cdf-2.png){fig-align="center" width="700"}

</details>


In `R`, we have functions that calculate $F(x)$ for some special distributions, including the binomial and hypergeometric distributions. For a binomial distribution, we use the function `pbinom(x, size, prob)` Similarly, for the hypergeometric distribution, we use `phyper(x, m, n, k)`.

Going back to the example above, let's compute the probabilities using `pbinom(x, 10, 0.4)` and compare them to the probabilities computed earlier using `dbinom(x, size, prob)` 

```{r}
#| code-fold: false

# P(X = 5)
dbinom(x = 5, size = 10, prob = 0.4)

# P(X = 5)
pbinom(5, 10, 0.4) - pbinom(4, 10, 0.4)

# P(X <= 5)
dbinom(x = 0, size = 10, prob = 0.4) + dbinom(x = 1, size = 10, prob = 0.4) + 
  dbinom(x = 2, size = 10, prob = 0.4) + dbinom(x = 3, size = 10, prob = 0.4) +
  dbinom(x = 4, size = 10, prob = 0.4) + dbinom(x = 5, size = 10, prob = 0.4)

# P(X <= 5)
pbinom(5, 10, 0.4)

# P(3 <= X <= 8)
dbinom(x = 3, size = 10, prob = 0.4) + dbinom(x = 4, size = 10, prob = 0.4) + 
  dbinom(x = 5, size = 10, prob = 0.4) + dbinom(x = 6, size = 10, prob = 0.4) +
  dbinom(x = 7, size = 10, prob = 0.4) + dbinom(x = 8, size = 10, prob = 0.4)

# P(3 <= X <= 8)
pbinom(8, 10, 0.4) - pbinom(2, 10, 0.4)
```

What is going on in the last expression? Why is $P(3 <= X <= 8) = F(8) - F(2)$?

<details><summary> Check your answer </summary>

$P(3 <= X <= 8)$ consists of all the probability at the points $3, 4, 5, 6, 7, 8$. 
$F(8) = P(X\le 8) $ is all the probability up to $8$, including any probability at $8$. We subtract off all the probability up to and including $2$ from $F(8)$ and are left with the probability at the values $3$ up to and including $8$, which is what we want.

</details>


## Connections between discrete random variables and draws from a box of tickets

#### Example: the discrete uniform random variable

Say we have a ten sided die. How can we set up a simulation to model for rolling this die once? (We can simulate it using a box of tickets, or in R).

<details><summary> Check your answer </summary>

A box with ten tickets, marked $\fbox{1}, \fbox{2}, \ldots \fbox{10}$; and draw once from this box. 

We can also simulate the probability distribution by drawing over and over again, and looking at the empirical distribution. You can see below that it closely matches the actual probability distribution.

:::{.content-visible when-format="html"}
```{r}
#| fig-width: 10
#| fig-height: 5

die_10 <- seq(from = 1, to = 10, by = 1)
prob_10 <- rep(0.1, 10)

set.seed(12345)

n_rolls <- 10^5

rolls <- sample(die_10, n_rolls, replace = TRUE)

p1 <- data.frame(die_10) %>% 
  mutate(prob_10 = prob_10) %>%
  ggplot(aes(x = factor(die_10), y = prob_10)) +
  geom_col(fill = "goldenrod2", width = 0.98) + 
   ylab("probability") + 
   xlab(" number of spots") + 
  ggtitle("Outcomes from rolling a ten sided die:\n probability distribution")
  

p2 <- data.frame(rolls) %>% 
  ggplot(aes(x=factor(rolls))) + 
  geom_bar(aes(y = after_stat(count)/sum(after_stat(count))), fill="blue", width = 0.98) + 
  ylab("proportion of draws") + 
  xlab("ticket drawn") + 
  ggtitle("Outcomes from rolling a ten sided die:\n empirical distribution") 
 

p1 + p2
```
:::

:::{.content-visible when-format="pdf"}
```{r}
#| fig-width: 10
#| fig-height: 5
#| echo: false
#| message: false
#| warning: false

die_10 <- seq(from = 1, to = 10, by = 1)
prob_10 <- rep(0.1, 10)

set.seed(12345)

n_rolls <- 10^5

rolls <- sample(die_10, n_rolls, replace = TRUE)

p1 <- data.frame(die_10) %>% 
  mutate(prob_10 = prob_10) %>%
  ggplot(aes(x = factor(die_10), y = prob_10)) +
  geom_col(fill = "goldenrod2", width = 0.98) + 
   ylab("probability") + 
   xlab(" number of spots") + 
  ggtitle("Outcomes from rolling a ten sided die:\n probability distribution")
  

p2 <- data.frame(rolls) %>% 
  ggplot(aes(x=factor(rolls))) + 
  geom_bar(aes(y = after_stat(count)/sum(after_stat(count))), fill="blue", width = 0.98) + 
  ylab("proportion of draws") + 
  xlab("ticket drawn") + 
  ggtitle("Outcomes from rolling a ten sided die:\n empirical distribution") 
 

p1 + p2
```
:::

#### Example: the binomial(*n*, *p*) random variable

Let $X \sim Bin(30, 0.5)$. We can simulate the values of this random variable by drawing from the box ![](images/binom-box.jpeg){width="80"} $30$ times, and summing the draws. This will simulate counting the number of successes in $n$ trials. As in the example above, we will plot the probability distribution of $X$ on the left, and the empirical distribution on the right. 

:::{.content-visible when-format="html"}

```{r}
#| fig-width: 10
#| fig-height: 5

box <- c(0,1)
binom_30 <- seq(from = 0, to = 30, by = 1)
prob_30 <- round(dbinom(binom_30, size = 30, prob = 0.5), 3)

set.seed(12345)

n_sims <- 10^4

draws <- replicate(n_sims, sum(sample(box, 30, replace = TRUE)))

p1 <- data.frame(binom_30) %>% 
  mutate(prob_30 = prob_30) %>%
  ggplot(aes(x = factor(binom_30), y = prob_30)) +
  geom_col(fill = "goldenrod2", width = 0.98, color = "white") + 
   ylab("probability") + 
   xlab(" number of successes") + 
  ggtitle("Probabilities for the binomial random variable \n n = 30, p = 0.5") +
  theme_bw()
  

fac_30 <- factor(binom_30, levels = 0:30)

p2 <- data.frame(draws) %>% 
  ggplot(aes(x=draws)) + 
  geom_bar(aes(y=..prop..), fill="blue", width = 0.98, color = "white") + 
  ylab("proportion of draws") + 
  xlab("sum of draws") + 
  ggtitle("Empirical distribution for random numbers generated \n
          from the Bin(30, 0.5) distribution")  +
  theme_bw() +
  scale_x_discrete(limits = factor(seq(0,30)), labels = fac_30, drop = FALSE)
  

p1 + p2

```
:::

:::{.content-visible when-format="pdf"}
```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-width: 10
#| fig-height: 5

box <- c(0,1)
binom_30 <- seq(from = 0, to = 30, by = 1)
prob_30 <- round(dbinom(binom_30, size = 30, prob = 0.5), 3)

set.seed(12345)

n_sims <- 10^4

draws <- replicate(n_sims, sum(sample(box, 30, replace = TRUE)))

p1 <- data.frame(binom_30) %>% 
  mutate(prob_30 = prob_30) %>%
  ggplot(aes(x = factor(binom_30), y = prob_30)) +
  geom_col(fill = "goldenrod2", width = 0.98, color = "white") + 
   ylab("probability") + 
   xlab(" number of successes") + 
  ggtitle("Probabilities for the binomial random variable \n n = 30, p = 0.5") +
  theme_bw()
  

fac_30 <- factor(binom_30, levels = 0:30)

p2 <- data.frame(draws) %>% 
  ggplot(aes(x=draws)) + 
  geom_bar(aes(y=..prop..), fill="blue", width = 0.98, color = "white") + 
  ylab("proportion of draws") + 
  xlab("sum of draws") + 
  ggtitle("Empirical distribution for random numbers generated \n
          from the (30, 0.5) distribution")  +
  theme_bw() +
  scale_x_discrete(limits = factor(seq(0,30)), labels = fac_30, drop = FALSE)
  

p1 + p2
```
:::

[^covid]: <https://www.covidstates.org/reports/state-of-the-covid-19-pandemic>
[^wm]:<https://www.latimes.com/world/la-xpm-2011-jun-20-la-naw-wal-mart-court-20110621-story.html>
[^binom]: <https://en.wikipedia.org/wiki/Binomial_coefficient>

## Summary

- In these notes, we defined random variables, and described discrete and continuous random variables.
- For any random variable, there is an associated probability distribution, and this is described by the probability mass function or pmf $f(x)$. We also defined a function that, for a random variable $X$, and any real number $x$, describes all the probability that is to the left of $x$. This function is called the cumulative distribution function (cdf) of $X$ and is denoted $F(x)$.
- We looked at some special distributions (Bernoulli, binomial, discrete uniform, and hypergeometric)
- We defined functions in `R` that can compute the pmf and cdf for special distributions.
- Finally, we looked at connections between random variables and boxes with tickets, and saw how to simulate some empirical distributions.
