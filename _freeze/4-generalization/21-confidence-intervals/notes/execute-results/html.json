{
  "hash": "8fbc17509601d4e7ee1ec0349ba56174",
  "result": {
    "markdown": "---\ntitle: \"Confidence Interval for a Population Parameter\"\nsubtitle: \"Point and Interval Estimates\"\ndate: \"11/02/2022\"\nformat:\n  html:\n    toc-depth: 4\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\n---\n\n::: {.cell}\n\n:::\n\n\nWhen we have a sample and want to use the data to estimate something about the population, we are making an *inference*.\nThis might be as simple as using the mean of a sample to infer the mean of the population. \nWe have seen so far in this unit that it's important to use chance to select our sample so that it is representative of our population,\nand likewise a representative sample is needed to make inferences about the population. \n\nIn these notes, we will delve a bit deeper into the triptych from the last set of notes to help you solidify the connections between the population, sample, and chance mechanism that produced the sample. After that, we will go on to use the triptych to make inferences about the population. \n\nRecall that there are three parts to the triptych:\n\n+ the population that we are interested in making generalizations about;\n+ the chance process used to select a sample from the population (we have focused on the Simple Random Sample (SRS) where draws are made at random without replacement, and on Sampling with Replacement);\n+ the sample, the data that the chance process gave you. \n\n\nLet's build a triptych one panel at a time for an example. \n\n## Example: Food Safety Scores\n\nEvery year, the city of San Francisco's health department visits all the restaurants in the city and inspects them for food safety. Each restaurant is given an inspection score; these range from 100 (perfectly clean) to 48 (serious potential for contamination). \nWe have these scores from 2016.\n\n <!-- ADD REF -->\n\n### The Population Distribution\n\nOur population consists of the restaurants in San Francisco. Since the data are published online for all restaurants, we have a census of scores for every restaurant in the city. That is: the target population is the access frame is the sample.\n\n*Population* = *BOX*: There are 5766 tickets in the box, one for each restaurant. Since we are interested in the inspection score,\neach ticket has the restaurant's score marked on it.\n\n+ The histogram of the values written on the tickets is our *population histogram*. It shows the distribution of scores in the population. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(url(\"https://www.dropbox.com/s/nbgw1uzj5ccwce2/fs_scores.rda?dl=1\"))\n\npop_dist <- ggplot(as.data.frame(fs_scores), aes(x=fs_scores, y=..density..)) + \n      geom_histogram(breaks=seq(47.5, 100.5, by = 1), \n                 color=\"black\", fill=\"gray\") + \n      xlab(\"Food Safety Scores\") +\n      ylab(\"Proportion\") +\n      ggtitle(\"Population Distribution\")\n\npop_dist\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-2-1.png){width=480}\n:::\n:::\n\n\n\n The *population histogram* has several interesting features. How would you describe it?\n \n<details><summary>Check your answer</summary>\nThe population is skewed left with a long left tail. The highest possible score is 100. It appears that even scores are more popular than odd scores for scores in the 90s; in fact there are no scores of 99, 97, and 95. \n</details>\n\n+ The *population mean* (aka the *population parameter* and for this class aka the *box average*) is 87.6.\n\n+ The *population SD* (aka the box SD) is 8.9.\n\n\n\n### The Empirical Distribution\n\nAlthough we have data on all of the restaurants in the city, impagine that you're an inspector who has visited a simple random sample of 100 restaurants. That is, you draw 100 times without replacement from the population (box). Your sample is:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(103122)\nsamp_scores <- sample(fs_scores, 100)\n\nsamp_scores\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1]  92  86  90  94  90 100  80  72 100  94 100  88  83  98  74  68  88  82\n [19]  92  86  72  73  92  66 100  96  74 100  83  86  78  77  71  70  76  98\n [37]  66  98  98 100  94  85  58  92  96  90  96  72  92  92  77  83  90  96\n [55]  88  67  85  91  90  80  94  75  96  83  84  81  90  92  94  86  86  94\n [73]  71 100  81  92  75  83  84  96  70  86  92  85  90  86  92 100  83  88\n [91] 100  89  87  83  88  60  94  87  92  93\n```\n:::\n:::\n\n+ The *empirical distribution* aka the empirical distribution appears below.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nemp_dist <- ggplot(as.data.frame(samp_scores), aes(x=samp_scores, y=..density..)) + \n      geom_histogram(breaks=seq(47.5, 100.5, by = 1), \n                 color=\"black\", fill=\"gray\") + \n      xlab(\"Food Safety Scores\") +\n      ylab(\"Proportion\") +\n      ggtitle(\"Empirical Distribution\")\n\nemp_dist\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-4-1.png){width=480}\n:::\n:::\n\n\n+ The *sample mean* is 86.27.\n\n+ The *sample SD* is 9.9.\n\nNotice: The empirical distribution resembles the population distribution. It's not a perfect match but the shape is similar. The sample average and the sample SD are also close to but not the same as the population average and SD.\n\n### The Sampling Distribution\n\nIf you compared your sample to that of another inspector who took visited 100 restaurants, their sample would not be identical to yours, but it would still resemble the population distribution, and its mean and SD would be close to those of all the restaurants in the city. \n\nThe distribution of the possible values of the sample mean of a SRS of 100 restaurants is a probability distribution. It is called the *sampling distribution* of the mean (of the sample). We can use it to, for example, find the chance that the sample mean will be over 88, or the chance that the sample mean will be between 85 and 95. \n\nWe use simulation to approximate this probability distribution. We repeat 100,000 times the process of drawing a SRS of 100 tickets from the box and computing the mean.  The distribution of the 100,000 simulated sample means is close to the true sampling distribution. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10312022)\nsamp_means <- replicate(100000, mean(sample(fs_scores, 100)))\n```\n:::\n\n\nHere are a few of the 100,000 sample averages: 88.24, 88.92, 87.39, 88.99, 87.46, 87.87.\n\n+ The *sampling distribution* looks like the following. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsampling_dist <- ggplot(as.data.frame(samp_means), \n                        aes(x=samp_means, y=..density..)) + \n      geom_histogram(bins = 45, \n                 color=\"black\", fill=\"gray\") + \n      xlab(\"Average Food Safety Scores for a SRS of 100\") +\n      ylab(\"Proportion\") +\n      ggtitle(\"Sampling Distribution\")\n\nsampling_dist\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-6-1.png){width=480}\n:::\n:::\n\n\n+ The *sampling distribution mean* (aka the expected value) is 87.6.\n\n+ The sampling distribution SD, which is called the *Standard Error*, is 0.9. This convention of using a different name for the SD for a probability distribution of a statistic helps keep straight which kind of standard deviation we're talking about.   \n\nNotice: The sampling distribution of the sample mean doesn't look anything like the population or sample. Instead, it's roughly symmetric in shape with a center that matches the population mean, and a small SE. The size of the SE reflects the fact that the sample mean tends to be quite close to the population mean. \n\nAgain, the sampling distribution provides the probability distribution for the possible values of the sample mean. From this distribution, we find that the chance the sample mean is over 88 is about 0.33, and the chance the sample mean is between 85 and 95 is roughly, 1. \n\n### Putting the Three Panels Together\n\nLet's look at these three aspects of the sample design side-by-side.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npop_dist + sampling_dist + emp_dist\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-7-1.png){width=864}\n:::\n:::\n\n\n|             | Population  | Sampling | Empirical   |\n| ----------- | ----------- | ----------- | ----------- |\n| Distribution | left skew | bell-shaped / normal | left skew |\n| Mean | 87.6      | 87.6      | 86.27  |\n| SD | 8.9      | 0.89      | 9.9  |\n| Size| 5766 | 1 (sample statistic)  | 100 |\n\nWe note that:\n\n1. The population mean and the expected sample average are the same.\n\n2. The SD of the population and the SE of the sample averages are related in the following:\n\n$$SE(sample~mean) = SD(pop)/\\sqrt{n}$$\nActually, the above formula is true for a random sample with replacement. When we have a SRS, the exact formula is\n\n$$SE(sample~mean) = \\sqrt{\\frac{N-n}{N-1}} SD(pop)/\\sqrt{n}$$\n\nThis additional term, called the *finite population correction factor*, adjusts for the fact that we are drawing without replacement. Here $N$ is the number of tickets in the box (the size of the population) and $n$ is the number of tickets drawn from the box (the size of the sample).\n\nTo help make sense of this correction factor, think about the following two cases:\n\n+ Draw $N$ tickets from the box (that is, $n = N$).\n+ Draw only one ticket from the box.\n\nWhat happens to the SE in these two extreme cases?\n\n<details>\nIn the first case, you will always see the enitre population if you are drawing without replacement. So, the sample mean will exactly match the population mean. The sampling distribution has no variation, so $SE = 0$.\n\nIn the second case, since you take only one draw from the box, it doesn't matter if you replace it or not. So the SE for a SRS should match the SE when sampling with replacement in this special case. And it does!\n</details>\n\n3. The histogram of the sample averages is not skewed like the histogram of the box, on the contrary, it is symmetric about the middle and bell-shaped, like the normal curve.\n\n4. The histogram of our sample of 100 resembles the population histogram. \n\n5. Since 100 is a pretty large sample,\n\n$$\n\\begin{aligned}\nmean(pop) &\\approx mean(sample) \\\\\nSD(pop) &\\approx SD(sample) \\\\\n\\end{aligned}\n$$\n\nNow we're ready to make inferences.\n\n## Inference for a Population Average\n\nDrawing on our understanding of the triptych, we ask: \n\n*What happens when you don't see the population, you just have your sample, and you want to make an inference about the population?*\n\nYour triptych has holes in it!\n\n![](images/triptych-sample-only.jpg){fig-align=center width=400}\n\nWell, you can use your sample average to infer the population average. This is called a *point estimate* for the population parameter. \n\nBut can you do better than that? Can you bring in more of the information that you have learned from the triptych?\nFor example, can you accompany your point estimate with a sense of its accuracy? Ideally, this would be the SE of the sample mean. Unfortunately, you don't know the SE because it depends on the population SD. So now what do you do?\n\n<details>\nThe triptych tells you that the sample SD is close to the population SD (when you have a SRS). So we can substitute the sample SD into the formula for the SE.\n\n$$ SE(sample~mean) \\approx \\frac{SD(sample)}{\\sqrt{n}}$$\n</details>\n\n\nWhen presenting your findings, you might say, that based on a SRS of 100 restaurants in San Francisco, the average food safety score is estimated to be 86 with a standard error of about\n1.\n\n### Accuracy, sample size, and population size\n\nSuppose someone took a sample of 25 restaurants and provided an estimate of the average food safety score. Is that only 1/4 as accurate because the sample is 1/4 the size of ours?\n\nSuppose someone took a sample of 100 restaurants in New York City where there are 50,000 restaurants (this is a made up number). Is their estimate only 1/10 a accurate because the number of tickets in the box is 10 times yours?\n\nWe can use the formula for the SE to answer these questions. Or, if you like, you can carry out your own simulation study to discover them for yourself.  Recall the formula for the SE is \n\n$$\\sqrt{\\frac{N-n}{N-1}} \\frac{SD(pop)}{\\sqrt{n}}$$ \n\nIn the table below, we have calculated these SEs for a generic value of $SD(pop)$.\n\n| Population |  | Sample |  |\n| --- | --- | --- | --- |\n|  | 25 | 100 | 400 |\n| 500        | $0.98 SD(pop)/5$ | $0.90SD(pop)/10$ | $0.45SD(pop)/20$ |\n| 5,000      | $SD(pop)/5$ | $0.99 SD(pop)/10$ | $0.96 SD(pop)/20$ | \n| 50,000     | $SD(pop)/5$ | $SD(pop)/ 10$ | $SD(pop)/ 20$ | \n\n\nWhat do you notice about the relationship between sample size and population size and SE?\n\n<details>\n+ When the sample size is large relative to the population, then the correction factor needs to be taken into consideration. When, it's small relative to the population, you can ignore the correction factor.\n+ The absolute size of the population doesn't enter into the accuracy of the estimate, as long as the sample size is small relative to the population.\n+ A sample of 400 is twice as accurate as a sample of 100, which in turn is twice as accurate as a sample of 25 (assuming the population is relatively much larger than the sample). The accuracy improves according to the square root of the sample size. \n</details>\n\n## Confidence Interval for a Population Average\n\nConfidence intervals bring in more information from the triptych.\nThe confidence interval provides an interval estimate, instead a point estimate, that is based on the sampling distribution of the statistic.\n\nWe have seen that the sampling distribution is roughly normal in shape. (Note that this is not always the case. We'll come back to this point later.) We can fill in some of the holes in the triptych with approximations.\n\n![](images/triptych-subs.jpg){fig-align=center width=400} \n\nThe sketch that we have added in the middle panel shows a sampling distribution that looks like the normal curve. Let's make a quick\ndigression to talk about the normal curve.\n\n### Normal Curve\n\nThe *standard normal curve* looks like the following: \n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nggplot(data = data.frame(x = c(-3, 3)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylab(\"\") +\n  scale_y_continuous(breaks = NULL) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=384}\n:::\n:::\n\n\nIt's symmetric, unimodal, and has 'normal tails'.\nThe area under the curve corresponds to chance, \nFor example, the chance a value chosen at random from a \nprobability distribution that follows the normal curve is less than 0 is 1/2.\nWe can use `pnorm()` and symmetry to find other probabilities. \nWe put a few that are commonly used in a table.\n\n| Interval | Area under the normal curve |\n| ---- | ---- |\n| Between -1 and 1 | 0.68 |\n| Between -1.96 and 1.96 | 0.95 |\n| Between -2.58 and 2.58 | 0.99 |\n\nIf a distribution looks roughly normal in shape, then when we standardize the values (subtract the mean and divide by the SD) to get a standard normal curve. This is helpful when we want to calculate probabilities.\n\nNow back to the task of making an interval estimate.\n\n### Normal Confidence Intervals\n\nWhen the sampling distribution is roughly normal, \nthen 95% of the time, the process of taking a SRS will give us a sample mean in the interval, \n\n$$[mean(pop) - 1.96 SE, mean(pop) + 1.96SE]$$\nAfter, we take our sample, and we observe the value of our sample mean, we can use this notion to construct a 95% confidence interval for the population parameter.\nThe interval is as follows:\n\n$$[mean(sample) - 1.96 SE, mean(sample) + 1.96SE]$$\nFor your sample, the 95% confidence interval is: \n[84.3,\n88.2 ]\nand you would say, \n\nI am 95% confident that the population mean is between 84.3 and \n88.2.\n\nWe call this a confidence interval because 95% of the time, an interval constructed in this way will contain the population mean. \nFor the particular interval that you have created, you don't know if it contains the population mean or not. This is why we use the term *confidence* to describe it. We do not use terms such as chance or probability at this point. Chance comes into play when taking the sample, after that our confidence interval is an observed value, and we don't refer to it in probability terms.\n\n### Confidence not Chance\n\nTo make this idea clearer. Let's take 100 samples of size 25 from the restaurant scores, and calculate a 95% confidence interval for the population mean for each of our 100 samples. How many of these 100 confidence intervals do you think will include the population mean? \nLet's simulate it!\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(10312022)\nsamp25_means <- replicate(100, mean(sample(fs_scores, 25)))\n\nlower <- samp25_means - 1.96 * sd(fs_scores)/5\nupper <- samp25_means + 1.96 * sd(fs_scores)/5\ntrial <- 1:100\ncover <- (mean(fs_scores) >= lower) & (mean(fs_scores) <= upper)\nCIs <- data.frame(trial, lower, upper, cover)\n```\n:::\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nci100 <- ggplot(CIs, aes(y = trial)) +\n  geom_segment(aes(x=lower, y=trial, xend=upper, yend=trial, color= cover), \n               show.legend=FALSE) +\n  annotate(\"segment\", x=mean(fs_scores), xend=mean(fs_scores),\n                y=0, yend=101, color=\"black\") +\n  labs(x=\"95% Confidence Interval\", y = \"Iteration\") +\n  theme_minimal()\n\nci100\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-10-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nWe have found that 95 of the 100 confidence intervals cover the population parameter. This is expected. If we simulate another 100 times, we may get a different number, but it is likely to be close to 95. \n\n## Confidence Interval for a Population Proportion\n\nTo gain practice with making confidence intervals, we turn to another example. This time we sample from a 0-1 box. You will see that the process is very much the same, although there are a few simplifications that arise due to the nature of the box.\n\nSuppose we only want to eat at restaurants with food safety scores above 95. Let's make a confidence interval for the proportion of restaurants in San Francisco with scores over 95. \n\nTo tackle this problem, we can modify our box. Since we need only to keep track of whether a score is at least 95 or not, we can replace the scores on the tickets with 0s and 1s, where 1 indicates the score is 95 and above. Of the 5766 \nrestaurants in San Francisco, 1240 have scores of 95 and above. So our box has 5766 tickets in it, and 1240 are marked 1, and 4526 are marked 0. This time let's take a SRS of 25.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhi_scores <- 0 + (fs_scores >= 95)\nset.seed(10312022)\nhi_score_samp <- sample(hi_scores, 25)\nhi_score_means <- replicate(100000, mean(sample(hi_scores, 25)))\n```\n:::\n\n\nThe triptych appears as \n\n::: {.cell}\n\n```{.r .cell-code}\nhi_score_dist <- ggplot(as.data.frame(hi_scores), \n                        aes(x=hi_scores, y=..density..)) + \n      geom_histogram(breaks=seq(-0.5, 1.5, by = 1), \n                 color=\"black\", fill=\"gray\") + \n      xlab(\"Scores over 95\") +\n      ylab(\"Proportion\") +\n      ggtitle(\"Population\")\n\nhi_samp_dist <- ggplot(as.data.frame(hi_score_samp), aes(x=hi_score_samp, y=..density..)) + \n      geom_histogram(breaks=seq(-0.5, 1.5, by = 1), \n                 color=\"black\", fill=\"gray\") + \n      xlab(\"Scores over 95\") +\n      ylab(\"Proportion\") +\n      ggtitle(\"Empirical Distribution\")\n\nhi_sampling_dist <- ggplot(as.data.frame(hi_score_means), \n                        aes(x=hi_score_means, y=..density..)) + \n      geom_histogram(bins = 20, \n                 color=\"black\", fill=\"gray\") + \n      xlab(\"Proportion over 95\") +\n      ylab(\"Proportion\") +\n      ggtitle(\"Sampling Distribution\")\n\nhi_score_dist + hi_sampling_dist + hi_samp_dist\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-12-1.png){width=864}\n:::\n:::\n\n\n\n|             | Population  | Sampling | Empirical   |\n| ----------- | ----------- | ----------- | ----------- |\n| Distribution | left skew | bell-shaped / normal | left skew |\n| Mean | 0.22      | 0.22      | 0.2  |\n| SD | 0.4      | 0.08      | 0.4  |\n| Size| 5766 | 1  | 25 |\n\nIn the special case of a 0-1 box:\n\n+ The population average is the proportion of 1s in the box, let's call this parameter $p$. \n+ The population SD is $\\sqrt{p(1-p)}$.\n+ The sampling distribution has mean $p$.\n+ Ignoring the finite population correction factor, the SE of the sample proportion is $\\frac{\\sqrt{p(1-p)}}{\\sqrt{n}} =$ 0.08. \n+ The sample average is the proportion of 1s in the sample.\n+ The sample SD follows the formula for the population SD, with the sample proportion taking the place of the population proportion.\n\nLet's make a 99% confidence interval for the proportion of restaurants with scores at least 95, \n[0.09, 0.31 ].\n\n## Summary\n\nIn summary, we have used the box model, and the accompanying triptych, to understand how a chance process can be used to make inferences from a sample to a population. We have restricted ourselves to the simple random sample and the case of sampling with replacement, but you can imagine how simulation can help us make estimates and compute standard errors under other sampling schemes.\n\nWe saw how the size of the sample impacts the standard error of the estimate. The larger the sample, the more accurate our estimates are and in particular the accuracy improves according to $1/\\sqrt{n}$. We also found that the size of the population doesn't impact the accuracy, as long as the sample is small compared to the population.   \n\nWe made confidence intervals for population averages and proportions. But this approach can be extended to other properties of a population, such as the median of a population, or the coefficient in a regression equation. (We will consider the regression coefficient in the next section.) \n\nThe confidence intervals that we have made are approximate in the following sense:\n\n+ The sampling distribution of the statistic is approximately normal.\n+ The SD of the sample is used in place of the SD of the population in  calculating the SE of the statistic.\n\nThere are times when we are unwilling to make the assumption of normality. This is the topic of the next set of notes. \n\n",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}