{
  "hash": "8383c996b4c561692cbf39ada7b38368",
  "result": {
    "markdown": "---\ntitle: \"Classification\"\nsubtitle: \"Penguin census and the K-nearest-neighbors algorithm\"\ndate: \"10/12/2022\"\nimage: images/3-species.png\nformat:\n  html:\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\n---\n\n\n[[Discuss]{.btn .btn-primary}](https://edstem.org) [[Reading Questions]{.btn .btn-primary}](https://www.gradescope.com/courses/416233) [[PDF]{.btn .btn-primary}](notes.pdf)\n\n\\\n\n-  A doctor diagnosing a disease\n-  Gmail's spam detector deciding whether or not a Stat 20 announcement is spam\n-  An Ecologist determining which species of penguin they are looking at\n\n<!--# A self-driving car deciding what to do at a stop light -->\n\n[T]{.dropcap}hese are all examples of predictive modeling.\nWe are given some input variables -- e.g. a patient's symptoms -- and want to predict some output variable -- e.g. a disease. \nIn these examples, however, the variable we are trying to predict is a nominal categorical variable (recall the taxonomy of data[^tree]) instead of a numerical variable.\nThe linear and polynomial models from the last few lectures only work for predicting  numerical variables so we need a new modeling framework.\n\n**Classification**\n\n:    The task of predicting a response variable that is categorical.\n\n[^tree]: https://www.stat20.org/1-questions-and-data/02-taxonomy-of-data/notes.html#a-taxonomy-of-data\n\n\n## Penguin census\n\n![](images/3-species.png){fig-align=center width=\"400\"}\n\nYou are new Ecology researcher studying penguins in Dr. Gorman's lab at Palmer Station.\nYour project is to conduct a penguin census; Dr. Gorman has asked you to count how many penguins of each species are in the region (Adelie, Chinstrap, and Gentoo).\nAs a new Ecologist you are missing a key skill for this project.\nHow do you identify which species a given penguin belongs to?\n\nYou need a penguin *classifier*. An algorithm that takes some attributes of a penguin and guesses which species (class) the penguin belongs to.\nFortunately, there there is some historical data you can use to develop the penguin classifier.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nlibrary(tidyverse)\ndata(\"penguins\")\n\n# for now just work with these variables\npenguins <- penguins %>% \n    select(flipper_length_mm, bill_length_mm, species) %>% \n    drop_na()\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 342 × 3\n   flipper_length_mm bill_length_mm species\n               <int>          <dbl> <fct>  \n 1               181           39.1 Adelie \n 2               186           39.5 Adelie \n 3               195           40.3 Adelie \n 4               193           36.7 Adelie \n 5               190           39.3 Adelie \n 6               181           38.9 Adelie \n 7               195           39.2 Adelie \n 8               193           34.1 Adelie \n 9               190           42   Adelie \n10               186           37.8 Adelie \n# … with 332 more rows\n```\n:::\n:::\n\n\n\n### Historical species data\n\n::: {layout=\"[-15, 20, 20, -15]\"}\n\n![](images/bill_length.png)\n\n![](images/flipper_length.png)\n\n:::\n\nLet see if we can determine the penguin species from the *bill length* and *flipper length*[^horst-art].\nFirst things first, let's take a look at the historical data where we know the true values of all three variables of interest (flipper length, bill length, and species).\n\n[^horst-art]: Penguin artwork by \\@allison_horst.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>% \n    ggplot(aes(x=flipper_length_mm, y=bill_length_mm, color=species)) + \n    geom_point() +\n    labs(x = 'Flipper Length (mm)',\n         y = \"Bill Length (mm)\",\n         color = \"Species\") +\n    ggtitle(\"Palmer Penguin historical data\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nThis plot indicates our goal is achievable.\nThe penguins in each species seems to *cluster* together by their bill and flipper length measurements.\nIn other words, if you measure the bill/flipper length of a new penguin for the census you should be able to guess which species it is.\nThe penguin classifier based on bill/flipper measurements might not be perfect (e.g. there is some overlap between Adelie and Chinstrap in the plot), but this plot indicates it should work most of the time.\n\nIf the historical data had looked like the following plot then our task would probably not be achievable.\nAny classifier built on this (hypothetical) dataset would probably have a very low accuracy rate.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(123) # seed for random permutation\npenguins %>% \n    mutate(species=sample(species, size=nrow(penguins))) %>% # shuffle the species columns\n    ggplot(aes(x=flipper_length_mm, y=bill_length_mm, color=species)) + \n    geom_point() +\n    labs(x = 'Flipper Length (mm)',\n         y = \"Bill Length (mm)\",\n         color = \"Species\") +\n    ggtitle(\"Hypothetical historical data for undifferentiated species\") + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=480}\n:::\n:::\n\n\n<!-- If the historical data for species flipper/bill length looked like this second (hypothetical) plot, you would have to turn to other variables to build the species classifier. -->\n\n## Most Similar Penguin Classifier\n\nLet's try a simple data driven classification rule: classify a penguin's species based on the species of the most similar penguin in the historical (training) data set. \nIn other words, use following algorithm to guess the species a new penguin:\n\n1. Search through the historical data to find the most similar penguin based on bill length and bill depth.\n2. Return the species of that penguin.\n\nTo implement this algorithm we need to specify what we mean by \"similar\". \nIf there were just one variable -- say bill length -- it would be easy to define what we mean by \"most similar\".\nIn words, we would find the nearest bill length value in the historical database.\nIn mathematical notation, we would find the smallest value of \n\n$$\n|x_{\\text{test}} - x_1|, |x_{\\text{test}} - x_2|, \\dots, |x_{\\text{test}} - x_n|\n$$\n\nwhere $x_1, \\dots, x_n$ are the bill length values of the historical penguins and $x_{\\text{test}}$ is the bill length value of the new \"test penguin\".\n\n### Multiple variables\n\nFor two or more variables we have to do something a little different. \nFor multiple variables we use *Euclidean distance* to measure similarity.\n\nSuppose $a$ and $b$ are two *vectors* of $d$ numbers.\nA vector is a list of numbers.\nFor example in a (n x d) data frame each row (observation) is a vector of d numbers and each column (variable) is a vector of n numbers.\nThe Euclidean distance between $a$ and $b$ is\n\n$$\ndist(a, b) = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + \\dots (a_d - b_d)^2 }\n$$\n\nIf $d=1$ this reduces to the absolute difference i.e. $dist(a, b) = |a - b|$. \nIf $d=2$, then Euclidean distance is the normal two-dimensional distance we are familiar with i.e. what you would get if you use a ruler to measure distance between two points on a piece of paper.\n\nReturning to our Most Similar Penguin Classifier with multiple variables, we now measure similarity with Euclidean distance.\nIn other words, we find the penguin in the historical dataset whose Euclidean distance of bill and flipper length is nearest the test penguin.\n\n### Classifying a test penguin\n\n<!-- Let's walk though this in code. -->\n<!-- We'll eventually use software packages that do all the heavy lifting for us, but implementing an algorithm is an excellent way to understand it. -->\n\nLet's say our test penguin has a bill length of 50 mm and flipper length of 200 mm.\nFrom the plot below it looks like this test penguin should be a Chinstrap.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_bill_length <- 50\ntest_flipper_length <- 200\n\npenguins %>% \n    ggplot(aes(x=flipper_length_mm, y=bill_length_mm, color=species)) + \n    geom_point() +\n    labs(x = 'Flipper Length (mm)',\n         y = \"Bill Length (mm)\",\n         color = \"Species\") +\n    geom_point(x=test_flipper_length, y=test_bill_length,\n               shape='X', size=10, color='black') + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=528}\n:::\n:::\n\n\nLet's check if the algorithm agrees.\nWe compute the Euclidean distance between the test penguin and each penguin in the data set (plot below), then find the nearest penguin.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npenguins %>% \n    ggplot(aes(x=flipper_length_mm, \n               y=bill_length_mm, \n               color=species)) + \n    geom_point() +\n    labs(x = 'Flipper Length (mm)',\n         y = \"Bill Length (mm)\",\n         color = \"Species\") +\n    geom_point(x=test_flipper_length, y=test_bill_length,\n               shape='X', size=10, color='black') + \n    geom_segment(aes(xend = flipper_length_mm, \n                     yend = bill_length_mm),\n                 x=test_flipper_length,\n                 y=test_bill_length, color='red', alpha=.1)+\n    theme_bw() + \n    ggtitle(\"Euclidean distance between test and each training point\")\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=528}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntest_bill_length <- 50\ntest_flipper_length <- 200\n\npenguins %>%\n    mutate(euclid_dist=sqrt((bill_length_mm - test_bill_length)^2 +\n                                (flipper_length_mm - test_flipper_length)^2)) %>% \n    arrange(euclid_dist)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 342 × 4\n   flipper_length_mm bill_length_mm species   euclid_dist\n               <int>          <dbl> <fct>           <dbl>\n 1               200           50.5 Chinstrap        0.5 \n 2               200           49.5 Chinstrap        0.5 \n 3               201           50.5 Chinstrap        1.12\n 4               201           50.8 Chinstrap        1.28\n 5               201           51.4 Chinstrap        1.72\n 6               198           49.8 Chinstrap        2.01\n 7               202           50.2 Chinstrap        2.01\n 8               198           50.2 Chinstrap        2.01\n 9               199           48.1 Chinstrap        2.15\n10               201           52   Chinstrap        2.24\n# … with 332 more rows\n```\n:::\n:::\n\n\nAnd voila, our nearest penguin algorithm guessed Chinstrap!\n\n### Aside: broadcasting\n\nOne thing should seem weird about this code; `test_bill_length` and `test_flipper_length` are numbers but `bill_length_mm` and `flipper_length_mm` are vectors (columns of the penguins data frame). \nWhy can you use numbers and vectors in the same arithmetic expression, for example `bill_length_mm - test_bill_length`?\nThis is one place R will try to help you out by *broadcasting* the number to each entry of the vector.\nIn other words, R knows you wanted to subtract `test_bill_length` from each entry of `bill_length_mm`.\n\n<!-- To rephrase this, we assign each penguin $i=1,\\dots, n$ in the dataset a \"dissimilarity score\" to the test penguin -->\n<!-- $$ -->\n<!-- s_1 = |x_{\\text{test}} - x_1| \\\\ -->\n<!-- \\vdots \\\\ -->\n<!-- s_n = |x_{\\text{test}} - x_n| -->\n<!-- $$ -->\n<!-- then find the penguin with the smallest dissimilarity score (the most similar penguin!). -->\n\n<!-- When there are two or more variables it becomes more difficult to assign a dissimilarity score. -->\n<!-- While there are many ways to  -->\n\n## Most similar K penguins\n\nWe might be able to improve our penguin classifier though a bit of democracy.\nInstead of finding the most similar penguin, why don't we find the most similar 3 penguins and have them vote to determine the predicted class?\n\nAs a motivating example, suppose our test penguin has a bill length of 48 mm and flipper length of 203 mm.\nLooking carefully at the plot below we see the nearest penguin is a Gentoo, but the other nearby penguins are all Chinstraps.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntest_bill_length_2 <- 48\ntest_flipper_length_2 <- 203\n\npenguins %>% \n    ggplot(aes(x = flipper_length_mm, \n               y = bill_length_mm,\n               color = species)) + \n    geom_point() +\n    xlab('Flipper Length (mm)') +\n    ylab(\"Bill Length (mm)\") +\n    geom_point(x=test_flipper_length_2, y=test_bill_length_2,\n               shape='X', size=5, color='black') + \n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-7-1.png){fig-align='center' width=528}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_bill_length_2 <- 48\ntest_flipper_length_2 <- 203\n\npenguins %>%\n    mutate(euclid_dist=sqrt((bill_length_mm - test_bill_length_2)^2 +\n                                (flipper_length_mm - test_flipper_length_2)^2)) %>% \n    arrange(euclid_dist)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 342 × 4\n   flipper_length_mm bill_length_mm species   euclid_dist\n               <int>          <dbl> <fct>           <dbl>\n 1               203           48.4 Gentoo          0.400\n 2               203           49.3 Chinstrap       1.30 \n 3               202           50.2 Chinstrap       2.42 \n 4               203           50.7 Chinstrap       2.70 \n 5               203           51   Chinstrap       3    \n 6               201           50.5 Chinstrap       3.20 \n 7               200           49.5 Chinstrap       3.35 \n 8               201           50.8 Chinstrap       3.44 \n 9               200           50.5 Chinstrap       3.91 \n10               201           51.4 Chinstrap       3.94 \n# … with 332 more rows\n```\n:::\n:::\n\n\nTherefore if we had the 3 most similar penguins vote the algorithm would predict Chinstrap (2 votes for Chinstrap, 1 vote for Gentoo).\nThis seems like a better prediction from looking at the plot.\n\nThere was nothing special about 3, we could ask any number, K, of the most similar penguins to vote and long as K is less than or equal to the number of penguins in the historical dataset.\n\n## Summary\n\nThese notes presented classification: predictive modeling when the outcome variable is categorical instead of numerical.\nWe walked through a simple classification algorithm: the K-closest-penguin classifier.\nThis is conventionally named the *k-nearest neighbors* (KNN) algorithm and can be summarized as follows.\n\nStart with training data set $x_{1}, \\dots, x_n$ with labels $y_1, \\dots, y_n$ and pick a value of $K$ less than or equal to $n$.\nTo get the prediction for a new test point $x_{\\text{test}}$:\n\n1. Compute the Euclidean distance between each training point and the test point $d(x_1, x_{\\text{test}}), \\dots, d(x_n, x_{\\text{test}})$.\n\n2. Find the nearest $K$ training points.\n\n3. Have these $K$ nearest training points vote using their labels to obtain the predicted class.\n",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}