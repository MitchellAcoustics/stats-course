{
  "hash": "400176fdb2d189dc7e70c40c667c3a35",
  "result": {
    "markdown": "---\ntitle: \"Classification Case Study\"\nsubtitle: \"Using KNN in R with the caret package\"\ndate: \"10/14/2022\"\nimage: images/cv_curve.png\nformat:\n  html:\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\neditor: \n  markdown: \n    wrap: sentence\n---\n\n\n[[Discuss]{.btn .btn-primary}](https://edstem.org) [[Reading Questions]{.btn .btn-primary}](https://www.gradescope.com/courses/416233) [[PDF]{.btn .btn-primary}](notes.pdf)\n\n\\\n\nIn the previous lecture we started building a k-nearest-neighbor (KNN) classifier to classify penguin species based on bill length and flipper length.\nThese notes focus on the same problem, but go through the entire classification analysis.\nWe will put together several concepts we've already touched on: train/test set split, hyperparameter tuning, issues of scale, and the KNN algorithm.\n\n:::{.callout-tip}\n\n## Code along\n\nAs you read through these notes, keep RStudio open in another window to code along at the console.\n:::\n\nIn these notes we'll build the species classifier using all of the physical measurements we have about the penguins including: bill length, bill depth, flipper length, and body mass.\nTo simplify things we will drop any observation that has a missing value.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(palmerpenguins)\nlibrary(tidyverse)\ndata(\"penguins\")\n\npenguins <- penguins %>%\n    select(species, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g) %>%\n    drop_na()\n\npenguins\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 342 × 5\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>            <dbl>         <dbl>             <int>       <int>\n 1 Adelie            39.1          18.7               181        3750\n 2 Adelie            39.5          17.4               186        3800\n 3 Adelie            40.3          18                 195        3250\n 4 Adelie            36.7          19.3               193        3450\n 5 Adelie            39.3          20.6               190        3650\n 6 Adelie            38.9          17.8               181        3625\n 7 Adelie            39.2          19.6               195        4675\n 8 Adelie            34.1          18.1               193        3475\n 9 Adelie            42            20.2               190        4250\n10 Adelie            37.8          17.1               186        3300\n# … with 332 more rows\n```\n:::\n:::\n\n\nWe will use several new functions below that come from a package called `caret`, which is a powerful modeling package in R.\nThese are `createDataPartition`, `preProcess`, `knn3`, `trainControl`, and `train`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ninstall.packages(\"caret\")\nlibrary(caret)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n## Classification Analysis\n\nFor predictive modeling we want to construct a model *and* evaluate how accurate we think that model will be on new, unseen data.\nThe outline of a predictive analysis is:\n\n1.  Split the data into train and test sets\n2.  Transform features in training data\n3.  Fit model to training data\n4.  Evaluate model on test data\n\n### Train/test set split\n\nThe very first step of predictive modeling is to do a train/test set split.\nAfter doing the train/test split we do not want to touch the test data until we are ready to evaluate the model.\n\nThe `caret` library provides a helpful function called `createDataPartition` that creates the train/test set split for us.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(83234)\ntrain_indices <- createDataPartition(y = penguins$species, \n                                     p = 0.7, \n                                     list = FALSE)\n\ntrain_df <- penguins %>% \n    slice(train_indices)\n\ntest_df <-  penguins %>% \n    slice(-train_indices)\n```\n:::\n\n\nA couple of notes are in order.\n\n- `createDataPartition` randomly samples the indices of the observations that should go in the training set. Here we've specified that 70% of the data go into the training set, leaving 30% for the test set. Run the code and see what `train_indices` looks like.\n\n-   `set.seed` makes sure this random split will be the same everytime we run the code.\n    If we did not set the *random seed* the code above would give a slightly different slit everytime we run it.\n    To make sure an analysis is *reproducible* -- that is someone else can run your code and get the same answers -- it's important to set seeds!\n\n-  `createDataPartition` uses *stratified sampling* on the species variable.\n    This means it ensures the proportions of each class are the same in the train and test sets.\n    Stratified sampling is important when there are classes with few observations.\n\n-  The code `penguins$species` pulls out the species column as a vector.\n\n- We used a tidyverse function called `slice`. Slice selects rows (like `filter`), but by their index instead of a condition on the values of their variables. `slice(penguins, 1)` returns the first row of the penguins data frame; `slice(penguins, c(1, 3))` returns the first and third rows; `slice(penguins, -c(1, 3))` returns every row *except* the first and 3rd rows.\n\n\n### Feature standardization\n\nFor most data analyses we transform the original variables we were given to create new variables that are (hopefully) better for the task we are interested in.\nThis is often called *feature engineering* or *feature transformation* -- recall feature is a synonym for variable or column of a data frame.\n\nOne of the most common feature transformations is called *standardization*.\nStandardizing features (usually) means subtract off the mean then divide by the standard deviation[^centering].\nIn other words, we create a new version of each variable, `x`, with the following formula\n\n[^centering]: It's called *centering* because the mean of the resulting variable is zero i.e. the new variable is \"centered\" at 0.\n\n$$\nx_{\\text{new}} = \\frac{x - \\overline{x}}{\\text{s}_x}\n$$\n\nor in code this would be\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntrain_df %>% \n    mutate(bill_length_stand = (bill_length_mm - mean(bill_length_mm)) / sd(bill_length_mm)) %>% \n    select(bill_length_mm, bill_length_stand)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 241 × 2\n   bill_length_mm bill_length_stand\n            <dbl>             <dbl>\n 1           39.1            -0.883\n 2           39.5            -0.809\n 3           40.3            -0.659\n 4           36.7            -1.33 \n 5           39.3            -0.846\n 6           38.9            -0.921\n 7           34.1            -1.82 \n 8           42              -0.342\n 9           37.8            -1.13 \n10           41.1            -0.510\n# … with 231 more rows\n```\n:::\n:::\n\n\nIn other words, we compute the mean and standard deviation of `bill_length_mm` then subtract off the mean and divide by the standard deviation.\nThe standardized features -- sometimes called *z-scores* --  measure how far above/below the average bill length a penguin is on a scale relative to the natural variability of penguin bill lengths.\nIn other words, if `bill_length_stand = - 1` for a penguin then that penguin's bill is one standard deviation shorter than average.\n\nAny reasonable measure of center (e.g. median) and scale (e.g. MAD or IQR) can be used in place of mean/standard deviation.\n\nFeature standardization is important for KNN because different variables can be measured on different scales, which can artificially over or under weight a feature when we compute Euclidean distances.\nFor example, imagine if bill length were measured in mm but bill depth were measured in kilometres -- bill depth would basically be ignored in the Euclidean distance calculation!\n\n### Feature standardization code\n\nAt this stage we only want to use the training data.\nWhen we eventually evaluate the model on the test dataset we will need to apply the same feature transformation to the test data that we applied to the training data.\nThis means we want to: 1) compute and save the mean/standard deviation of each variable from the training data, 2) transform the training data,  then 3) eventually transform the test data using the same mean/std.\nThe `caret` package has a nice function that does all of this for us!\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# compute the mean/standard deviation of each variable in the training data frame\nstandardize_params <- preProcess(train_df,\n                                 method = c(\"center\", \"scale\"))\n\n# transform each variable in the training data frame\ntrain_stand <- predict(standardize_params, train_df)\n\ntrain_stand\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 241 × 5\n   species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   <fct>            <dbl>         <dbl>             <dbl>       <dbl>\n 1 Adelie          -0.883      0.806               -1.44      -0.532 \n 2 Adelie          -0.809      0.151               -1.07      -0.469 \n 3 Adelie          -0.659      0.453               -0.411     -1.16  \n 4 Adelie          -1.33       1.11                -0.558     -0.910 \n 5 Adelie          -0.846      1.76                -0.779     -0.658 \n 6 Adelie          -0.921      0.352               -1.44      -0.690 \n 7 Adelie          -1.82       0.503               -0.558     -0.879 \n 8 Adelie          -0.342      1.56                -0.779      0.0992\n 9 Adelie          -1.13      -0.000418            -1.07      -1.10  \n10 Adelie          -0.510      0.251               -1.37      -1.23  \n# … with 231 more rows\n```\n:::\n:::\n\n\nNote the `predict` function is being used in a similar manner to how it was used for a linear model, but here the first argument is the standardization operation instead of the linear model.\n\nWe saved the transformed features in a new data frame called `train_stand`.\nWe can verify each column of `train_stand` has mean zero. Verify yourself they have standard deviation 1.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\ntrain_stand %>% \n    select(-species) %>% \n    summarize_all(mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n           <dbl>         <dbl>             <dbl>       <dbl>\n1      -3.92e-16     -6.88e-16          6.87e-16    1.00e-16\n```\n:::\n:::\n\n\nNotice the use of `summarize_all()`.\nThis is a tidyverse function like `summarize`, but applies a function to every variable.\n\nWe might have expected the numbers to be 0, but they instead they are values like `1.004929e-16` (meaning $1.004929 \\times 10^{-16}$).\nThis is basically zero: 1 with 16 zeros in front of it!\nWe say this is zero *up to numerical precision*.\nComputers cannot do many calculations perfectly, only to very high accuracy.\n\n\n### Fitting KNN\n\nWe are now ready to fit the KNN model.\nFor now we will just fix K=1.\nWhen we fit a linear regression model the `lm` function computed the coefficients from the data (recall the formula for simple linear regression).\nTechnically KNN does not have any *parameters* that are obtained from the training data -- it just memorizes the training data.\nThe `caret` package has a nice function called `knn3` that sets up the KNN algorithm for us.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# notice we use the standardized data!\nknn_fit <- knn3(species ~ ., k = 1, data = train_stand)\n```\n:::\n\n\n`knn3` uses the same formula notation as the `lm` function.\nThe notation `species ~ .` means predict species from all of the remaining variables.\nIf we wanted to predict species from just the first two variables we could have instead written `species ~ bill_length_mm + bill_depth_mm`.\n\n### Evaluating test set error\n\nWe are now ready to evaluate our KNN model on the test data.\nThis will give us an estimate of how accurate it will be when we use it on future penguins counted in our penguin census!\n\nSince we fit the KNN model on the standardized training data, we also need to standardize the test data.\nWe again accomplish this with the `predict` function with `standardize_params`.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# process the test data\ntest_stand <- predict(standardize_params, test_df)\n```\n:::\n\n\nNow we can get the predictions using the `predict` function applied to the `knn_fit` object.\nNotice the `type` argument -- this is important.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# get test predictions\nspecies_pred_test = predict(knn_fit, \n                            newdata = test_stand, \n                            type = 'class')\n```\n:::\n\n\nAnd finally we can compute the test accuracy.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute test set accuracy\ntest_stand %>% \n    mutate(species_pred = species_pred_test,\n           correct_prediction = species_pred == species) %>% \n    summarize(accuracy = mean(correct_prediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  accuracy\n     <dbl>\n1    0.980\n```\n:::\n:::\n\n\n## Putting the K in K-nearest-neighbors\n\nWe are missing one key detail; how do we select K for K-nearest-neighbors?\nThis is known as *hyperparameter tuning*.\nThe value K for KNN, just like the degree of a polynomial in regression, are called hyperparameters[^hyperparam] and the procedure of selecting the best *hyperparameter* is called *tuning*.\n\n[^hyperparam]: We also sometimes these *tuning parameters* instead of hyperparameters.\n\nSometimes we can pick a sensible default value for hyperparameters and avoid tuning them.\nFor exapmle, K = 1, 3, or 5 can be a decent default (these would actually work well for the penguins data set).\nMost of the time in practice, however, hyperparameter tuning is important to obtain good classification performance.\n\nNow the outline of our classification analysis looks like\n\n1.  Train/test set split\n2.  Feature transformation\n3.  Hyperparameter tuning\n4.  Fit model to training data with best hyperparameters\n5.  Evaluate model on test data\n\nWe already did 1 and 2 above so lets go straight to 3.\n\n### Cross-validation\n\nWe could select K using the validation set procedure we discussed in a previous lecture.\nIn other words\n\n1) Split the training data into a smaller training set and validation set.\n\n2) For each value of K, fit KNN on the new training set, and evaluate the resulting model on the validation set.\n\n3) Select the best K based on the validation set performance.\n\n4) Refit KNN on the full training set.\n\nInstead of tuning with a validation set we are going to tune with *cross-validation*.\nCross-validation means doing the aforementioned procedure a number of times where we resample a different train/validation split each time.\nFor each value of K we average the performance over the different data splits.\nSince we often have a limited amount of data, the validation set evaluations can be quite noisy.\nRepeating them a number of times and taking an average can give use more accurate estimates.\n\n\nCross-validation quite general and is one of the most widely used procedures in statistics and artificial intelligence.\n\n### Implementing cross-validation\n\nThe `caret` package implements cross-validation and many other tuning procedures.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nset.seed(9832)\n\n# create a numeric vector with values from 1 to 60\nk_values_to_try <- seq(from = 1, to = 60)\n\n# setup the tuning parameter selection procedure\ntraining_control <- trainControl(method = \"repeatedcv\", repeats = 5)\n\n# try all values of k then fit the best to the full training data set\nknn_cv <- train(species ~ ., \n                data = train_stand,\n                method = \"knn\",\n                trControl = training_control,\n                metric = \"Accuracy\",\n                tuneGrid = data.frame(k = k_values_to_try))\n```\n:::\n\n\nA couple of notes:\n\n- We specify the details of the tuning parameter selection procedure using `trainControl`.\nThe code above does 5-fold cross-validation.\n\n- We have to specify what values of K to try (often called the tuning parameter sequence).\nHere we are trying every value between 1 and 60.\n\n- The `train` function does the following\n    - Split the training data into a new train and validation set\n    - Fit KNN to the new training data for each value of K\n    - Evaluate the resulting models on the validation set\n    - Repeat this 5 times\n    - Decide the best value of K (the K that gives the highest accuracy on the validation set)\n    - Refit KNN to the full training data set\n\nAnd voila we have a tuned KNN model, that is, a KNN classifier that uses a value of $K$ that is tuned to the particular structure of the training data.\n\nWe can plot the validation accuracy as a function of K using the `plot` function.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nplot(knn_cv)\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\nThe *inverted-U* shape of this plot is characteristic of hyperpameter tuning.\nOne normally expects to see a  Golidlocks phenomenon[^gold] where too small or too large a value for the hyperparameter is suboptimal, but some intermediate value is optimal.\n\n[^gold]: https://en.wikipedia.org/wiki/Goldilocks_principle\n\n\nFinally we evaluate our tuned KNN model on the test set.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# get test predictions\n# unlike knn3 we dont need type=\"class\" again\nspecies_pred_test_cv <- predict(knn_cv, newdata = test_stand)\n\n# compute the accuracy\ntest_stand %>% \n    mutate(species_pred = species_pred_test_cv) %>% \n    mutate(correct_prediction = species_pred == species) %>% \n    summarize(accuracy = mean(correct_prediction))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 1\n  accuracy\n     <dbl>\n1    0.970\n```\n:::\n:::\n\n\n\n## Summary\n\nThis lecture walked through an entire classification analysis with the K-Nearest Neighbors algorithm.\nWhen we fixed the value of K this included: train/test split, standardizing the training data, fitting the KNN model on the training data, and finally evaluating the model on the test data.\nWhen we tuned K, we added an additional hyperparameter tuning step.\nWhile there are many operations involved in each of the above steps, the `caret` package makes this entire process happen in a few lines of code.\nYou'll have a chance to apply this code to your own analysis when you build a biomedical classifier for your next lab.\n",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}