{
  "hash": "e37fc6e1d0ab9d08f843799498c8114f",
  "result": {
    "markdown": "---\ntitle: \"Overfitting handout\"\nformat: pdf\n---\n\n\nInstead of $R^2$ we often just use the residual sum of squares (RSS), $\\sum_{i=1}^n (y_i - \\widehat{y}_i)^2$ or the mean square error (MSE) where MSE = $\\frac{1}{n}\\sum_{i=1}^n (y_i - \\widehat{y}_i)^2.$ For this handout just use the test set or training set MSE to evaluate regression models.[^1]\n\n[^1]: There is a mathematical subtlety that comes up when using test set $R^2 = 1 - RSS/TSS$. If you fit a linear (or polynomial) regression model, the training set $R^2$ is always between 0 and 1. BUT in general this is not true; the test set $R^2$ is NOT required to always be between 0 and 1. Even more frustrating; the training $R^2$ is only between 0 and 1 for a linear regression based model, but not necessarily for other modeling frameworks (e.g. deep learning).\n\nDownload the following training and test datasets.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n-- Attaching packages --------------------------------------- tidyverse 1.3.2 --\nv ggplot2 3.4.0      v purrr   0.3.5 \nv tibble  3.1.8      v dplyr   1.0.10\nv tidyr   1.2.1      v stringr 1.4.1 \nv readr   2.1.2      v forcats 0.5.1 \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\ntrain <- read_csv('https://raw.githubusercontent.com/idc9/course-materials/main/3-prediction/14-overfitting/train.csv')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 195 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (2): x, y\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nval <- read_csv('https://raw.githubusercontent.com/idc9/course-materials/main/3-prediction/14-overfitting/validation.csv')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 212 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (2): x, y\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\ntest <- read_csv('https://raw.githubusercontent.com/idc9/course-materials/main/3-prediction/14-overfitting/test.csv')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 193 Columns: 2\n-- Column specification --------------------------------------------------------\nDelimiter: \",\"\ndbl (2): x, y\n\ni Use `spec()` to retrieve the full column specification for this data.\ni Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\n# Question 1\n\nWhat is the best MSE value you can obtain on the test set by fitting a polynomial model to the training set?\n\n-   Fit polynomial models to the training set for different degrees (I suggest sticking to degrees less than 10).\n\n-   Evaluate the models' MSE on the validation set.\n\n-   Pick the best model from the validation set and compute the MSE for the test set.\n\n# Question 2\n\nIs there a way to cheat on question 1? If so, try cheating and see how low of a MSE you can get on the test set.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}