{
  "hash": "d3a26852c473aadac29bbccb5e2ed11f",
  "result": {
    "markdown": "---\ntitle: \"Overfitting\"\nsubtitle: \"Overfitting with scissors, train/test split, hyperparameter tuning.\"\ndate: \"10/07/2022\"\nimage: images/poly.png\nformat:\n  html:\n    code-fold: true\n    code-link: true\n    code-summary: \".\"\nexecute: \n  warning: false\n  message: false\n---\n\n\n[[Discuss]{.btn .btn-primary}](https://edstem.org) [[Reading Questions]{.btn .btn-primary}](https://www.gradescope.com/courses/416233)\n[[PDF]{.btn .btn-primary}](notes.pdf)\n\nLearning through examples is one of the most important ways we acquire knowledge.\nHave you ever given someone an example to illustrate a concept, but that person gets too caught up in the details of the example and learns the wrong lesson?\nThere is an expression for this \"missing the forest for the trees\"[^1].\nBefore reading these notes please listen to Act 3 of [this podcast](https://www.thisamericanlife.org/584/for-your-reconsideration) (Kids Look Back -- 10 minutes).\n\n[^1]: Definition (Merriam-Webster): to not understand or appreciate a larger situation, problem, etc., because one is considering only a few parts of it.\n\n<!-- https://www.merriam-webster.com/dictionary/miss%20the%20forest%20for%20the%20trees#:~:text=Definition%20of%20miss%20the%20forest,a%20few%20parts%20of%20it -->\n\nAlmost all predictive modeling -- from linear regression to artificial intelligence based on deep learning -- is accomplished though learning by example.\nIn simple linear regression we show our algorithm a bunch of pairs of examples $(x_1, y_1), \\dots, (x_n, y_n)$ and ask the linear model to best approximate $y$ through the linear function[^2] $\\widehat{y} = b_0 + b_1 \\cdot x$.\n\n[^2]: In other words, the linear function coefficients $(b_0, b_1)$ come from these examples (remember the formulas from previous lectures).\n\nMissing the forest for the trees turns out to be one of the most important concepts predictive modeling.\nIn statistics we call it *overfitting*.\n\n## Overfitting\n\nLet's illustrate overfitting with an example ðŸ˜‰.\n\nHere we collected data about the association between number of hours studied and students' test scores in a math class.\nOur goal is to predict the exam score from number of hours studied.\nBoth plots below show the same data, but show the predictions from two different predictive models.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(broom)\n\nset.seed(1)\nn_samples <- 10\n\nmin_n_hours <- 5\nmax_n_hours <- 10\n\nexam_avg <- 75\npoint_per_hour <- 5\n\nintercept <- exam_avg - point_per_hour * (min_n_hours + max_n_hours) / 2\nnoise <-  rnorm(n=n_samples, mean=0, sd=5)\n\n# Setup data\nmath <- tibble(hours=runif(n=n_samples, min=min_n_hours, max=max_n_hours),\n             score=point_per_hour * hours + intercept + noise)\n\n# same x data, but much finer grid. useful for plotting lines\nmath_filled_in_range <- tibble(hours=seq(from=min_n_hours, to=max_n_hours, by=.01))\n\n# fit models\nlm_lin <- lm(score ~ hours, data=math)\nlm_poly <- lm(score ~ poly(hours, degree=20, raw=T), data=math)\n\n# add predictions into model\nmath <- math %>% \n    mutate(y_pred_linear = predict(lm_lin, newdata=math),\n           y_pred_poly = predict(lm_poly, newdata=math))\n\nmath_filled_in_range_filled_in_range <- math_filled_in_range %>% \n    mutate(y_pred_linear = predict(lm_lin, newdata=math_filled_in_range),\n           y_pred_poly = predict(lm_poly, newdata=math_filled_in_range))\n\nplot_linear <- math %>% \n    ggplot(aes(x=hours, y=score)) +\n    geom_point() +\n    geom_line(aes(x=hours, y=y_pred_linear), color='blue') +\n    lims(x=c(5, 10), y=c(65, 90)) +\n    ggtitle(\"Math class data\") +\n    theme_bw()\n\nplot_poly <- math %>% \n    ggplot(aes(x=hours, y=score)) +\n    geom_point() +\n    geom_line(data=math, aes(x=hours, y=y_pred_poly), color='red')+\n    lims(x=c(5, 10), y=c(65, 90)) +\n    theme_bw()\n\nplot_linear + plot_poly\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWhich model looks more appropriate?\nFor example,\n\n-   Does it make sense that there should be a big difference between studying 6.7 hours vs studying 6.8 hours?\n\n-   Should studying a little more make your score go down?\n\nThe predictive model on the left seems more reasonable: studying more should steadily increase your score.\nThe predictive model on the right seems like it took the particular data points too seriously!\nWe call this phenomenon *overfitting*.\n\n## Overfitting with polynomials\n\n> With great modeling power comes great responsibility to not overfit.\n\nThe blue model on the left above is fairly simple (just a line!) while the red overfit model on the right looks much more complex (it is fairly irregular and goes up and down).\nWe created the overfitted predictive model on the right by fitting a polynomial with a high degree.\n\nPolynomials are quite powerful models and are capable of creating very complex predictive functions.\nThe higher the polynomial degree, the more complex function it can create.\n\nLet's illustrate by fitting polynomial models with progressively larger degrees to the data set above.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nplot_deg_1 <- math_filled_in_range %>% \n    mutate(y_pred = predict(lm(score ~ poly(hours, degree=1, raw=T), data=math), newdata=math_filled_in_range)) %>% \n    ggplot( aes(x=hours, y=score)) +\n    geom_line(aes(x=hours, y=y_pred), color='red')+\n    geom_point(data=math, aes(x=hours, y=score)) +\n    lims(x=c(5, 10), y=c(65, 90)) +\n    ggtitle(\"Degree 1 polynomial\") +\n    theme_bw()\n\nplot_deg_3 <- math_filled_in_range %>%\n    mutate(y_pred = predict(lm(score ~ poly(hours, degree=3, raw=T), data=math), newdata=math_filled_in_range)) %>%\n    ggplot(aes(x=hours, y=score)) +\n    geom_line(aes(x=hours, y=y_pred), color='red')+\n    geom_point(data=math, aes(x=hours, y=score)) +\n    lims(x=c(5, 10), y=c(65, 90)) +\n    ggtitle(\"Degree 3 polynomial\") +\n    theme_bw()\n\nplot_deg_5 <- math_filled_in_range %>%\n    mutate(y_pred = predict(lm(score ~ poly(hours, degree=5, raw=T), data=math), newdata=math_filled_in_range)) %>%\n    ggplot(aes(x=hours, y=score)) +\n    geom_line(aes(x=hours, y=y_pred), color='red') +\n    geom_point(data=math, aes(x=hours, y=score)) +\n    lims(x=c(5, 10), y=c(65, 90)) +\n    ggtitle(\"Degree 5 polynomial\") +\n    theme_bw()\n\nplot_deg_1 + plot_deg_3 + plot_deg_5\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=768}\n:::\n:::\n\n\nThe higher the polynomial degree, the closer the prediction function comes to perfectly fitting the data[^3].\nWe can quantitatively evaluate this by looking at the $r^2$ value.\nThe plot below shows the $r^2$ value for models fit with different polynomial degrees.\n\n[^3]: We say a function that perfectly predicts each data point *interpolates* the data.\n    See the first red curve for the math class exams.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n#########################\n# Digression: for loops #\n#########################\n\n# We have not learned for loops. Here is a simple example where we calculate the squares of the\n# first 10 integers\n# squares <- c()\n# for (i in 1:10){\n#     squares <- c(squares, i^2)\n# }\n# squares\n\n#####################\n# Evaluate Rsquared #\n#####################\ndegrees2evaluate <- 1:10\n\nRsq_values <- c()\n# each iteration of this for loop evaluates the model for a different polynomial degree \nfor (degree in degrees2evaluate){\n    \n    # fit the model on the training data with the specified degree\n    model <- lm(score~poly(hours, degree=degree, raw=T), data=math)\n    rsq <- glance(model)$r.squared\n    \n    # track the Rsquared values\n    # vector <- c(vector, value) will add the value to the end of the vector\n    # i.e. c does concatenation\n    Rsq_values <- c(Rsq_values, rsq)\n}\n\ntibble(degree=degrees2evaluate,\n       Rsq=Rsq_values) %>% \n    ggplot(aes(x=degree, y=Rsq)) + \n    geom_point() + \n    scale_x_continuous(breaks=degrees2evaluate)+\n    # ggtitle(\"r^2 gets better with degree!\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nThe $r^2$ value goes straight up as the polynomial degree goes up.\nIn fact this is mathematically guaranteed to happen: for a fixed dataset the $r^2$ value for a polynomial model with higher degree will always be better than a polynomial model with lower degree.\nThis should be disconcerting.\nFrom visually inspecting the above plots we can see that higher degree polynomials seem to be worse models.\nDoes that mean $r^2$ is not a good measure of fit?\n\n## Train vs. test set evaluation\n\nWe can rescue $r^2$ by changing our workflow.\nPreviously we both *fit* the model (set the $b_0, b_1$ coefficeints) and *evaluated* the model (calculated $r^2$) with the same dataset.\nNow we are going to fit the model and evaluate the model with different datasets.\n\nIn brief we will 1) split the observations into two parts 2) fit the model on the first part of the data and 3) evaluate the model using the second part of the data.\nThe first set of observations is called the *training data* or *training set* and the second set of observations is called the *test data* or *test set*.\n\nTrain/test set splitting is a natural idea.\nIf the Stat 20 final exam were made up of just homework questions, then it will be difficult to tell if students learned the concepts or just memorized the homework questions.\n\nLet's illustrate train/test evaluation with the exam scores from a biology class with 200 students.\nHere we are going to compare two models; a linear fit vs. a 5th degree polynomial fit.\n\nWe are going to randomly select 80% of the observations for the training set then put the remaining 20% of observations into the test set.\nWe call this an 80/20 train/test set split.\nIt's important the training and test set do not overlap; an observation is only in one or the other!\nIt's also important the data are divided up randomly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn_samples_big <- 200\nset.seed(1233)\nnoise <-  rnorm(n = n_samples_big, mean = 0, sd = 5)\n\n# Setup data\nbiology <- tibble(hours = runif(n = n_samples_big,\n                                min = min_n_hours,\n                                max=max_n_hours),\n                  score = point_per_hour * hours + intercept + noise)\n```\n:::\n\n\nFirst we randomly assign observations to train/test set.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# fix the random seed so we get the same train/test split everytime\nset.seed(13)\n\n# randomly sample train/test set split\nset_type <- sample(x = c('train', 'test'), \n                   size = 200, \n                   replace = TRUE, \n                   prob = c(0.8, 0.2))\n\nbiology <- biology %>% \n    mutate(set_type = set_type)\nbiology\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 200 Ã— 3\n   hours score set_type\n   <dbl> <dbl> <chr>   \n 1  9.78  88.7 train   \n 2  5.26  60.8 train   \n 3  9.84  86.3 train   \n 4  6.39  61.7 train   \n 5  7.19  72.4 test    \n 6  6.43  61.3 train   \n 7  5.71  66.4 train   \n 8  9.53  92.4 train   \n 9  6.92  71.7 test    \n10  7.72  80.8 train   \n# â€¦ with 190 more rows\n```\n:::\n:::\n\n\n<!-- ```{r} -->\n\n<!-- biology %>%  -->\n\n<!--     count(set_type) -->\n\n<!-- ``` -->\n\nLet's visualize these data.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nbiology %>% \n    ggplot(aes(x=hours, y=score, color=set_type)) +\n    geom_point() + \n    ggtitle(\"Biology class\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nNext let's split the original data frame into a training data frame and test data frame.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nbiology_train <- biology %>% \n    filter(set_type == 'train')\n\nbiology_test <- biology %>% \n    filter(set_type == 'test')\n```\n:::\n\n\nNow fit two models on the training data.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# notice we using the training data to fit the model!\nlm_linear <- lm(score~hours, data=biology_train)\nlm_poly <- lm(score~poly(hours, degree=20, raw=T), data=biology_train)\n```\n:::\n\n\nNext we visually inspect the predictions.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# same x data, but much finer grid. useful for plotting lines\nbiology_filled_in_range <- tibble(hours = seq(from = min_n_hours, to = max_n_hours, by = .01))\n\n# this is useful for plotting\nbiology_filled_in_range <- biology_filled_in_range %>% \n    mutate(y_pred_linear = predict(lm_linear,\n                                   newdata = biology_filled_in_range),\n           y_pred_poly = predict(lm_poly, \n                                 newdata = biology_filled_in_range)) \n\nbiology %>% \n    ggplot(aes(x=hours, y=score, color=set_type)) +\n    geom_point() +\n    geom_line(data=biology_filled_in_range, aes(x=hours, y=y_pred_linear), color='red') +\n    geom_line(data=biology_filled_in_range, aes(x=hours, y=y_pred_poly), color='blue') +\n    lims(x=c(5, 10), y=c(65, 90)) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=576}\n:::\n:::\n\n\nFirst let's evaluate the $r^2$ for the training data.\nWe can do this just like before with `glance`.\nWhich model do you expect to have a better *training* set $r^2$ value?\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# this calculates the training r^2\nglance(lm_linear) %>%\n    select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 1\n  r.squared\n      <dbl>\n1     0.544\n```\n:::\n\n```{.r .cell-code  code-fold=\"false\"}\nglance(lm_poly) %>%\n    select(r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 1\n  r.squared\n      <dbl>\n1     0.588\n```\n:::\n:::\n\n\nJust as we might have guessed from looking at the model fits, the polynomial model has a better $r^2$ value when evaluated on the training set.\nNext let's evaluate these models on the *test* set.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\n# get predictions on test set\nscore_pred_linear <- predict(lm_linear, newdata=biology_test)\nscore_pred_poly <- predict(lm_poly, newdata=biology_test)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate R squared for test set\nbiology_test %>%\n    mutate(score_pred_linear = score_pred_linear,\n           score_pred_poly = score_pred_poly,\n           resid_sq_linear = (score - score_pred_linear)^2,\n           resid_sq_poly = (score - score_pred_poly)^2) %>%\n    summarize(TSS = sum((score - mean(score))^2),\n              RSS_linear = sum(resid_sq_linear),\n              RSS_poly = sum(resid_sq_poly)) %>%\n    mutate(Rsq_linear = 1 - RSS_linear/TSS,\n           Rsq_poly = 1 - RSS_poly/TSS) %>%\n    select(Rsq_linear, Rsq_poly)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 Ã— 2\n  Rsq_linear Rsq_poly\n       <dbl>    <dbl>\n1      0.586    0.538\n```\n:::\n:::\n\n\nVoila the linear model's test set $r^2$ is better than the polynomial model's test $r^2$!\n\n## Hyperparameter tuning\n\nSuppose we want to fit a polynomial model for a dataset; how do we pick the degree?\nSelecting the degree of a polynomial model is an example of *hyperparameter tuning*.\nIt's tempted to fit polynomials for a bunch of degrees -- say every degree between 1 and 10 -- then pick the polynomial with the best $r^2$.\nThe above sections about overfitting should make us suspicious of this idea; larger degree polynomials will always give better $r^2$ values!\n\nWe can use what we learned in the previous train/test set section to select the degree.\nIn detail, we\n\n1. split our dataset into a training and test set \n2. fit each polynomial model on the training set and then \n3. evaluate each polynomial model on the test set.\n\nThis procedure is called *tuning with a validation set*; we conventionally call this test set that gets queried a number of times a *validation set*.\n\n#### Example: Physics Exam Scores\n\nConsider the following physics exam scores dataset where the relationship is no longer linear.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Setup data\nn_samples_physics <- 20\n\nset.seed(23)\n\nnoise <-  rnorm(n=n_samples_physics, mean=0, sd=2)\n\nphysics <- tibble(hours=runif(n=n_samples_physics, \n                              min=min_n_hours,\n                              max=max_n_hours),\n                  score= (hours - 7.5)^3 + 75 + noise)\n\nphysics %>% \n    ggplot(aes(x=hours, y=score)) +\n    geom_point() +\n    ggtitle(\"Physics class\") +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-13-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nWe can select the degree using a train/validation set by fitting 10 different models - each using a different degree - and calculating their $r^2$ for the train and test sets separately.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# In this class the focus is not on the code used in this section, but rather\n# how the procedure works. If you are curious about the code, read on...\n# Two new programming concepts are used: functions and for loops\n\n#########################\n# Digression: functions #\n#########################\n\n# we have not covered functions yet, but they are pretty straightforward\n# the name of the function is the first thing, the word function is special,\n# all of the arguments go in the parentheses, and the last line of the function is returned\n# Here is a simple example of a function that adds two numbers\nadd_two_nums <- function(a, b){\n    a + b\n}\n\n\n# create a function to return the Rsquared value\nget_R_sq <- function(model, test_data, test_y){\n    y_pred <- predict(model, newdata=test_data)\n    \n    TSS = sum((test_y - mean(test_y, na.rm=T))^2, na.rm=T)\n    RSS = sum((test_y - y_pred)^2, na.rm=T)\n    \n    1 - RSS / TSS\n}\n\n\n########################\n# train test set split #\n########################\n\n# Randomly assign observations to either train or test set\nphysics <- physics %>% \n    mutate(set_type=sample(x=c('train', 'test'),\n                           size=n_samples_physics, replace=TRUE, prob=c(0.8, 0.2)))\n\nphysics_train <- physics %>% \n    filter(set_type=='train')\n\nphysics_test <- physics %>% \n    filter(set_type=='test')\n\n########################\n# Hyperparemter tuning #\n########################\n\ndegrees2evaluate <- 1:10\n\n\nmodels <- c()\nRsq_values_train <- c()\nRsq_values_test <- c()\n# each iteration of this for loop evaluates the model for a different polynomial degree \nfor (degree in degrees2evaluate){\n    \n    # fit the model on the training data with the specified degree\n    model <- lm(score~poly(hours, degree=degree, raw=T), data=physics_train)\n    \n    # compute the R squared value with the train and test data\n    train_rsq <- get_R_sq(model=model, test_data=physics_train, test_y=physics_train$score)\n    test_rsq <- get_R_sq(model=model, test_data=physics_test, test_y=physics_test$score)\n\n    # save everything\n    # vector <- c(vector, value) will add the value to the end of the vector\n    # i.e. c does concatenation\n    models <- c(models, model)\n    Rsq_values_train <- c(Rsq_values_train, train_rsq)\n    Rsq_values_test <- c(Rsq_values_test, test_rsq)\n\n}\n\n# we have not yet learned the gather function\n# but you can probably figure out what it does by starting at this code\ntuning_results = tibble(degree=degrees2evaluate,\n                        train = Rsq_values_train,\n                        test = Rsq_values_test) %>%\n    gather('type', 'Rsq', -degree)\n\ntuning_results\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 20 Ã— 3\n   degree type    Rsq\n    <int> <chr> <dbl>\n 1      1 train 0.851\n 2      2 train 0.872\n 3      3 train 0.948\n 4      4 train 0.948\n 5      5 train 0.948\n 6      6 train 0.976\n 7      7 train 0.976\n 8      8 train 0.976\n 9      9 train 0.977\n10     10 train 0.977\n11      1 test  0.598\n12      2 test  0.674\n13      3 test  0.826\n14      4 test  0.825\n15      5 test  0.823\n16      6 test  0.620\n17      7 test  0.620\n18      8 test  0.620\n19      9 test  0.666\n20     10 test  0.666\n```\n:::\n:::\n\n\nFinally let's visualize the results.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntuning_results %>% \n    ggplot(aes(x=degree, y=Rsq, color=type)) + \n    geom_point() +\n    geom_line() + \n    scale_x_continuous(breaks=degrees2evaluate) +\n    theme_bw()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-15-1.png){fig-align='center' width=480}\n:::\n:::\n\n\nTaking a look at the numerical results we see degree 3 is the best model, though degrees 4 and 5 seem to work pretty well too.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntuning_results %>% \n    filter(type=='test') %>% \n    arrange(desc(Rsq))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 Ã— 3\n   degree type    Rsq\n    <int> <chr> <dbl>\n 1      3 test  0.826\n 2      4 test  0.825\n 3      5 test  0.823\n 4      2 test  0.674\n 5      9 test  0.666\n 6     10 test  0.666\n 7      7 test  0.620\n 8      8 test  0.620\n 9      6 test  0.620\n10      1 test  0.598\n```\n:::\n:::\n\n\n## Summary\n\nThis lecture is about overfitting; what happens when your model takes the particular dataset it was built on too seriously.\nThe more complex a model is, the more prone to overfitting it is.\nPolynomial models are able to create very complex functions thus high-degree polynomial models can easily overfit.\nFitting a model and evaluating it on the same dataset can be problematic; if the model is overfitted the evaluation metric (e.g. $r^2$) might be very good, but the model itself might not be.\nA better way to approach modeling is to fit the model to a training set then evaluate it with a separate test set.\nThis approach can be used to select the polynomial degree -- or other *model hyperparameters--* though a process called hyperparameter tuning.\n",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}