{
  "hash": "e3ac33767f48db61e86b3d4f30935c08",
  "result": {
    "markdown": "---\ntitle: \"From Samples to Populations\"\nsubtitle: \"The bias and variance of moving from sample to population.\"\ndate:  \"3/6/2023\"\n---\n<!-- The bit before that is commented out adds publish date to document metadata and button links at the top of the doc. This workflow can be revisited but currently doesn't work because listings aren't updated with these dates, only the renderd docs. -->\n\n::: {.cell}\n\n:::\n\n\n\n[[Discuss]{.btn .btn-primary}](https://edstem.org/us/courses/31657) [[Reading Questions]{.btn .btn-primary}](https://www.gradescope.com/courses/477232) [[PDF]{.btn .btn-primary}](notes.pdf)\n\n\n\n\\\n\n[G]{.dropcap}eneralization is the process of using a subset of information to draw conclusions about some broader set or phenomenon. It is powerful - it allows us to draw conclusions about things we have not observed - but it is tricky to do well. In these notes, you will learn the sources of error that can creep in when making a generalization.\n\n![](images/generalization.png){width=\"200\" fig-align=\"center\"}\n\nThere are four terms that you will see come up again and again as we discuss generalization. They are familiar terms that have tightly coupled meanings, so we present them together.\n\n**Sample**\n: The subset of units that are observed, measured, and analyzed. Commonly referred to as a data set. The size of the sample is indicated by $n$.\n\n**Population**\n: The set of units from which your sample is drawn. The size of the population is indicated by $N$.\n\n**Statistic**\n: A numerical summary of a sample. Examples include a sample mean, a sample median, a sample proportion, a sample correlation coefficient, and an estimated coefficient of a linear model.\n\n**Population Parameter**\n: A numerical summary of a population. Every statistic of a sample has an analog in the population (population mean, population proportion, etc).\n\nTo see how these terms interrelate and to introduce the sources of error that can creep in while generalizing from a sample to a population, let's look a scenario.\n\n### What year are my students?\n\nOn the first day of class, the professor strides into Pimentel Hall to present a lecture to a new crop of students. If you have not yet had the pleasure of having a class in Pimentel, it looks like this.\n\n![](images/pimentel-sm.jpeg){fig-align=\"center\" width=\"350\"}\n\n[^pimentel]: Photo of Pimentel Hall by flickr user TheRealMichaelMoore.\n\nPimentel[^pimentel] is the second largest lecture hall at Cal, with exactly 527 seats. On this first day of class, the room is packed and all 527 of the students registered for Stat 20 are in the hall.\n\nEager to ensure that the lecture is calibrated to the interest and experience of the students, the professor seeks to learn how many years they have been at Cal. The professor calls on a student sitting in the middle of the front row and asks, \"What year are you at Cal?\" The student replies, \"sophomore\", so the professor write 2 on the board. The professor proceeds to repeat this question to all of the students sitting in the 18 seats at the front row of Pimentel. By the end, there is a data frame on the board with one column and 18 rows, the first three of which read\n\n| Year |\n|:----:|\n|  2   |\n|  1   |\n|  1   |\n\nEver the statistician, the professor then visualizes the distribution of data by sketching on the board a bar chart and jots down the sample mean, which is 1.77.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n\npimentel <- data.frame(year = c(2, 1, 1, 1, 2, 2, \n                                4, 1, 1, 3, 3, 1, \n                                2, 2, 2, 1, 2, 1))\n\npimentel %>%\n  ggplot(aes(x = year)) +\n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=288}\n:::\n:::\n\n\nHow good of an answer is 1.77 to the professors question of, \"What year are my students?\"\n\n#### Identifying the components\n\nIn this setting, the population is the set of all $N = 527$ students in Pimentel. That is the set of units (students) that the professor seeks to understand and from which the sample is drawn. The sample is the subset of $n = 18$ students who were sitting in the front row and who were asked their year at Cal. The sample mean, $\\bar{x} = 1.77$, is an example of a statistic. Other statistics that could be calculated from the sample include the median (2) and the mode (1). These statistics be used to estimate their analogs in the population, such as the population mean, $\\mu$. The value of $\\mu$ is what the professor seeks to learn, but at this point is still unknown.\n\nUsing this terminology, we can ask the question again: how good of an estimate of the mean year of the entire class is the sample mean, 1.77? How much error did we incur when generalizing and where did that error come from?\n\n## Sources of Error\n\nTo understand the forms that estimation error can take, consider the analogy of darts thrown at a dart board. The center of the dart board represents the parameter that we are trying to hit and each dart that we throw represents a statistic calculated from a sample. There are two ways in which a dart throw can miss the bullseye. One way is that we could systematically tend to throw above and to the left of the bullseye. This form of error is called bias. Another way that our dart-throwing could miss the bullseye is if we are an erratic thrower and one throw tends to be very different from this next. This form of error is called variation.\n\n![](images/bullseye.png){fig-align=center width=\"370\"}\n\nIf a sample is *representative* of the population, there is no bias present. That is represented by the top row of bullseyes.\n\n### Types of Statistical Bias\n\nStatistical bias comes in many forms. Here we describe two of the most important types.\n\n**Selection bias**\n:    When the mechanism used to choose units for the sample tends to select certain units more often than they should.\n\nAs an example, a *convenience sample* chooses the units that are most easily available. Problems can arise when those who are easy to reach differ in important ways from those harder to reach. Another example of selection bias can happen with observational studies and experiments. These studies often rely on volunteers (people who choose to participate), and this self-selection has the potential for bias if the volunteers differ from the target population in important ways.\n\n**Measurement bias**\n:    When your process of measuring a variable systematically misses the target in one direction.\n\nFor example, low humidity can systematically give us incorrectly high measurements for air pollution. In addition, measurement devices can become unstable and drift over time and so produce systematic errors. In surveys, measurement bias can arise when questions are confusingly worded or leading, or when respondents may not be comfortable answering honestly.\n\nBoth of these types of bias can lead to situations where the data are not centered on the unknown targeted value. A common method to address selection bias is to draw a *simple random sample*, where each unit from the population is equally likely to be drawn. A pilot survey can improve question wording and so reduce measurement bias. Procedures to calibrate instruments and protocols to take measurements in, say, random order can reduce measurement bias.\n\nBias does not need to be avoided under all circumstances. If an instrument is highly precise (low variance) and has a small bias, then that instrument might be preferable to another that has high variance and little to no bias. Biased studies can be useful to pilot a survey instrument or to capture useful information for the design of a larger study.\n\n#### Statistical Bias in Pimentel\n\nThe method used by the professor likely suffers from statistical bias. The units that were drawn into the sample (the 18 students in the front row who were called on) constitute a convenience sample: they were sampled because they were easy to sample. That is not necessarily a problem, but there is good reason to think that students of all years are not equally likely to sit in the front row. First year students, bright-eyed and bushy-tailed with enthusiasm, will be more likely to sit in the front row. If that is true, the professor incurred selection bias that will lead to an estimate of the population mean that is too low.\n\nWhat about measurement bias? In this case, the process of measuring a student's year involves the act of asking them a question, hearing their answer, and writing it on the board. How could this be systematically in error? One way would be if first year students occasionally lie about their year when asked, for fear of being thought of as an over-eager over-achiever. If this were true, then lots of the 2s that we recorded were in fact 1s, and the estimate of 1.77 would be too high.\n\n### Types of Variation\n\nWhether or not bias is present, data typically also exhibit variation.\n\n**Sampling variability**\n:    If the sample is drawn from the population with some amount of randomness, the sampling variability describes the variability from one sample to the next.\n\n<!-- Add a few examples -->\n\n**Measurement variability**\n:    When we take multiple measurements on the same object, we get variations in measurements that are centered on the truth.\n\n<!-- Add a few examples -->\n\n#### Variability in Pimentel\n\nThere is some amount of randomness that plays into which units made their way into the professor's sample of size 18. At the start of a semester, students tend to sit in a different seat each day they come to class. Even in a world were all students are equally likely to sit in the first row (therefore there is no bias), our sample of 1.77 might be too low because that day an unusally high number of first year students happened to sit in the front row, purely due to chance. The next day, perhaps an unually high number of juniors would sit in the front row and the estimate would leap up to, say, 2.7. This is sampling variability.\n\nVariability in measurement here would refer to a process by which the recorded year for a student would differ from one measurement to the next. Imagine a student who sits in the front row on the first day and is recorded as a two That same student sits in the front row on the 10th day and is recorded as a one. It's hard to imagine there being much measurement variability here but you can imagine a very absent-minded professor who, upon hearing the year, will sometimes immediately forget what had been said and will record a random year.\n\n## The Sampling Distribution\n\nOne of the key concepts in understanding estimation errors when using statistics is to understand the shape of the sampling distribution of that statistic.\n\n**Sampling Distribution**\n\n: The distribution of a statistic upon repeated sampling.\n\nEven though this distribution has an innocuous name, something big is happening with this definition. No longer are we considering the (usually unknowable) distribution of the population or the observed distribution of the data (an empirical distribution). The sampling distribution is a distribution of a *statistic*, illustrating the different values that it could take, along with the probability of getting each of those values in a given sample.\n\nUsually the sampling distribution is a hypothetical thing: what would our statistic have looked like if we had taken a different sample of data? We can make it concrete by working in a setting where we can actually do just that.\n\n## Drawing Samples from Population\n\nTo understand the role that bias and variation play in estimates, we will for the moment assume that we have access to the entire population. This is almost never the case: if we had the population, we wouldn't need to bother with estimating it from a sample! It is a very useful thought experiment, however. It allows us to see the different ways that samples and statistics can be drawn under different scenarios.\n\nFor this thought-experiment, we will will be drawing from a population that has 527 observations / rows, one for every student in Pimentel, with a population distribution of year that looks like this.\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\npop_eager <- data.frame(year = factor(rep(c(1, 2, 3, 4), \n                                   times = c(245, 210, 47, 25))),\n                        eagerness = rep(c(10, 6, 3, 1), \n                                        times = c(245, 210, 47, 25)))\nset.seed(34)\npop_eager <- pop_eager %>%\n  slice_sample(n = nrow(pop_eager)) # shuffle the rows to make them look natural\npop_eager %>%\n  ggplot(aes(x = year)) +\n  geom_bar(fill = \"purple\") +\n  labs(title = \"Population Distribution\")\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=288}\n:::\n:::\n\n\n\n#### Scenario 1: Calling on the front row\n\nIn the data collection scheme used by the professor, the students who happened to be sitting in the 18 seats at the front of the class are the ones who will make it into the sample. As discussed above, it is likely that this process will result in selection bias since first year students tend to be more eager and eager students tend to sit at the font of the class.\n\nWe can do our best to envision what this selection bias would look like by selecting each student out of the population with a probability that is proportional to their eagerness. Let's say first year students have an eagerness score of 10 out of 10, sophomores have a 6, juniors a 3, and seniors a 1. Here are the first 5 rows of the population data frame with the eagerness scores right next to the year of the student.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nslice(pop_eager, 1, 2, 3, 4, 5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  year eagerness\n1    3         3\n2    2         6\n3    2         6\n4    1        10\n5    2         6\n```\n:::\n:::\n\n::: {.cell}\n\n:::\n\n\nWhen simulating the process that the professor used to draw 18 students, we can select the first student, a junior, with probability of 3 divided by the total eagerness of all of the students (the sum of the eagerness column), which is 4900. The probability of that student (or any junior) is $3/4900 = 0.0006$. That is small, but that's not surprising: there are 527 students in the class, so the probability of selecting just one of them should be small.\n\nThe probability of selecting the fourth student from the population data frame above, a first year, can be calculated as $10/4900 = .002$. That's also small, but it is 3.33 (10/3) times the chance of selecting a junior. In this setting, we're using eagerness as a *sampling weight* to determine the relative probability of selecting more eager students into the sample.\n\nLet's now simulate the process of drawing 18 rows out of the data frame of 527 rows, where each row is being selected with a probability proportional to it's eagerness. The three plots along the top row of the plot below illustrate what the empirical distribution of three samples might look like. They help us envision what the professor's plot on the board would look like on three different days where the 18 students in the front row were called on, and in each of those days, it is the first year students who are most likely to end up the front row.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(infer)\nsamp_1 <- pop_eager %>%\n  slice_sample(n = 18,\n                   replace = FALSE,\n                   weight_by = eagerness) \n\nmany_samps <- samp_1 %>%\n  mutate(replicate = 1)\n\nset.seed(40211)\n\nfor (i in 2:500) {\n  many_samps <- pop_eager %>%\n    slice_sample(n = 18,\n                 replace = FALSE,\n                 weight_by = eagerness) %>%\n    mutate(replicate = i ) %>%\n    bind_rows(many_samps)\n}\n\np1 <- many_samps %>%\n  filter(replicate == 1) %>%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 1\")\n\np2 <- many_samps %>%\n  filter(replicate == 2) %>%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 2\")\n\np3 <- many_samps %>%\n  filter(replicate == 3) %>%\n  ggplot(aes(x = year)) + \n  geom_bar() +\n  labs(title = \"Sample 3\")\n\nmany_xbars <- many_samps %>%\n  group_by(replicate) %>%\n  summarize(xbar = mean(as.numeric(year)))\n\np4 <- many_xbars %>%\n  ggplot(aes(x = xbar)) +\n  geom_bar(fill = \"purple\") +\n  lims(x = c(0, 4)) +\n  labs(title = \"Sampling Distribution\")\n\nlibrary(patchwork)\n\n(p1 + p2 + p3) / p4\n```\n\n::: {.cell-output-display}\n![](notes_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\nAs you look from Sample 1 to Sample 3, you notice that the distribution of year varies from sample to sample. This is sampling variability. If you compare these plots to the population distribution above, you'll find these three samples on balance seem to systematically have more first year students that you might expect. This is an effect of selection bias.\n\nBoth of these notions are captured in the sampling distribution. To understand the notion of a sampling distribution, imagine that you:\n\n1. Calculate the sample mean for sample 1 and store it away as $x_1 =$ 1.4.\n2. Calculate the sample mean for sample 2 and store it away as $x_2 =$ 1.2.\n3. Calculate the sample mean for sample 3 and store it away as $x_3 =$ 1.5.\n4. Repeat this process 500 times then\n5. Plot the distribution of those 500 $\\bar{x}$s.\n\nThis distribution is shown in the bottom row. We can see that in this scenario, it's possible to get $\\bar{x}$s that are as low as around 1 and as high as around 1.8. The $\\bar{x}$ that we would expect from this process is around 1.4. \n\n<!-- #### Scenario 2: Drawing names from a box -->\n\n<!-- ```{r} -->\n<!-- pop_equal <- pop_eager %>% -->\n<!--   mutate(eagerness = 1) -->\n\n<!-- samp_1 <- pop_equal %>% -->\n<!--   slice_sample(n = 18, -->\n<!--                    replace = FALSE, -->\n<!--                    weight_by = eagerness) -->\n\n<!-- many_samps <- samp_1 %>% -->\n<!--   mutate(replicate = 1) -->\n\n<!-- for (i in 2:500) { -->\n<!--   many_samps <- pop_equal %>% -->\n<!--     slice_sample(n = 18, -->\n<!--                  replace = FALSE, -->\n<!--                  weight_by = eagerness) %>% -->\n<!--     mutate(replicate = i ) %>% -->\n<!--     bind_rows(many_samps) -->\n<!-- } -->\n\n<!-- p1 <- many_samps %>% -->\n<!--   filter(replicate == 1) %>% -->\n<!--   ggplot(aes(x = year)) + -->\n<!--   geom_bar() + -->\n<!--   labs(title = \"Sample 1\") -->\n\n<!-- p2 <- many_samps %>% -->\n<!--   filter(replicate == 2) %>% -->\n<!--   ggplot(aes(x = year)) + -->\n<!--   geom_bar() + -->\n<!--   labs(title = \"Sample 1\") -->\n\n<!-- p3 <- many_samps %>% -->\n<!--   filter(replicate == 3) %>% -->\n<!--   ggplot(aes(x = year)) + -->\n<!--   geom_bar() + -->\n<!--   labs(title = \"Sample 1\") -->\n\n<!-- many_xbars <- many_samps %>% -->\n<!--   group_by(replicate) %>% -->\n<!--   summarize(xbar = mean(as.numeric(year))) -->\n\n<!-- p4 <- many_xbars %>% -->\n<!--   ggplot(aes(x = xbar)) + -->\n<!--   geom_bar(fill = \"purple\") + -->\n<!--   lims(x = c(0, 4)) + -->\n<!--   labs(title = \"Sampling Distribution\") -->\n\n<!-- (p1 + p2 + p3) / p4 -->\n<!-- ``` -->\n\n## Summary\n\nIn these notes we laid out a common goal of making generalizations: estimating the value of population parameters using statistics calculated from samples. The process of generalization is subject to several sources of error that are lumped into statistical bias and variation. Two of the central forms of statistical bias are selection bias and measurement bias. Two common forms of variation are sampling variability and measurement variability. With these notions of error in mind, we learned about the sampling distribution, the distribution of statistics that we would observe if we were to sample from the sample population many times and compute many statistics.\n\nThe sampling distribution is the central concept in making generalizations so we will revisit it throughout the coming weeks.\n\n---------------------------\n---------------------------\n\n## The Ideas in Code\n\n#### `slice_sample()`\n\nThe workhorse function to generate samples of data from a data frame is `slice_sample()`. This function takes a random sample of rows from a data frame. This function is in the `tidyverse` package. Useful arguments include:\n\n- `n`: the sample size.\n- `replace`: `TRUE` or `FALSE`. whether to sample rows with or without replacement.\n- `weight_by`: a column from the original data frame to use as sampling weights for each row.\n  \n##### Example 1\n\nTreat the `penguins` data set as your population. Take a random sample of three penguins, without replacement, from the `penguins` data frame where each of the penguins is equally likely.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(stat20data)\n\nset.seed(35)\npenguins %>%\n  slice_sample(n = 3,\n               replace = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 8\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Chinstrap Dream            43.5          18.1          202    3400 fema…  2009\n2 Gentoo    Biscoe           45.1          14.4          210    4400 fema…  2008\n3 Chinstrap Dream            52.2          18.8          197    3450 male   2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n:::\n\n\nObserve:\n\n- If you do not specify the sampling weights, the function will select each row with equal probability.\n- We set a seed so that everything this code is run, it will come up with the same sample of 12 rows. Try running this code yourself. Do you get the same sample?\n\n#### Example 2\n\nTake a random sample of three penguins without replacement using sampling weights proportional to the bill lengths; that is, a penguin with a bill length that is twice is long as another penguin should be twice as likely to be selected for the sample.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\npenguins %>%\n  slice_sample(n = 3, \n               replace = FALSE,\n               weight_by = bill_length_mm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 8\n  species   island bill_length_mm bill_depth_mm flipper_le…¹ body_…² sex    year\n  <fct>     <fct>           <dbl>         <dbl>        <int>   <int> <fct> <int>\n1 Chinstrap Dream            45.6          19.4          194    3525 fema…  2009\n2 Chinstrap Dream            51.3          19.9          198    3700 male   2007\n3 Adelie    Biscoe           45.6          20.3          191    4600 male   2009\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n```\n:::\n:::\n\n\nObserve:\n\n- While it is still possible for every penguin to be selected into the sample, the ones with longer bill lengths are more likely. The precise probability that the first penguin was selected was its bill length (43.5) divided by the sum of the bill lengths of all of the penguins in the population.\n\n\n#### `rep_slice_sample()`\n\nAn extension of `slice_sample()` that doesn't take just a single sample of size `n`, but repeats the process many times and draws many samples of size `n`. The useful new argument is `reps`, the number of replicates (or separate samples) you wish to generate. This function is in the `infer` package.\n\n##### Example 3\n\nTake two separate sample samples from the `penguins` data frame without replacement, and each of sample size three.\n\n\n::: {.cell}\n\n```{.r .cell-code  code-fold=\"false\"}\nlibrary(infer)\npenguins %>%\n  rep_slice_sample(n = 3,\n                   replace = FALSE,\n                   reps = 2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 9\n# Groups:   replicate [2]\n  replicate species   island bill_length_mm bill_d…¹ flipp…² body_…³ sex    year\n      <int> <fct>     <fct>           <dbl>    <dbl>   <int>   <int> <fct> <int>\n1         1 Chinstrap Dream            52.7     19.8     197    3725 male   2007\n2         1 Adelie    Dream            41.5     18.5     201    4000 male   2009\n3         1 Gentoo    Biscoe           52.5     15.6     221    5450 male   2009\n4         2 Chinstrap Dream            43.2     16.6     187    2900 fema…  2007\n5         2 Gentoo    Biscoe           45       15.4     220    5050 male   2008\n6         2 Adelie    Biscoe           45.6     20.3     191    4600 male   2009\n# … with abbreviated variable names ¹​bill_depth_mm, ²​flipper_length_mm,\n#   ³​body_mass_g\n```\n:::\n:::\n\n\nObserve:\n\n- The data frame produced by this function has a new column that has been appended to the left side: `replicate`. This column keeps track of which replicate the row belongs to. The first three rows constitute the first sample of size 3. The second three rows constitute the second sample of size 3.\n- The dimensions of the output data frame are 6 rows by 9 columns. Compare that to the dimensions of the original penguins data frame: 366 by 8. This function added one new column and contains `n` $\\times$ `reps` rows.\n\n:::{.content-hidden unless-profile=\"staff-guide\"}\n## Learning Objectives\n\n#### Concept Acquisition\n\n#### Tool Acquisition\n\n- `slice_sample()`\n- `rep_slice_sample()`\n\n#### Concept Application\n\n#### Key Terms\n\n- sample\n- population\n- population parameter\n- selection bias\n- non-response bias\n- measurement bias\n- sampling variability\n- measurement variability\n- random assignment\n\n## Reading Questions\n\n(@) What source of error do you suspect will be most prominent.\n\n(@) What source of error do you suspect will be most prominent.\n\n(@) Def of sampling distribution\n\n(@) The `mtcars` data frame contains 11 different variables (including its horsepower, `hp`, and its miles per gallon, `mpg`) recorded on 32 different models of car. Treating this data frame as your population, which expression would you use to draw a random sample of size five without replacement, where each car is equally likely?\n\nYou're welcome to test this out before answering. `mtcars` is built into R.\n\n( ) `mtcars %>% rep_slice_sample(n = 5, replace = FALSE, reps = 5)`\n\n( ) `mtcars %>% slice_sample(n = 5, replace = FALSE)`\n\n( ) `mtcars %>% rep_slice_sample(n = 5, replace = TRUE)`\n\n( ) `mtcars %>% slice_sample(n = 5, replace = FALSE, weight_by = hp)`\n\n## Materials from class\n\n- [Main Slides](slides.qmd)\n- [Worksheet](worksheet-analog.qmd)\n\n:::",
    "supporting": [
      "notes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}