---
title: "Overfitting handout"
format: pdf
editor: visual
---

Instead of $R^2$ we often just use the residual sum of squares (RSS), $\sum_{i=1}^n (y_i - \widehat{y}_i)^2$ or the mean square error (MSE) where MSE = $\frac{1}{n}\sum_{i=1}^n (y_i - \widehat{y}_i)^2.$
For this handout just use the test set or training set MSE to evaluate regression models.[^ugh]



Download the following training and test datasets.
```{r}
train <- read_csv('https://github.com/idc9/course-materials/tree/main/3-prediction/14-overfitting/train.csv')

val <- read_csv('https://github.com/idc9/course-materials/tree/main/3-prediction/14-overfitting/val.csv')


test <- read_csv('https://github.com/idc9/course-materials/tree/main/3-prediction/14-overfitting/test.csv')
```


# Question 1

What is the best MSE value you can obtain on the test set by fitting a polynomial model to the training set?


- Fit a polynomial models to the training set for different degrees (I suggest sticking to degrees less than 10).

- Evaluate the models' MSE on the validation set.

- Pick the best model from the validation set and compute the MSE for the test set.

# Question 2

Is there a way to cheat on question 1? If so, try cheating and see how low of a MSE you can get on the test set.


[^ugh]: There is a mathematical subtlety that comes up when using test set $R^2 = 1 - RSS/TSS$. If you fit a linear (or polynomial) regression model, the training set $R^2$ is always between 0 and 1. 
BUT in general this is not true; the test set $R^2$ is NOT required to always be between 0 and 1.
Even more frustrating; the training $R^2$ is only between 0 and 1 for a linear regression based model, but not necessarily for other modeling frameworks (e.g. deep learning). 


