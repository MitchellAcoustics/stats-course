---
title: "Questions and Data Scope"
date: "10/11/2022"
---

Let's pause and return to the data science lifecycle that was introduced in the beginning of this course. 

![](images/lifecycle-diagram.png){fig-align=center width="400"}

(Note that we have modified the earlier diagram a bit to distinguish the actions needed to work with data.)

We are at the point where we want to begin to understand the world and connect our question to the data we collect to how we can generalize beyond these data to improve our understanding of the world. 

## The Stages of the Data Science Lifecycle

There are four basic stages in the lifecycle: asking a question, obtaining data, understanding the data, and understanding the world.
We have made these stages very broad on purpose. In our experience, the mechanics of the lifecycle change frequently as data become more complex and we continue to develop new methodologies to address these new data scenarios. Despite these changes, we have found that almost every data project follows the four steps in our lifecycle.  


### Ask a Question. 

Asking good questions lies at the heart of statistics and data science, and recognizing different kinds of questions guides us in our analyses. For example, "How have house prices changed over time?" is very different from the question "How will this new policy affect house prices?" We focus on four broad categories of questions: descriptive, exploratory, inferential, and predictive. You have seen three of these four approaches to data analysis so far, and we will soon be addressing inferential questions. 
After we introduce inference, we will again revisit the concept of the question to be answered and describe how to match the question to be answered with the type of analysis to perform. 

Narrowing down a broad question into one that can be answered with data is a key element of the first stage in the lifecycle. It can involve consulting the people participating in a study, figuring out how to measure something, designing data collection protocols, and more. A clear and focused research question helps us determine the data we need, the patterns to look for, and how to interpret results. These considerations help us plan the data collection phase of the lifecycle.

### Obtain Data. 

When data are expensive and hard to gather and when our aim is to generalize from the data to the world, then we aim to define precise protocols for collecting the data needed to answer the question. Other times, data are cheap and easily accessed. This is especially true for online data sources. For example, Twitter lets people quickly download millions of data points.  In fact, when data are plentiful, we can start an analysis by obtaining data, exploring it, and then honing the research question. In both situations, most data have missing or unusual values. We need to check data quality. And, typically, we must manipulate measurements and create new variables before we can analyze the data more formally. That is, we may need to modify the structure of a data frame, and clean and transform data values to prepare for analysis. The descriptive analysis that we carry out in this stage may adequately answer our question, or, we may need to go on to the next stage to better understand and explore the data.

### Understand the Data. 

After obtaining and preparing data, we typically begin the task of  exploring the data. In these explorations, we make plots to uncover interesting patterns and summarize the data visually. We also continue to look for problems with the data. As we search for patterns and trends, we use summary statistics and build statistical models, like linear regression. In our experience, this stage of the lifecycle is highly iterative. Understanding the data can lead us back to the earlier stages where we, say, find that we need to modify or redo our data cleaning, acquire more data to supplement our analysis, or refine our research question given the limitations of the data. The descriptive and exploratory analyses that we carry out in this stage may adequately answer our question, or, we may need to go on to the next stage in order to make generalizations beyond our data.

### Understand the World. 

Often our goals are purely exploratory, and the analysis ends at the `Understand the Data' stage of the lifecycle. At other times, we aim to quantify how well the trends we find generalize beyond our data. We may want to use a model that we have fitted to our data to make inferences about the world or give predictions for future observations. To draw inferences from a sample to a population, we use statistical techniques like A/B testing and confidence intervals. And to make predictions for future observations, we create other kinds of interval estimates and use test/train splits of the data.


We use data to answer questions, and the quality of the data collection process can significantly impact the validity and accuracy of the data, the strength of the conclusions we can draw from an analysis, and the decisions we make. Here, we describe a general approach for understanding data collection, and use it to evaluate the usefulness of the data in addressing the question of interest. Ideally, we aim for data to be representative of the phenomenon that we are studying, whether that phenomenon is a population characteristic, a physical model, or some type of social behavior. Typically, our data do not contain complete information (the scope is restricted in some way), yet we want to use the data to accurately describe a population, estimate a scientific quantity, infer the form of a relationship between variables, or predict future outcomes. In all of these situations, if our data are not representative of the object of study, then our conclusions can be limited, possibly misleading, or even wrong.

## Target Population, Access Frame, Sample

The initial step in the data lifecycle is to express the question of interest in the context of the subject area and consider the connection between the question and the data collected to answer that question. It's good practice to do this before even thinking about the analysis or modeling steps because it may uncover a disconnect where the question of interest cannot be directly addressed with these data. As part of making the connection between the data collection process and the topic of investigation, we identify the population, the means of accessing the population, instruments of measurement, and additional protocols used in the collection process. These concepts help us understand the scope of the data, whether we aim to gain knowledge about a population, scientific quantity, physical model, social behavior, etc.


*Target* The target population consists of the collection of elements that you ultimately intend to describe and draw conclusions about. By element we mean those individuals that make up our population. The element may be a person in a group of people, a voter in an election, a tweet from a collection of tweets, or a county in a state. We sometimes call an element a unit or an atom. You have seen with the penguin study that the unit does not need to be a person. But, it can also be something quite abstract. We will get to that notion in a moment.

*Access Frame* The access frame is the collection of elements that are accessible to you for measurement. These are the units by which you can access the target population. Ideally, the access frame and population are perfectly aligned; meaning they consist of the exact same elements. However, the units in an access frame may be only a subset of the target population; additionally, the frame may include units that don’t belong to the population. For example, to find out how a voter intends to vote in an election, you might call people on the phone. Someone you call may not be a voter so they are in your frame but not in the population. On the other hand, a voter who never answers a call from an unknown number can't be reached so they are in the population but not in your frame.

*Sample* The sample is the subset of units taken from the access frame to measure, observe, and analyze. The sample gives you the data to analyze to make predictions or generalizations about the population of interest.

The contents of the access frame, in comparison to the target, and the method used to select units from the frame to be in the sample are important factors in determining whether or not the data can be considered representative of the target population. If the access frame is not representative of the target population, then the data from the sample is most likely not representative either. And, if the units are sampled in a biased manner, problems with being representative also arise.

You will also want to consider time and place in the data scope. For example, the effectiveness of a drug trial tested in one part of the world where a disease is raging might not compare as favorably with a trial in a different part of the world where background infection rates are lower. Additionally, data collected for the purpose of studying changes over time, like with the monthly measurements of CO2 in the atmosphere change in time so we need to be mindful of this limitation as we examine the data. At other times, there might be spatial patterns in the data. For example, environmental health data that are reported for each census tract in the State of California can be examined with maps to look for spatial correlations.

And, if you didn’t collect the data, you will want to consider who did and for what purpose. This is especially relevant today since more data is passively collected instead of collected with a specific goal in mind. Taking a hard look at *found data* and asking yourself whether and how these data might be used to address your question can save you from making a fruitless analysis or drawing inappropriate conclusions.

For each of the following examples, we begin with a general question, narrow it to one that can be answered with data, and in doing so, we identify the target population, access frame, and sample. These concepts are represented by circles in a diagram of data scope, and the configuration of their overlap helps reveal key aspects the scope. Also in each example, we describe relevant temporal and spatial features to the data scope.

### Example A

TBD

### Example B

TBD

### Example C

TBD


## Instruments and Protocols

When we consider the scope of the data, we also consider the instrument being used to take the measurements and the procedure for taking measurements, which are a part of what we call the protocol. For a survey, the instrument is typically a questionnaire that an individual in the sample answers. The protocol for a survey includes how the sample is chosen, how nonrespondents are followed up, interviewer training, protections for confidentiality, etc.

Good instruments and protocols are important to all kinds of data collection. If we want to measure a natural phenomenon, such as the length of the orbit of an asteroid that has been hit by a spacecraft, we need to quantify the accuracy of the instrument. 

The protocol for calibrating the instrument and taking measurements is vital to obtaining accurate measurements. Instruments can go out of alignment and measurements can drift over time leading to poor, highly inaccurate measurements.

Protocols are also critical in experiments. Ideally, any factor that can influence the outcome of the experiment is controlled. For example, temperature, time of day, confidentiality of a medical record, and even the order of taking measurements need to be consistent to rule out potential effects from these factors getting in the way.

With digital traces, the algorithms used to support online activity are dynamic and continually re-engineered. For example, Google’s search algorithms are continually tweaked to improve user service and advertising revenue. Changes to the search algorithms can impact the data generated from the searches, which in turn impact systems built from these data, such as the Google Flu Trend tracking system. This changing environment can make it untenable to maintain data collection protocols and difficult to replicate findings.

Many data analysis projects involve linking data together from multiple sources. Each source needs to be examined through this data-scope construct and any difference across sources considered. Additionally, matching algorithms used to combine data from multiple sources need to be clearly understood so that populations and frames from the sources can be compared.

Measurements from an instrument taken to study a natural phenomenon can also be cast in the scope-diagram of a target, access frame, and sample. This approach is helpful in understanding their accuracy.


The scope-diagram introduced for observing a target population can be extended to the situation where we want to measure a quantity such as the count of particles in the air, the age of a fossil, the speed of light, etc. In these cases we consider the quan‐ tity we want to measure as an unknown value. (This unknown value referred is often referred to as a parameter.) In our diagram, we shrink the target to a point that represents this unknown. The instrument’s accuracy acts as the frame, and the sample con‐ sists of the measurements taken by the instrument within the frame. You might think of the frame as a dart board, where the instrument is the person throwing the darts. If they are reasonably good, the darts land within the circle, scattered around the bulls‐ eye. The scatter of darts correspond to the measurements taken by the instrument. The target point is not seen by the dart thrower, but ideally it coincides with the bulls‐eye.

### Example - DART spacecraft

[Prior to DART’s impact, it took Dimorphos 11 hours and 55 minutes to orbit its larger parent asteroid, Didymos. Since DART’s intentional collision with Dimorphos on Sept. 26, astronomers have been using telescopes on Earth to measure how much that time has changed. Now, the investigation team has confirmed the spacecraft’s impact altered Dimorphos’ orbit around Didymos by 32 minutes, shortening the 11 hour and 55-minute orbit to 11 hours and 23 minutes. This measurement has a margin of uncertainty of approximately plus or minus 2 minutes.]




## Accuracy

When we have a census, the sample captures the entire population (and the access frame matches the population). In this situation, if we administer a well-designed questionnaire, then we have complete and accurate knowledge of the population, and the scope is complete. Similarly in measuring the length of an asteroid's orbit, if our instrument has perfect accuracy and is properly used, then we can measure the exact length of the orbit. These situations are rare, if not impossible. In most settings, we need to quantify the accuracy of our measurements in order to generalize our findings to the unobserved. For example, we often use the sample to estimate an average value for a population, infer the value of a scientific unknown from measurements, or predict the behavior of a new individual. In each of these settings, we also want a quantifiable degree of accuracy. We want to know how close our estimates, inferences, and predictions are to the truth.

The analogy of darts thrown at a dart board can be useful in understanding accuracy. We divide accuracy into two basic parts: bias and variance (also known as precision). Our goal is for the darts to hit the bullseye on the dart board and for the bullseye to line up with the unseen target. The spray of the darts on the board represents the variance in our measurements, and the gap from the bullseye to the unknown value that we are targeting represents the bias. 

Representative data puts us in the top row of the diagram, where there is low bias, meaning that the bullseye and the unseen target are in alignment. Ideally our instruments and protocols put us in the upper left part of the diagram, where the variance is also low. The pattern of points in the bottom row systematically miss the targeted value. Taking larger samples will not correct this bias.

### Types of Bias

Bias comes in many forms. We describe some classic types here and connect them to our target-access-sample framework.

+ Coverage bias occurs when the access frame does not include every unit in the target population. For example, a survey based on phone calls cannot reach those with no phone. In this situation, those who cannot be reached may differ in important ways from those in the access frame.

+ Selection bias arises when the mechanism used to choose units for the sample tends to select certain units more often than they should. As an example, a convenience sample chooses the units most easily available. Problems can arise when those who are easy to reach differ in important ways from those harder to reach. Another example of selection bias can happen with observational studies and experiments. These studies often rely on volunteers (people who choose to participate), and this self-selection has the potential for bias, if the volunteers differ from the target population in important ways.

+ Non-response bias comes in two forms: unit and item. Unit non-response happens when someone selected to be in the sample is unwilling to participate. Item non-response occurs when, say, someone in the sample refuses to answer a particular survey question. Non-response can lead to bias if those who choose not to participate or to not answer a particular question are systematically different from those who respond.

+ Measurement bias happens when an instrument systematically misses the target in one direction. For example, low humidity can systematically give us incorrectly high measurements of air pollution. In addition, measurement devices can become unstable and drift over time and so produce systematic errors. In surveys, measurement bias can arise when questions are confusingly worded or leading, or when respondents may not be comfortable answering honestly.

Each of these types of bias can lead to situations where the data are not centered on the unknown targeted value. Often we cannot assess the potential magnitude of the bias, since little to no information is available on those who are outside of the access frame, less likely to be selected for the sample, or disinclined to respond. Protocols are key to reducing these sources of bias. Chance mechanisms to select a sample from the frame or to assign units to experimental conditions can eliminate selection bias. A non-response follow-up protocol to encourage participation can reduce non- response bias. A pilot survey can improve question wording and so reduce measurement bias. Procedures to calibrate instruments and protocols to take measurements in, say, random order can reduce measurement bias.

Bias does not need to be avoided under all circumstances. If an instrument is highly precise (low variance) and has a small bias, then that instrument might be preferable to another with higher variance and no bias. As an example, biased studies are potentially useful to pilot a survey instrument or to capture useful information for the design of a larger study. Many times we can at best recruit volunteers for a study. Given this limitation, it can still be useful to enroll these volunteers in the study and use random assignment to split them into treatment groups. That’s the idea behind randomized controlled experiments.

Whether or not bias is present, data typically also exhibit variation. Variation can be introduced purposely by using a chance mechanism to select a sample, and it can occur naturally through an instrument’s precision.


### Types of Variation

Variation that results from a chance mechanism has the advantage of being quantifiable.

+ Sampling variation results from using chance to take a sample. We can in principle compute the chance a particular sample is selected.

+ Assignment variation of units to treatment groups in a controlled experiment produces variation. If we split the units up differently, then we can get different results from the experiment. This randomness allows us to compute the chance of a particular group assignment.

+ Measurement error for instruments result from the measurement process; if the instrument has no drift and a reliable distribution of errors, then when we take multiple measurements on the same object, we get variations in measurements that are centered on the truth.

The Box Model is a simple abstraction that can be helpful for understanding variation. This model examines a container (a box) full of identical tickets that have been labeled, and we use the simple action of drawing tickets from the box to reason about sampling schemes, randomized controlled experiments, and measurement error. For each of these types of variation, the urn model helps us estimate the size of the variation using either probability or simulation. The example of selecting Wikipedia contributors to receive an informal award provides two examples of the urn model.

## Summary

No matter the kind of data you are working with, before diving into cleaning, explo‐ration, and analysis, take a moment to look into the data’s source. If you didn’t collect the data, ask yourself:

+ Who collected the data?
+ Why were the data collected?

Answers to these questions can help determine whether these found data can be used to address the question of interest to you.

Consider the scope of the data. Questions about the temporal and spatial aspects of data collection can provide valuable insights:

+ When were the data collected?
+ Where were the data collected?

Answers to these questions help you determine whether your findings are relevant to the situation that interests you, or whether your situation that may not be comparable to this other place and time.

Core to the notion of scope are answers to the following questions:

+ What is the target population (or unknown parameter value)?
+ How was the target accessed?
+ What methods were used to select samples/take measurements?
+ What instruments were used and how were they calibrated?

Answering as many of these questions as possible can give you valuable insights as to how much trust you can place in your findings and how far you can generalize your findings.

These notes have provided you with a terminology and framework for thinking about and answering these questions. The chapter has also outlined ways to identify possible sources of bias and variance that can impact the accuracy of your findings.

Next, we develop the Box model for situations when a chance mechanism can be used to select a sample from an access frame, divide a group into experimental treatment groups, or take measurements from a well calibrated instrument.


