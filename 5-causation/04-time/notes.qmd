---
title: "Using Time to Measure Causal Effects"
subtitle: "pre/post, interrupted time series, and difference-in-difference designs"
format:
  html:
    code-fold: true
    code-link: true
    code-summary: "."
  pdf: default
execute: 
  warning: false
  message: false
---



## Causality and variation across time.

When we don't have access to a randomized experiment or a natural experiment, 
creative methods are 
required to extract evidence about causal claims from data.  In the last set of
notes, we looked at matching methods, which approximate unobserved counterfactual
outcomes by identifying different that appear very similar based on their 
covariate values. Today we think about a very different way to approximate 
missing counterfactuals: repeated observations for the same subject at different
times with different treatments.


### Same unit at different times

 Let's say that we collected data from Evelyn Fix before and after she graduated from Cal.

```{r}
#| echo: false
#| message: false
#| warning: false

library(tidyverse)
library(kableExtra)
options(kableExtra.html.bsTable = TRUE)

tab <- tibble(`Student` = c("Evelyn Fix", "Evelyn Fix"),
              `date` = c("June 2022", "April 2022"),
              `GPA` = c(3.9, 3.9),
              `Years exp` = c(3, 3),
              `Rec` = c("strong", "strong"),
              `Cal Grad` = c("yes", "no"),
              `Good Job` = c("yes", "no"))
```

::: {.content-hidden unless-format="html"}
```{r}
#| echo: false
#| message: false
#| warning: false
tab %>%
    kbl(escape = FALSE) %>%
    kable_styling(bootstrap_options = c("hover", "striped"))
```
:::

::: {.content-hidden unless-format="pdf"}
```{r}
#| echo: false
#| message: false
#| warning: false
tab %>%
    kable(format = "latex")
```
:::

Here, the unit of observation is a student at a particular time, which allows us to compare Evelyn before and after graduating from Cal. How close of a counterfactual is pre-graduation Evelyn? Keep in mind the true counterfactual would be the alternate universe where everything about the world is the same but for one thing: Evelyn hadn't graduated from Cal. Is everything the same in this pre-graduation world?

The answer may be yes. Here we see her GPA is unchanged, she hasn't gained any more work experience, and she's sending out the same strong letter of recommendation. If these are the only other variables involved in getting a good job, then this would be good evidence that the graduation is the cause of the good job.

But what if her GPA had inched up that final semester? Or what if the jobs that she applied for pre-graduation were simply not as good as the jobs that she applied for after graduation? In this setting, there could be multiple reasons that she got the good job because we haven't isolated the variable of interest: graduating from Cal.

Data with repeated measures for individual subjects is called *longitudinal* or
*panel* data.  There are several study designs that rely upon this structure 
to interrogate causal claims. *Pre/post*
designs use two measurements for each subject, one with the treatment and one
without, to measure a causal effect.  *Interrupted time series* designs are similar
but use multiple observations before and after each unit enters treatment to 
estimate counterfactual trends.  Finally, *difference in difference* designs unite
some of the benefits of matching and pre/post designs.

## Do supermarket checkout displays cause unhealthy eating?

![](images/supermarket-checkout.png){#fig-inflation fig-align=center width="500px"}[^checkoutpic]

You (as well as any small children you know) have probably noticed that 
supermarket checkout lines often feature a display
of candy, soda, chips, or other "less-healthy" snacks.  Government agencies and 
consumer advocacy groups have called for supermarket chains to remove such 
displays, claiming that they lead to unhealthy eating choices and poorer health
for consumers.  In the United Kingdom, some supermarkets have responded by 
instituting 'checkout line' policies under which candy is removed and replaced
with healthier options.  We will look
at data on the number of purchases of unhealthy foods at four stores that 
instituted such a policy in the mid-2010s.  

```{r}
load('checkout_data.Rdata')
checkout_concise
```

The causal claim here is that consumers would have purchased fewer unhealthy 
snacks if the candy display had been removed from the checkout line (the 
broader claims about the impact of this change on public health are not 
things we can test with data from the supermarkets alone).  A great way to 
test this theory might be to randomly assign some supermarkets to remove their
checkout displays and others to keep theirs, and compare purchases.  
Unfortunately that study was not conducted, and it's not clear that these 
stores decided to institute their policies in a quasi-random way (e.g. what if
only the stores selling very low levels of candy agreed to participate?).  We
could also consider matching stores with the policy to those without the policy
and comparing their sales.  But we our dataset doesn't include many
store covariates, and in fact  only includes stores that instituted the
new policy.


[^checkoutpic]: Image from Shutterstock/SpeedKingz, obtained from <https://www.today.com/parents/kids-cant-resist-candy-stores-try-junk-food-free-healthy-t60621>.

## Pre/post design

Instead, we will focus on comparing the same stores before and after the policy
change, an idea called a pre/post design.

**Pre/post design**
:   A study in which a group of units receiving a treatment is compared to 
earlier versions of themselves before treatment occurred.

A pre/post design takes the simple approach of subtracting the pre-treatment
value from the post-treatment value for each store and taking the average.
The following plot shows the change in sales for each of four stores in the data,
and the pre/post estimate is computed below using the \texttt{infer} package.

```{r}
checkout_concise |>
  ggplot(mapping = aes(x = checkout_policy, y = sales, group = store_id)) + 
  geom_line() + labs(title = 'Sales of unhealthy foods often decreased post-policy',
                     x = 'Checkout policy instituted?',
                     y = 'Sales (number of packages sold)')
```

```{r}
library(infer)
checkout_concise |>
  specify(response = sales,
          explanatory = checkout_policy) |>
  calculate(stat = "diff in means", order = c('TRUE','FALSE'))
  
```

### Pros and cons of pre/post

To see the benefits of a pre/post design, think about what covariate balance
looks like on store covariates such as size, geographic location (urban/suburban/rural),
demographics of surrounding neighborhood (senior citizens vs. young professionals
vs. families with children), and staffing.  Even though we don't have these
covariates measured in our dataset, we can be pretty confident that most of them
are perfectly or near-perfectly balanced.

Covariate balance also helps identify the biggest weakness of a pre/post design.
What would the SMD look like for time (if we measured as a numeric variable counting
the number of months starting
from January 2016)?  It would probably be quite large since the treated observations
are systematically later than the control observations.  So although most 
covariates will be well-controlled, pre/post designs are highly susceptible to time
trends in the outcome (or other outcome-related covariates that shift over time).
For example, what if one of the stores rolled out its policy on January 1st, so
that the four weeks prior to the policy included the winter holidays, when British
consumers traditionally buy and consume a lot of sweets, and the four weeks immediately
following the policy included the beginning of the year when many people are starting
diet and exercise plans?  A matched study could avoid this particular issue
by comparing different stores, one with the policy and one without, during the 
same time period. So while matched studies require **no unmeasured confounding**,
pre/post designs require counterfactual outcomes and unmeasured confounding to 
be **stable across time**.


### Confidence intervals for pre/post effect estimates

We can use the bootstrap to build confidence intervals for pre/post designs,
but we have to be careful.  The pre/post design depends on having exactly two 
observations for each store; if we take a bootstrap sample from all observations,
we may end up with three or more for some stores and only one for others.  To
avoid this issue we first convert our observations to pre/post differences in
sales, and bootstrap the differences.

```{r}
#create new dataframe with one row per store and pre/post sales difference
checkout_treated <- checkout_concise |>
  filter(checkout_policy == TRUE) |>
  mutate('treated_sales' = sales)
checkout_control_sales <- checkout_concise |>
  filter(checkout_policy == FALSE) |>
  mutate('control_sales' = sales) |>
  select(control_sales)
checkout_by_store <- cbind(checkout_treated, checkout_control_sales) |>
  mutate('sales_diff' = treated_sales - control_sales) |>
  select(store_id, sales_diff)

#pre/post estimate calculated from this dataframem
pp_est <- checkout_by_store |>
 specify(response = sales_diff) |>
 calculate(stat = 'mean')

#bootstrap sales difference
set.seed(2024-3-28)
bootstrap_checkout <- checkout_by_store |>
  specify(response = sales_diff) |>
  generate(reps = 500, type = 'bootstrap') |>
  calculate(stat = 'mean')

checkout_conf_int <- bootstrap_checkout |>
  get_confidence_interval(level = .95)

bootstrap_checkout |>
  visualize() +
  shade_confidence_interval(checkout_conf_int)
  


```

Although our 95\% confidence interval mostly includes negative values, it also
includes zero and some positive effects, so these four stores alone don't provide
a compelling case for a strong causal effect of checkout policies on sales.

## Dealing with effects of time.

Suppose there is a gradual downward trend in purchase of unhealthy snacks over
time, as British consumers become better informed about health risks and
gradually switch to healthier options.  This is a problem for our pre/post design,
which can't distinguish between reductions in sales due to the nationwide trend
and reductions in sales due to the policy change that we're interested in. 
Fortunately if we have the right kind of additional data available we can attempt
to identify the time trend and distinguish it from our treatment effect.  We'll
focus on two approaches, each of which relies on a different kind of additional
data.

### More timepoints: interrupted time series

First note that in addition to measuring sales in the four-week periods
immediately before and after the policy change, we have data on several more 
four-week periods going further back in time before the change, and going
further forward in time after the change.  We can now extend the earlier plot
for the pre/post design to include the additional timepoints.

```{r}
checkout |>
  ggplot(mapping = aes(x = time, y = sales, group = store_id)) + 
  geom_line() + labs(title = 'Sales of unhealthy foods did not show a dramatic time trend',
                     x = 'Time (4-week periods)',
                     y = 'Sales (number of packages sold)') + 
  geom_vline(xintercept = 15, linetype = 2) +
  annotate('text', label = 'Checkout policy enacted', x = 15, y = 50)
```

By looking at the changes over time before and after the time, we are able to
get a sense for the time trend; in addition, by looking at the sudden shift in the 
line's height that occurs 
at the time of the policy change, we can get a separate sense for the average
treatment effect.  Because the treatment effect "interrupts" the pattern of the
smooth time trend, this design is known as an **interrupted time series**.

To estimate the treatment effect after separating out the trend, we can fit a 
regression model for sales two independent variables, a numeric variable for time 
period and a binary 
indicator for whether the policy change has been instituted:

```{r}
its_model <- lm(sales ~ checkout_policy + time, data = checkout)
its_model
```

Fitting this model to all our observations will give us two parallel lines, 
one for the before-treatment periods (drawn in red) and one for the 
after-treatment periods (drawn in blue)

```{r}
checkout |>
  ggplot(mapping = aes(x = time, y = sales, group = store_id)) + 
  geom_line() + labs(title = 'Checkout policy appears to cause small decrease in sales',
                     x = 'Time (4-week periods)',
                     y = 'Sales (number of packages sold)') + 
  geom_vline(xintercept = 15, linetype = 2) +
  geom_abline(slope = coef(its_model)[3], intercept = coef(its_model)[1], color
               = 'red', linetype = 3) + 
  geom_abline(slope = coef(its_model)[3], 
            intercept = coef(its_model)[1] + coef(its_model)[2], color = 'blue', 
            linetype = 3) + 
  
  annotate('text', label = 'Checkout policy enacted', x = 15, y = 50) +
  annotate('segment', x = 0, xend = 15, y = coef(its_model)[1], yend =
            coef(its_model)[1]+coef(its_model)[3]*15, color = 'red') +
  annotate('segment', x = 15, xend = 29,
           y = coef(its_model)[1]+coef(its_model)[2]+coef(its_model)[3]*15 , 
           yend = coef(its_model)[1]+coef(its_model)[2]+coef(its_model)[3]*29, 
            color = 'blue') 
```

The distance from the red line to the blue line is given by the coefficient of
\texttt{checkout_policy} and is our estimate
of the average treatment effect.  You can think of the dotted red line as the 
(counterfactual) version of sales had the policy not been instituted, to which
we are comparing the actual post-treatment sales (in solid blue).
<!--gives the difference between that counterfactual case and the observed reality
under treatment.-->

CUT OR FOOTNOTE THIS?  What if the points in the after-treatment period appear
to follow a different slope than the points in the before-treatment period?
This suggests that the average effect of treatment may change over time.  
Consider the plot below:


![](images/its_change_in_slope.png){#fig-inflation fig-align=center width="500px"}

Here the dashed line shows the counterfactual possibility of no treatment 
in the later periods (by extending the trend from the earlier periods), while the
blue line shows the trend in the actual data collected.  The
difference between the two lines is almost twice as large at the very end of the plot
as it is right when the intervention occurs.  This isn't especially plausible in
the supermarket checkout example (nor does the plot look like it requires two
separate lines) but may apply in other settings where treatment
has both an immemdiate and a more gradual long-term effect.  <!--in the first period when the
treatment is instituted, but is XX several periods later-->  To fit this model, 
you would simply fit two regression models, one to the before-treatment period
alone and the other two the after-treatment period alone.



Interrupted time series do not require unobserved
confounders to be stable over time, but they do require changes in 
unobserved confounders to affect outcomes only **linearly** (i.e. with a constant
slope). To see why this assumption is important, consider the following picture:

![](images/its_change_in_slope.png){#fig-inflation fig-align=center width="500px"}

Here the red band gives the shape of the actual pattern in the data, which
appears to be a smooth and gradually decreasing curve. If the time trend has this
kind of shape, the interrupted time series analysis will not be able to recognize
it and may instead insist on the presence of large treatment effects (as the lines
appear to show) which are unlikely to be real.


To make a confidence interval, it's important that we keep all units in a trajectory
together as we did with the pre/post design;  however, since we have a more 
complicated model to fit we can't just reduce everything to pre-post differences.
Instead we would need to take bootstrap samples where we select entire groups of 
observations (i.e. all the units for a given store) with replacement.  This is
known as the block bootstrap.  However, it is not possible to do this with 
\texttt{infer} package so we won't cover it.


### Pure controls: difference-in-difference design

The interrupted time series design relies on additional observations in pre and/or
post periods to separate
a time trend from a treatment effect. Another kind of data that can help us
is data from additional stores during the same time periods that never institute
a checkout policy.  Clearly these stores do not experience a treatment effect,
but if they are subject to the same time trend as the treated stores, we can
use them to estimate the time trend.  Here is a plot illustrating this idea:

![](images/did.png){#fig-inflation fig-align=center width="500px"}

The dashed line represents a counterfactual average outcome for the treated units 
if they had not received treatment, and we are interested in learning the difference
between this dashed line and the actual outcome for the intervention group.  
Although we don't have multiple observations prior to treatment to estimate the
slope of the dashed line anymore, we can learn it by looking at the pre/post differences
in the control group below.  We can then get an effect estimate by subtracting
the pure control pre/post difference in the outcome $Y$ from the treated group's 
pre/post difference in outcome $Y$:

$$
\text{Estimated effect} = (\overline{Y}^{treat}_{post} -  \overline{Y}^{treat}_{pre}) - 
(\overline{Y}^{control}_{post} -  \overline{Y}^{ control}_{pre}) 
$$
This is called a **difference-in-difference** design.  You can think of it as a
combination of the matching approach from the last set of notes and the panel data
approaches from these set of notes, since it takes advantages of comparisons
across both units and time.  It doesn't require that time trends be linear as
the interrupted time series does
(since we don't have to rely on a constant slope across previous time points), but
it does require that the time trend is the same in the pure control stores as in
the treated stores, an assumption known as **parallel trends**. 


We now test out difference-in-difference analysis on the supermarket checkout 
data.  We start by making a pre/post plot with our four treated stores and
two additional stores that never instituted a checkout policy [^simcontrol].
```{r}
checkout_w_control |>
  mutate('ever_treated' = !(store_id %in% c(244,248))) |>
  ggplot(mapping = aes(x = time, y = sales, group = store_id, color = ever_treated)) + 
  geom_line() + labs(title = 'Pure control stores have much larger sales volume',
                     x = 'Time period',
                     y = 'Sales (number of packages sold)')
```

One thing that immediately jumps out about this plot is that both pure control
stores have much larger sales volume in general than the treated stores. This 
probably means that the pure control stores are much bigger than their
treated counterparts. This would be very bad news if we were trying to match
treated stores in the post period to control stores in the post period, since
we wouldn't succeed in finding close pairs. Fortunately we don't need to 
construct close matched pairs for this design.  We do need to believe parallel
counterfactual time
trends (on average) between the treated stores and the pure control stores if 
neither were to be treated, but this
seems more plausible based on the plot[^did_matching].


We now compute pre/post differences for the treated stores and for the pure control
stores, and take their difference to get our effect estimate.
```{r}
#create new dataframe with one row per store and pre/post sales difference
checkout_post <- checkout_w_control |>
  filter(time == 'post') |>
  mutate('post_sales' = sales)
checkout_pre_sales <- checkout_w_control |>
  filter(time == 'pre') |>
  mutate('pre_sales' = sales) |>
  select(pre_sales)
checkout_by_store <- cbind(checkout_post, checkout_pre_sales) |>
  mutate('sales_diff' = post_sales - pre_sales,
         'ever_treated' = checkout_policy) |>
  select(store_id, sales_diff, ever_treated)

checkout_by_store |>
  group_by(ever_treated) |>
  summarize('Average Pre/Post Difference in Sales' = mean(sales_diff))

checkout_by_store |>
  specify(response = sales_diff, 
          explanatory = ever_treated) |>
  calculate(stat = 'diff in means', order = c('TRUE', 'FALSE'))

```



To make a confidence interval for a difference-in-difference effect, we can
use a similar strategy to the one we used for pre/post: take pre/post differences for all stores,
(treated and pure control) then bootstrap these.  The only difference is that we
keep track of which differences are treated and which are not, and we compute the
difference between their means instead of their overall mean:

```{r}
set.seed(2024-3-28)
bootstrap_did <- checkout_by_store |>
  specify(response = sales_diff, 
          explanatory = ever_treated) |>
  generate(reps = 500, type = 'bootstrap') |>
  calculate(stat = 'diff in means', order = c('TRUE', 'FALSE'))

bootstrap_confint <- bootstrap_did |>
  get_confidence_interval(level = 0.95)

bootstrap_did |>
  visualize() +
  shade_confidence_interval(bootstrap_confint)
```
This bootstrap distribution suggests there is no strong evidence of a treatment
effect of one particular sign; the data is consistent with a wide range of 
both positive and negative effects.  This is not surprising given our small 
sample size.  

[^simcontrol]: The version of the supermarket checkout data we have does not
actually contain pure controls, so observations for two control units have been 
synthetically generated for illustrative purposes.

[^did_matching]: We still might feel better about believing the parallel trends 
assumptions if 
pure control stores looked generally similar to treated stores in size and in
other attributes.  If we
had a larger set of pure controls available and more store covariates measured,
we could check covariate balance and conduct matching first, then run the 
difference-in-difference design on the pre/post data from the matched subjects
only.

## Summary

Repeated observations for the same subject across time can be a powerful tool
for developing arguments about causal claims.  When all units are measured once
before and once after treatment, a pre/post design helps address concerns about
unobserved confounders as long as the counterfactual outcomes and any unobserved 
confounders remain the same
across time.  Even when there is some effect of time on the outcome, we can
attempt to estimate that time effect and separate it from our effect estimate,
either by using additional measurements for each subject (interrupted time
series) or by using subjects who never receive treatment (difference-in-difference).
Like any non-randomized study, these approaches require assumptions about 
the true process that generated the data that you should think carefully about.
It is also important to be careful about confidence interval construction for
designs with repeated measures to make sure bootstrapped datasets are similar
to the original data.


